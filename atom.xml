<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>冲弱&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/ab8296a41c8b88ea9f8a771cd548cb5e</icon>
  <subtitle>多阅读 多积累</subtitle>
  <link href="https://oysz2016.github.io/atom.xml" rel="self"/>
  
  <link href="https://oysz2016.github.io/"/>
  <updated>2023-12-17T02:54:55.294Z</updated>
  <id>https://oysz2016.github.io/</id>
  
  <author>
    <name>冲弱</name>
    <email>ouyang-sz@foxmail.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>一篇文章搞懂EVA</title>
    <link href="https://oysz2016.github.io/post/a77440e0.html"/>
    <id>https://oysz2016.github.io/post/a77440e0.html</id>
    <published>2023-12-16T14:26:16.439Z</published>
    <updated>2023-12-17T02:54:55.294Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>本文首发于公众号“<a href="https://mp.weixin.qq.com/s/WKX8uDp7zqbXVG-1-LjlLw">CVTALK</a>”。</p><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2211.07636"><strong>https://arxiv.org/abs/2211.07636</strong></a><br /><strong>代码链接:</strong><a href="https://github.com/baaivision/EVA"><strong>https://github.com/baaivision/EVA</strong></a><br /></p><p>EVA是智源曹越团队的文章，也是微软时期swin transformer的作者之一。论文的全称是Exploring the Limits of Masked Visual Representation Learning at Scale。<strong>从名字可以看出，EVA试图探索掩码表征学习下模型规模的极限</strong></p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p><strong>在EVA中主要对比了之前视觉foundation model两种常用的方式，分别是以BEIT中重建tokenize的和特征蒸馏的方式。</strong>BEIT就不赘述了，之前在<a href="https://mp.weixin.qq.com/s/0JXKGXerN07NLbCRJJSpOg">IBOT</a>的论文中有较详细的写过。下面讲下特征蒸馏的方式。<br />EVA中所讨论的知识蒸馏文章是之前曹越在微软AI团队参与的工作<a href="https://arxiv.org/abs/2205.14141">Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation</a>。这篇文章的出发点是想使用相对MIM要简单不少的特征蒸馏，让student模型获取到和MIM的训练方式有相似性能的模型。下面介绍时将这篇文章简称为<strong>Feature Distillation。</strong><br />Feature Distillation的网络结构如下图所示：<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/EVA/1.png" alt=""></p><p><br />网络的整体架构比较简单，确实相对于MIM是非常简洁的结构:</p><ul><li><strong>输入:</strong> 一张图像，经过两次增广，分别作为teacher和student的输入</li><li><strong>tracher:</strong> 一个训练好的网络，可以是CLIP或者DINO</li><li><strong>student:</strong> 随机初始化的VIT模型或者swin transformer</li><li><strong>loss:</strong> 训练目标是让计算teacher网络输出的特征和strudent网络经过projector head后的特征的L1 loss。期望让student网络的特征和teacher网络的特征相似</li></ul><p><strong>论文的整体思路很简单，当然里面也有很多消融实验让蒸馏的效果尽可能好。</strong>例如和对比学习的方法中一样，student模型的输出增加了projector head；为了更好的对比不同teacher模型指导蒸馏的差异，将teacher输出的特征做了whitening处理将所有teacher模型的特征归一化到同一量级；VIT中采用相对位置编码，而不是原始的绝对位置编码。<br />回到EVA的故事，<strong>EVA分别验证了MIM中的tokenize和Feature Distillation中的蒸馏方式都不是必要的。</strong><br />不用toeknize和蒸馏的效果对比如下图所示，图a和b中第一行都是clip作为teacher模型在下游任务上fintune的效果，最后一行都是EVA模型的效果。区别在于图a的二三行使用了tokenize的方式训练了300和1600epoch，<strong>但效果都不如没有使用toeknize训练800epoch的EVA，证明了tokenize的方式并不必要</strong>；图b中二三行使用Feature Distillation的蒸馏方式训练了300和800epoch，<strong>验证了蒸馏时间变长，并没有带来更大的收益，并且也不如同样训练800epoch的EVA，证明了Feature Distillation的蒸馏方式不是必要的。</strong><br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/EVA/2.png" alt=""></p><h2 id="EVA"><a href="#EVA" class="headerlink" title="EVA"></a>EVA</h2><p><strong>论文链接: </strong><a href="https://arxiv.org/abs/2303.11331"><strong>https://arxiv.org/abs/2303.11331</strong></a><br /><strong>虽然EVA分别diss了BEIT中的tokenize和Feature distillation中的蒸馏方式，但在实现上EVA还是借鉴了两者，从某些角度理解算是两者的结合。</strong>EVA的大致流程如下图所示:<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/EVA/3.png" alt=""></p><p><br /></p><p>思路也非常简单:</p><ul><li><strong>输入:</strong> 一张图像，经过两次增广，分别作为teacher和student的输入。类似MIM，student输入的图片有40%的patch被mask了</li><li><strong>teacher:</strong> 训练好的CLIP网络</li><li><strong>student:</strong> 拥有10亿参数的VIT网络</li><li><strong>loss:</strong> 训练目标是计算teacher网络和student网络输出特征的余弦相似度，期望两者的特征尽量接近</li></ul><p>初看EVA时我也很疑惑，动机中提到了tokenizer和Feature Distillation的问题，但EVA的思路和Feature Distillation还是非常接近的，<strong>差异主要有以下几点:</strong></p><ul><li><strong>数据增广时使用了MIM的思想mask掉一些patch，迫使网络学习遮挡不变的特征；</strong></li><li><strong>student参数规模更大了，VIT达到了10亿；</strong></li><li><strong>由smooth L1 loss换成了余弦相似度</strong></li></ul><p>EVA相关的配置如下图:<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/EVA/4.png" alt=""></p><p><br /><strong>值得一提的是10亿参数量的VIT并没有使用太多的训练数据，仅用到了完全开源的3千万的数据，这主要得益于CLIP的teacher模型和MIN的数据增广方式。</strong></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/EVA/5.png" alt=""></p><p><br />图像分类上的结果如上图所示，最上面三行灰色的部分，模型参数量都在10亿以上，效果也是最好的，但使用了大量私有的数据集做训练。<strong>下面的结果处了BEIT外基本都在14M的IN-21K数据集上训练，但EVA在相同数据规模的情况下，参数量是最大的，且效果也是最好的。</strong><br />文中也对比了在不同数据集上的鲁棒性<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/EVA/6.png" alt=""></p><h3 id="视频动作识别"><a href="#视频动作识别" class="headerlink" title="视频动作识别"></a>视频动作识别</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/EVA/7.png" alt=""></p><p><br />在视频的任务中表现的也很出色</p><h3 id="目标检测和实例分割"><a href="#目标检测和实例分割" class="headerlink" title="目标检测和实例分割"></a>目标检测和实例分割</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/EVA/8.png" alt=""></p><p><br />目标检测和实例分割都是刷榜很严重的任务了，EVA在其中的表现也都挺好。</p><h2 id="EVA-02"><a href="#EVA-02" class="headerlink" title="EVA-02"></a>EVA-02</h2><p>这里顺便也介绍下EVA-02，因为改动并不算大，结合EVA-01的工作会能加深印象。主要差异有以下几点:</p><ul><li><strong>VIT的网络结构做了大量的消融实验，包含norm，参数初始化，FFN，位置编码方式</strong></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/EVA/9.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/EVA/10.png" alt=""></p><ul><li><strong>teacher模型由CLIP换成了EVA-01</strong></li><li><strong>数据量由3千万到4千万</strong></li><li><strong>参数量更少由10亿到3亿，但性能有提升</strong></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/EVA/11.png" alt=""></p><p><br />EVA-02包含5种参数量的模型<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/EVA/12.png" alt=""></p><p>整体效果相比EVA-01实现了全方位的提升<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/EVA/13.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>蒸馏方向的论文，基本都是为了让一个小的student网络在不损失太多性能的前提下，学习到大的teacher网络的特征。<br />而在大模型时代，<strong>EVA探索了student网络能达到的规模上限，并且在测试集上效果略微超过了teacher网络。伴随着EVA-01的成功，EVA-02做了更精细的调整，并且为了变得“可用”，参数量做了大量的缩减，而性能相比EVA-01有明显的提升。</strong><br />为什么EVA蒸馏后的网络会比teacher网络有更好的效果呢？个人感觉是CLIP确实足够强大，而且EVA中student网络的MIM训练方式足够的好。<strong>具体而言CLIP在4亿的图文对上做了预训练，输出的图像特征和语言的特征做了对齐，是一种高维的语义信息，而VIT作为一个backbone，更利于提取到低维的结构特征，并且MIM的方式迫使VIT学习遮挡不变的特征，最终的特征具有了很好的鲁棒性。</strong></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="MIM" scheme="https://oysz2016.github.io/tags/MIM/"/>
    
    <category term="蒸馏" scheme="https://oysz2016.github.io/tags/%E8%92%B8%E9%A6%8F/"/>
    
    <category term="CV foundation model" scheme="https://oysz2016.github.io/tags/CV-foundation-model/"/>
    
  </entry>
  
  <entry>
    <title>BLIP2——优化多模态训练成本</title>
    <link href="https://oysz2016.github.io/post/38f4a12a.html"/>
    <id>https://oysz2016.github.io/post/38f4a12a.html</id>
    <published>2023-10-05T02:49:18.899Z</published>
    <updated>2023-10-09T08:58:21.227Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2301.12597"><strong>https://arxiv.org/abs/2301.12597</strong></a><br /><strong>代码链接:</strong> <a href="https://github.com/salesforce/LAVIS/tree/main/projects/blip2"><strong>https://github.com/salesforce/LAVIS/tree/main/projects/blip2</strong></a><br /> <strong>DEMO链接:</strong> <a href="https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/"><strong>https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/</strong></a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>BLIP2是<a href="https://mp.weixin.qq.com/s/_rgWfJxmTNigA9L68NqVnw">BLIP</a>的续作，<strong>解决的是多模态大模型训练成本越来越高昂的问题</strong>。在多模态模型中，参数量和计算成本比较高的分别是image encoder和text encoder。<br /><strong>前面的相关文章介绍了<a href="https://mp.weixin.qq.com/s/QhDyTSgs6-4asN4XFY1LYA">ALBEF</a>和<a href="https://mp.weixin.qq.com/s/_rgWfJxmTNigA9L68NqVnw">BLIP</a>，在这篇文章的开始会先了解下BLIP2的大概结构和改动。</strong><br /><strong>在BLIP2中为了减少计算成本，将image encoder和text encoder的部分冻结；提出了轻量级的Q-Former模块，对齐图像和文本的特征。</strong><br /><strong>BLIP2提出了一个两阶段的预训练任务</strong>，如下图所示:<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP2/1.png" alt=""></p><ul><li><strong>第一阶段: Vision-and-Language Representation Learning。表征学习，用Q-Former对齐Image Encoder和LLM之间的特征</strong></li><li><strong>第二阶段: Vision-to-Language Generative Learning。生成学习，Q-Former提取到和文本最相关的图像特征，结合图文使用LLM获取其文本生成的能力。</strong></li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>Q-Former的结构和预训练任务如下图所示:<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP2/2.png" alt=""></p><p><strong>Q-Former中包含两个Transformer:</strong></p><ul><li>与被冻结的Image Encoder交互提取视觉特征的<strong>image transformer</strong>，输入包含可学习的Quires和Image Encoder的输出特征。</li><li>可以作为text encoder也可以作为text decoder的<strong>text transformer</strong></li></ul><p>Q-Former的参数初始化通过<code>BERTbase</code>，而cross attention层随机初始化。Q-Former的总参数量为1.88亿。Q-Former中包含32个queries，纬度为768，而Image Encoder在结构为ViT-L/14时输出的特征维度为(257, 1024)。在预训练的时候会迫使image Encoder和Q-Former的bottleneck结构通过learned queries提取到和文本最相关的视觉信息。</p><h3 id="Bootstrap-Vision-Language-Representation-Learning-from-a-Frozen-Image-Encoder"><a href="#Bootstrap-Vision-Language-Representation-Learning-from-a-Frozen-Image-Encoder" class="headerlink" title="Bootstrap Vision-Language Representation Learning from a Frozen Image Encoder"></a>Bootstrap Vision-Language Representation Learning from a Frozen Image Encoder</h3><p>该阶段的目的是训练Q-Former，使其可以提取到与提供文本最相关的视觉特征，和BLIP类似包含三个预训练任务：</p><ul><li><strong>Image-Text Contrastive Loss(ITC):</strong> 图像-文本对比学习，对齐图像和文本特征</li><li><strong>Image-grounded Text Generation(ITG):</strong> 用于做image caption任务，和BLIP中的LM任务的差异在于图像特征的来源并不是image encoder，而是learned queries</li><li><strong>Image-Text Matching (ITM):</strong> 判断图像和文本是否匹配，用于对齐特征</li></ul><p>这些任务会共享参数，由于任务的差异，为了避免信息泄漏，query token和text token之间有不一样mask的策略:<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP2/3.png" alt=""></p><ul><li><strong>ITM:</strong> 为了做匹配任务，在该任务中使用的是<strong>Bi-directional Self-Attention</strong>, 需要知道图像和文本彼此的特征才能做查询，因此不需要mask。</li><li><strong>ITG:</strong> 在该任务中，需要用生成描述图像对应的文本。<strong>为了避免信息泄漏，query不能看到文本的内容,因此做query的时候，text的部分需要mask</strong>；<strong>而在文本生成的部分，只能看到之前已经生成的文本，未生成的部分也需要mask。</strong></li><li><strong>ITC:</strong> 在该任务中，<strong>可以参考Figure .2，learned queries和input text分别经过self attention和MLP之后做对比学习任务，为了避免信息泄漏，Q和T之前彼此需要mask。</strong></li></ul><h3 id="Bootstrap-Vision-to-Language-Generative-Learning-from-a-Frozen-LLM"><a href="#Bootstrap-Vision-to-Language-Generative-Learning-from-a-Frozen-LLM" class="headerlink" title="Bootstrap Vision-to-Language Generative Learning from a Frozen LLM"></a>Bootstrap Vision-to-Language Generative Learning from a Frozen LLM</h3><p>在该阶段的目的是连接Q-Former和冻结参数的LLM，获取LLM的文本生成能力。如下图三所示，包含两个预训练任务。<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP2/4.png" alt=""></p><p>使用全连接层讲Q-Former输出的特征对齐到和LLM相同的维度，Q-Former提取的视觉特征用来约束LLM的生成，可以为LLM提供最有用的视觉信息。<strong>Q-Former的存在避免了为了对齐图像-文本特征而训练LLM带来的灾难性遗忘的问题(catastrophic forgetting problem)。</strong><br />该阶段的预训练任务包含两个:</p><ul><li><strong>LLM Decoder: 根据Q-Former提供的视觉表示从投开始生成文本</strong></li><li><strong>LLM Encoder+LLM Decoder: 将文本拆分成两部分，前缀文本和Q-Former的特征通过编码器，解码器用于生成剩下的文本</strong><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Instructed-Zero-shot-Image-to-Text-Generation"><a href="#Instructed-Zero-shot-Image-to-Text-Generation" class="headerlink" title="Instructed Zero-shot Image-to-Text Generation"></a>Instructed Zero-shot Image-to-Text Generation</h3></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP2/5.png" alt=""></p><p>BLIP2的框架能够有效的让LLM理解图像，并且保留文本生成能力。下图中展示了在visual knowledge reasoning、visual commensense reasoning、visual conversatio、personalized image-to-text generation等方面的可视化能力<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP2/6.png" alt=""></p><h3 id="Image-Captioning"><a href="#Image-Captioning" class="headerlink" title="Image Captioning"></a>Image Captioning</h3><p>在image caption任务上微调了BLIP模型，”a photo of”作为LLM的初始输入。在微调过程中冻结LLM，更新image encoder和Q-former的参数。使用了COCO训练数据做微调，并在COCO的测试集测试效果，也在NoCaps验证集上作了zero-shot的实验.<br />结果如下表所示，BLIP2在NoCaps数据机上达到了SOTA，表明了对out-domain的数据也有很好的泛化性。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP2/7.png" alt=""></p><h3 id="Visual-Question-Answering"><a href="#Visual-Question-Answering" class="headerlink" title="Visual Question Answering"></a>Visual Question Answering</h3><p>使用了VQA数据集微调，和Image Caption任务一样，在微调过程中冻结了LLM，更新image encoder和Q-former的参数。LLM的输入是Q-Former的输出和问题，需要生成答案。为了提取与问题更相关的图像特征，在Q-Former的text-encoder中会将问题作为输入，用于引导Q-Former中的Cross attention产生和问题相关的图像特征。下表展示了VQA任务上的效果.<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP2/8.png" alt=""></p><h3 id="Image-Text-Retrieval"><a href="#Image-Text-Retrieval" class="headerlink" title="Image-Text Retrieval"></a>Image-Text Retrieval</h3><p>使用了COCO数据微调，由于图像-文本检索任务不需要LLM的生成能力，因此该任务只需要对Q-Former相关的模块微调。对比如下：<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP2/9.png" alt=""></p><p>ITC和ITM对图文检索任务非常重要，但ITG任务的添加也会带来收益，原因是可以让queries生成和输入文本最相关的特征，提高视觉特征的xiaoguo 。<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP2/10.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>BLIP2论文中最关键的部分在于Q-Former。在初看BLIP2时让我想到了将transformer应用到目标检测领域的DETR，DETR中的decoder部分的输入包含object queries。关于obejct queries究竟学到了什么在DETR中有简单的说明:<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP2/11.png" alt=""></p><p>DETR中object queries的实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.query_embed = nn.Embedding(num_queries, hidden_dim)</span><br></pre></td></tr></table></figure><br>100个隐藏层维度为256的embedding，<strong>每个query中注入了不同object的位置信息和类别信息。</strong><br />Q-Former中的learn queries也是类似:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.query_tokens = nn.Parameter(torch.zeros(<span class="number">1</span>, config.num_query_tokens, config.qformer_config.hidden_size))</span><br></pre></td></tr></table></figure><br>32个隐藏层维度为hidden_size的embedding，<strong>每个query用于编码和文本最相关的视觉特征。</strong></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="多模态" scheme="https://oysz2016.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>BLIP——统一理解与生成的多模态模型</title>
    <link href="https://oysz2016.github.io/post/23099eb6.html"/>
    <id>https://oysz2016.github.io/post/23099eb6.html</id>
    <published>2023-09-28T10:07:18.324Z</published>
    <updated>2023-10-05T03:18:45.501Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>本文首发于公众号“<a href="https://mp.weixin.qq.com/s/_rgWfJxmTNigA9L68NqVnw">CVTALK</a>”。</p><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2201.12086"><strong>https://arxiv.org/abs/2201.12086</strong></a><br /><strong>代码链接:</strong><a href="https://github.com/salesforce/BLIP"><strong>https://github.com/salesforce/BLIP</strong></a><br /> <strong>作者blog链接:</strong><a href="https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/"><strong>https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/</strong></a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>BLIP的来源于标题中的<strong>B</strong>ootstrapping <strong>L</strong>anguage-<strong>I</strong>mage <strong>P</strong>re-training for Unified Vision-Language Understanding and Generation。从标题可以看出来BLIP也是文本-图像的预训练模型，并且目的是统一视觉-语言的理解和生成任务。同时支持和理解&amp;生成任务正是BLIP和之前任务的差异。<br />BLIP可以看作是<a href="https://zhuanlan.zhihu.com/p/655552483">ALBEF</a>的续作，一作是同一个人，并且都是多模态的领域，多模态任务展现出了比单模态更好的效果，但存在一些局限性:</p><ul><li><strong>模型角度: </strong>目前多模态的任务主要分为两种，分别是encoder-based model和encoder-decoder model, 都存在一些问题<ul><li><strong>encoder-based model: </strong>典型代表是CLIP, 因为预训练的时候没有生成任务，所以在下游任务中很难支持生成任务。例如CLIP不能完成image caption任务.</li><li><strong>encoder-decoder model: </strong>典型代表是SimVLM，该类方法目前在图文检索任务中的效果非常差</li></ul></li><li><strong>数据角度: </strong>通过互联网上爬取到的图像文本对，含有很多噪声</li></ul><p>相对于ALBEF，BLIP中提出了MED和CapFilt两个模块解决上面提到的两个局限性</p><h2 id="BLIP"><a href="#BLIP" class="headerlink" title="BLIP"></a>BLIP</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/1.png" alt=""></p><p>BLIP的网络结构如上图所示, 还是和ALBEF一样，将其拆分为几个部分。BLIP网络的构成包含3个encoder和1个decoder、3个预训练任务还有数据相关的CapFilt模块</p><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>MED的全称是multimodal mixture of encoder-decoder，即包含encoder和decoder的多模态融合，encoder的任务可以做多模态的理解，而decoder任务可以做多模态的生成。分别有以下部分:</p><ul><li><strong>单模态encoder: </strong>该部分包含两个encoder，分别是image encoder和text encoder，也是Figure 2中的前两个模块。分别对使用VIT和BERT提取图像和文本的特征。</li><li><strong>基于图像的文本encoder: </strong>该部分和Text encoder的区别在于引入了image encoder产生的特征做cross attention。因为包含图像和文本特征，会用来做图像文本匹配(ITM)任务</li><li><strong>基于图像的文本解码器: </strong>该部分和基于图像的文本encoder的区别在于将自注意力层替换为了因果自注意力层。用于做语言模型(LM)的任务.</li></ul><p>需要注意的是text相关的encoder和decoder有三个，text encoder和Image-grounded Text encoder的共有结构特征是共享的，为了标记差异，在文本的开头分别用”[CLS]”和”[Encoder]”标记。而Image-grounded Text decoder中使用”[Decoder]”</p><h3 id="Pre-training-Objectives"><a href="#Pre-training-Objectives" class="headerlink" title="Pre-training Objectives"></a>Pre-training Objectives</h3><p>在预训练中有三个优化目标，有两个基于理解的预训练任务和一个基于生成的预训练任务组成。计算量比较大的image encoder只需要运算一次。</p><h4 id="Image-Text-Contrastive-Loss-ITC"><a href="#Image-Text-Contrastive-Loss-ITC" class="headerlink" title="Image-Text Contrastive Loss(ITC)"></a>Image-Text Contrastive Loss(ITC)</h4><p>该部分和ALBEF中一致，目的都是为了对齐视觉和文本模态的特征，并且也使用了momentum encoder产生伪标签作为监督信号。在代码实现上也和ALBEF中的ITC loss完全一致<br /><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/2.png" alt=""></p><h4 id="Image-Text-Matching-Loss-ITM"><a href="#Image-Text-Matching-Loss-ITM" class="headerlink" title="Image-Text Matching Loss (ITM)"></a>Image-Text Matching Loss (ITM)</h4><p>该部分同样了ALBEF中一致，训练目标都是判断图像和文本是否匹配，并且也挖掘了难负样本。代码实现上也和ALBEF中ITM loss的计算完全一致<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/3.png" alt=""></p><h4 id="Language-Modeling-Loss-LM"><a href="#Language-Modeling-Loss-LM" class="headerlink" title="Language Modeling Loss (LM)"></a>Language Modeling Loss (LM)</h4><p>该部分不同于ALBEF中的LM loss，在ALBEF中参考的是Bert中的完形填空任务，目的是完成语言模型的训练，并且能对齐图像和文本的特征。但和Bert类似的任务用途是做语义的理解，并不能做生成任务。因此在BLIP中更类似于GPT中预测下一个token的任务，区别在于BLIP中做的是image caption的任务，即生成图像对应的文本描述。<strong>在LM中和ITM中共享了具有共同结构的参数，可以提高模型训练效率，也能从多任务学习中获益.</strong><br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/4.png" alt=""></p><h3 id="CapFilt"><a href="#CapFilt" class="headerlink" title="CapFilt"></a>CapFilt</h3><p>在ALBEF中也讨论了用于预训练的图像-文本对是从互联网上获取，但这部分数据质量不高，在ALBEF中使用了momentum distillation产生软标签减小噪声数据的影响。momentum的trick在BLIP中也有，但BLIP中有更进一步减少脏数据的影响——试图将预训练数据的质量变高。粗略的理解可以看下图Figure 1，在MED的预训练任务中包含生成字幕的预训练任务也有判断图文是否匹配的预训练任务，因此可以让模型生成图片的描述(Captioner)，再通过Filter用于判断图像和文本是否匹配。在图1中，原始的图文不匹配，在最终预训练时会被过滤掉，而Captioner生成的文本和图片匹配，则在最终预训练时会保留生成的数据。<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/5.png" alt=""></p><p>更具体的CapFilt可以参考下图，原始的数据集包含两部分:</p><ul><li>互联网上收集的(Iw,Tw)，存在脏数据</li><li>人工标注(Ih,Th), 可以认为是干净数据</li></ul><p>在Model pretraining模块中会使用包含互联网收集和人工标注两部分数据用于MED部分的预训练，然后会只使用人工标注的干净数据做不同预训练任务的微调:</p><ul><li>只微调MED中的ITC&amp;ITM任务，让模型有更好的图像文本理解能力，用于在Filtering。判断图像和文本是否匹配，文本的来源可以是互联网收集的和图像匹配的文本，也可以是captioning生成的文本</li><li>只微调MED中的LM任务，让模型有更好的生成字幕能力</li></ul><p>经过Filtering后会保留下来的数据有三部分:</p><ul><li>人工标注的数据(Iw,Tw)</li><li>互联网收集的图像和文本匹配的数据(Iw,Tw)</li><li>Captioner生成的和图像匹配的数据(Iw,Ts)</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/6.png" alt=""></p><h2 id="Experiments-and-Discussions"><a href="#Experiments-and-Discussions" class="headerlink" title="Experiments and Discussions"></a>Experiments and Discussions</h2><h3 id="Effect-of-CapFilt"><a href="#Effect-of-CapFilt" class="headerlink" title="Effect of CapFilt"></a>Effect of CapFilt</h3><p>关于CapFilter的效果在下表Table 1中可以看到，<strong>分别使用Captioner和Filter都可以提升下游任务上的效果，并且增大数据集的规模，提升会更明显。</strong></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/7.png" alt=""></p><p>下图展示了互联网上搜集到的文本和Captioner生成文本数据中一些噪声的示例。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/8.png" alt=""></p><h3 id="Diversity-is-Key-for-Synthetic-Captions"><a href="#Diversity-is-Key-for-Synthetic-Captions" class="headerlink" title="Diversity is Key for Synthetic Captions"></a>Diversity is Key for Synthetic Captions</h3><p>这一部分将CapFilt中Captioner生成字幕采取nucleus sampling(top-p)和beam search两种解码方式的差异，对比如下<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/9.png" alt=""></p><p>Nucleus的方式效果更好，相比Beam search的方法，前者生成的文本更加多样(不过也带来了更多的噪声数据)</p><h3 id="Parameter-Sharing-and-Decoupling"><a href="#Parameter-Sharing-and-Decoupling" class="headerlink" title="Parameter Sharing and Decoupling"></a>Parameter Sharing and Decoupling</h3><p>在预训练阶段，使用14M的数据做预训练。与不共享相比，共享除了Self Attention之外的能获得更好的效果，也能减少模型参数量，提升效率。而如果SA和CA层共享，由于编码和解码任务的冲突，模型的能力会变差</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/10.png" alt=""></p><p>在CapFilt阶段，下表中研究了captioner和filter保持和预训练中一样的参数共享方式或者解耦的差异，结果保持一样的参数共享方式，性能会有退化。作者将其归因于<strong>confirmation bias</strong>。因为如果Captioner和Filter也存在参数共享，Captioner生成的字幕会变得不容易被Filter判断为有噪声的数据。<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/11.png" alt=""></p><h2 id="Comparison-with-State-of-the-arts"><a href="#Comparison-with-State-of-the-arts" class="headerlink" title="Comparison with State-of-the-arts"></a>Comparison with State-of-the-arts</h2><p>主要比较了几个多模态的任务:</p><ul><li>Image-Text Retrieval</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/12.png" alt=""></p><ul><li>Image Captioning</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/13.png" alt=""></p><ul><li>Visual Question Answering (VQA)</li><li>Natural Language Visual Reasoning (NLVR2)</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/14.png" alt=""></p><ul><li>Visual Dialog (VisDial)</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/15.png" alt=""></p><ul><li>Zero-shot Transfer to Video-Language Tasks</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/16.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>BLIP和ALBEF的结构很相似，但关注点从除了单模态和多模态特征融合到开始关注文本的生成能力，其中一个预训练任务从类似Bert替换成了类似GPT，也顺应了NLP领域中的这一趋势。并且和其他大模型一样，更加关注数据的质量，用CapFilt模块获取了质量更高的数据。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="多模态" scheme="https://oysz2016.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>ALBEF:Align before Fuse</title>
    <link href="https://oysz2016.github.io/post/13dc672a.html"/>
    <id>https://oysz2016.github.io/post/13dc672a.html</id>
    <published>2023-09-11T02:33:27.961Z</published>
    <updated>2023-09-28T10:06:49.485Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span><br>本文首发于公众号“<a href="https://mp.weixin.qq.com/s/QhDyTSgs6-4asN4XFY1LYA">CVTALK</a>”。</p><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2107.07651">https://arxiv.org/abs/2107.07651</a></p><p><strong>代码链接:</strong><a href="https://github.com/salesforce/ALBEF">https://github.com/salesforce/ALBEF</a></p><p><strong>作者Blog链接:</strong><a href="https://blog.salesforceairesearch.com/align-before-fuse/">https://blog.salesforceairesearch.com/align-before-fuse/</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>语言和视觉是人类感知世界的两个最基本的渠道，因此多模态的算法里经常会将语言和视觉的模态结合。这一类方法被称为<strong>VLP(Vision-and-Language Pre-training)</strong>。然而现有的方法主要有以下局限.</p><p>之前多模态的方法大致可以分为两类，一类例如VL-Bert等，<strong>重点在于使用transformer作为encoder获取图像和文本交互的特征</strong>，由于视觉文本特征在transformer之前并没有对齐，因此在transformer任务中获取交互特征很具有挑战。为了让模型更容易学习交互特征，这类方法往往需要高分辨率的图像作为输入，并且需要预训练目标检测器(在<a href="https://arxiv.org/abs/2102.03334">ViLT</a>中尝试移除目标检测器，但效果有明显下降)。<strong>这类方法在需要结合视觉和文本的推理任务上效果较好，例如NLVR和VQA等任务中。</strong><br>第二种方法例如CLIP和ALIGN，<strong>重点在于分别学习图像和文本模型的任务，做特征对齐，使用对比学习让两个模态的特征都具有其他模态的信息</strong>。<strong>这类方法在图像-文本的检索任务上表现良好，但由于相比第一种方法缺乏图像和文本的交互特征，在更复杂的图像-文本交互任务上表现一般。</strong></p><p>在作者的<a href="https://blog.salesforceairesearch.com/align-before-fuse/">blog</a>中提到了之前方法的另外一点局限性:</p><blockquote><p>预训练的数据主要来源于互联网，数据来源并不干净。MLM等任务容易对噪声过拟合，从而影响到最终的特征。</p></blockquote><h2 id="ALBEF"><a href="#ALBEF" class="headerlink" title="ALBEF"></a>ALBEF</h2><p>ALBEF的名字来源于<u><strong>AL</strong></u>ign the image and text representations <u><strong>BE</strong></u>fore <u><strong>F</strong></u>using。论文的标题中也强调了Align before Fuse。区别于其他多模态方法，这篇文章提出在融合视觉-文本两个模态的特征之前，先做不同模态之间特征的对齐。出发点结合了上文提到的两类VLP方法的优点，并且减缓了脏数据对预训练的影响。</p><p>ALBEF的网络结构如下图所示:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/1.png" alt=""></p><p>ALBEF的结构并不复杂，弄清楚几个模块和loss也就明白其中的奥妙了。ALBEF包含3个encoder、3个loss和动量蒸馏模块。</p><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>encoder部分如下:</p><ul><li>image encoder: 12层的ViT-B/16</li><li>text encoder: bert中的前6层</li><li>multimodal encoder: Bert中的后6层，因为不同特征间需要attention，相比另外两个encoder多了cross attention的结构</li></ul><h3 id="Pre-training-Objectives"><a href="#Pre-training-Objectives" class="headerlink" title="Pre-training Objectives"></a>Pre-training Objectives</h3><p>训练目标主要包含三个预训练任务:</p><ul><li>image-Text contrastive learning(ITC): <strong>目标是在特征融合前对齐视觉和文本的特征以获取更好的单模态特征。</strong>受启发于moco，维护了两个队列分别存储动量编码器中的图像和文本特征。使用infoNCE loss分别计算image-to-text和text-to-image的损失。具体可以看下代码</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/2.png" alt="维护batch size大小的队列"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/3.png" alt="计算encoder的特征和momentum encoder特征，infoNCE loss，更新队列"></p><ul><li>Image-Text Matching(ITM): <strong>预测图像和文本是否匹配。</strong>一般的ITM任务相对简单，loss能降到非常低，ALBEF为了让这项预训练任务变得更有意义，挖掘了困难的负样本。具体做法是在ITC任务中infoNCE loss有相似度的信息，选取和相似度最高，但不匹配的样本作为负样本。损失函数是交叉熵损失。</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/4.png" alt=""></p><ul><li>Masked Language Modeling(MLM): <strong>训练语言模型。</strong>随机将15%的文本mask，利用图像和文本的上下文还原mask的区域，使用交叉熵损失。为了避免脏数据的影响，使用的是伪标签。具体而言存在teacher模型，从student模型采用指数滑动平均(Exponential Moving Average, EMA)的方式更新其参数，而student模型的学习标签是teacher模型预测的embedding。</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/5.png" alt="MLM任务loss"></p><h3 id="动量蒸馏"><a href="#动量蒸馏" class="headerlink" title="动量蒸馏"></a>动量蒸馏</h3><p>由于预训练的图像文本对基本都来自互联网，存在脏数据。一般表现在文本中包含图像中没有的信息，或者图像中包含文本没有的信息。对于MLM和ITC任务会有较大的影响。<strong>因此受启发于moco，使用动量更新的方式构建teacher网络，利用teacher网络产生的伪标签作为监督信号。</strong>具体的实现方式可以参考上面ITC和MLM的代码。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="评测场景"><a href="#评测场景" class="headerlink" title="评测场景"></a>评测场景</h3><p>预训练好的模型应用在了5个场景，用于评估效果:</p><ul><li><strong>Image-Text Retrieval:</strong> 包含两个任务，图像到文本的检索和文本到图像的检索</li><li><strong>Visual Entailment:</strong> 视觉推理任务，预测图片和文本的关系是包含、中立还是相反的。</li><li><strong>Visual Question Answering:</strong> 提供图片和问题，预测答案</li><li><strong>Natural Language for Visual Reasoning:</strong> 预测提供的文本是否描述了图片</li><li><strong>Visual Grounding:</strong> 在图像中定位出文本中描述的物体的坐标</li></ul><h3 id="Evaluation-on-the-Proposed-Methods"><a href="#Evaluation-on-the-Proposed-Methods" class="headerlink" title="Evaluation on the Proposed Methods"></a>Evaluation on the Proposed Methods</h3><p>展示了ALBEF使用不同模块在下游任务上的效果</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/6.png" alt=""></p><h3 id="Evaluation-on-Image-Text-Retrieval"><a href="#Evaluation-on-Image-Text-Retrieval" class="headerlink" title="Evaluation on Image-Text Retrieval"></a>Evaluation on Image-Text Retrieval</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/7.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/8.png" alt=""><br>上面的图分别展示了微调和zero-shot在图文检索任务和之前方法的对比</p><h3 id="Evaluation-on-VQA-NLVR-and-VE"><a href="#Evaluation-on-VQA-NLVR-and-VE" class="headerlink" title="Evaluation on VQA, NLVR, and VE"></a>Evaluation on VQA, NLVR, and VE</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/9.png" alt=""></p><p><strong>使用4M的图片做预训练取得了比之前模型更好的效果，并且数据集增大到14M性能普遍有进一步提升。并且由于没有使用检测器，并且输入图片分辨率更低，推理耗时比VILLA减少10倍</strong></p><h3 id="Weakly-supervised-Visual-Grounding"><a href="#Weakly-supervised-Visual-Grounding" class="headerlink" title="Weakly-supervised Visual Grounding"></a>Weakly-supervised Visual Grounding</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/10.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/11.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/12.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/13.png" alt=""></p><p>在不同任务上可视化cross attention的特征图，和文本描述的区域匹配</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/14.png" alt=""><br>研究了在文本-图像检索任务上困难负样本在图文检索任务上的影响</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/15.png" alt=""><br>在NLVR任务中研究了文本分配(text assignment, TA任务)和不同特征共享方式的影响，细节配置可以看文章的第五部分(Downstream V+L Tasks)</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>ALBEF比之前多模态的方法效果好有几个关键点:</p><ul><li><strong>在融合不同模态特征前，做了特征的对齐</strong></li><li><strong>使用momentum distillation避免了噪声数据在预训练的影响</strong></li><li><strong>区别于之前ITC任务使用了难负例</strong></li></ul><p><strong>并且由于没有目标检测器，下游任务使用时使用更小的图片输入，耗时会极大的缩小</strong></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="多模态" scheme="https://oysz2016.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>长尾优化汇总</title>
    <link href="https://oysz2016.github.io/post/8592ee55.html"/>
    <id>https://oysz2016.github.io/post/8592ee55.html</id>
    <published>2023-09-03T07:03:41.340Z</published>
    <updated>2022-12-05T01:09:45.458Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="经典的深度学习数据集"><a href="#经典的深度学习数据集" class="headerlink" title="经典的深度学习数据集"></a>经典的深度学习数据集</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295054598.png" alt="enter description here"><br>Mnist: 数据规模较小，10个类别<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295071148.png" alt="enter description here"><br>ImageNet: 百万数据量，1000个类别<br>两者的共同点：类别是均匀分布的</p><h2 id="真实场景中的深度学习任务"><a href="#真实场景中的深度学习任务" class="headerlink" title="真实场景中的深度学习任务"></a>真实场景中的深度学习任务</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295017948.png" alt="enter description here"></p><ul><li>类别不平衡是常态<h2 id="长尾问题"><a href="#长尾问题" class="headerlink" title="长尾问题"></a>长尾问题</h2><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295083966.png" alt="enter description here"><br>长尾问题常见的表现及原因 ：</li><li>头部类的效果好，尾部类的效果差</li><li>模型是数据驱动的，头部类的数据多，尾部类的数据少</li><li>尾部类数量少的同时可能造成类别中样本差异较大，网络学习不充分<h1 id="Resampling"><a href="#Resampling" class="headerlink" title="Resampling"></a>Resampling</h1><h2 id="BBN-Bilateral-Branch-Network-with-Cumulative-Learning-for-Long-Tailed-Visual-Recognition"><a href="#BBN-Bilateral-Branch-Network-with-Cumulative-Learning-for-Long-Tailed-Visual-Recognition" class="headerlink" title="BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition"></a>BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition</h2></li></ul><p><strong>论文链接：</strong><a href="https://arxiv.org/pdf/1912.02413.pdf">https://arxiv.org/pdf/1912.02413.pdf</a></p><p>two stage finetuning:</p><ul><li>第一阶段在原始不平衡的数据集上训练</li><li>第二阶段以一个很小的学习率使用resampling/reweighting的方法fintune</li></ul><p>根据two stage fintuning方法比只使用resampling/reweighting好的原因做了假设</p><ul><li>reblance的方法有效的原因在于提升了分类器的性能</li><li>会损害网络学习到的特征<br>为了验证上述假设，将网络拆解成了分类器和特征提取器两部分，分别实验:</li><li>在第一阶段使用交叉熵和resampling/reweighting训练整个网络</li><li>将第一阶段特征提取器的参数固定，使用交叉熵和resampling/reweighting训练分类器</li></ul><p>为了验证上述假设，将网络拆解成了分类器和特征提取器两部分，分别实验:</p><ul><li>在第一阶段使用交叉熵和resampling/reweighting训练整个网络</li><li>将第一阶段特征提取器的参数固定，使用交叉熵和resampling/reweighting训练分类器</li></ul><p>设计了一个<a href="https://arxiv.org/pdf/1912.02413.pdf">双分支的网络</a></p><p>主要思想是设计了一个<strong>two stage finetuning</strong>的训练方式</p><ul><li>第一阶段在原始不平衡的数据集上训练</li><li>第二阶段以一个很小的学习率使用resampling/reweighting的方法fintune</li></ul><p>根据two stage fintuning方法比只使用resampling/reweighting好的原因做了假设</p><ul><li>reblance的方法有效的原因在于提升了分类器的性能</li><li>会损害网络学习到的特征</li></ul><p>为了验证上述假设，将网络拆解成了分类器和特征提取器两部分，分别实验:</p><ul><li>在第一阶段使用交叉熵和resampling/reweighting训练整个网络</li><li>将第一阶段特征提取器的参数固定，使用交叉熵和resampling/reweighting训练分类器</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295186969.png" alt="enter description here"><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295194164.png" alt="enter description here"><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295201430.png" alt="enter description here"></p><p>总结下BBN思想中的<strong>关键点:</strong></p><ul><li>第一个采样器是一个公平的采样器</li><li>第二个采样器是一个resample的采样器</li><li>两个分支共享权重，减少参数量的同时，让第二个分支受益于第一个分支中更好的特征</li><li>使用一个adaptor的策略，调节两个分支在网络训练中的权重</li></ul><p>下面是添加BBN的结果和其他方法的对比:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295216148.png" alt="enter description here"><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295235292.png" alt="enter description here"></p><h1 id="Cost-sensitive-learning"><a href="#Cost-sensitive-learning" class="headerlink" title="Cost-sensitive learning"></a>Cost-sensitive learning</h1><h2 id="Class-Balanced-Loss-Based-on-Effective-Number-of-Samples"><a href="#Class-Balanced-Loss-Based-on-Effective-Number-of-Samples" class="headerlink" title="Class-Balanced Loss Based on Effective Number of Samples"></a>Class-Balanced Loss Based on Effective Number of Samples</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/pdf/1901.05555.pdf">https://arxiv.org/pdf/1901.05555.pdf</a><br>之前方法存在的问题：在reweighting等方法中，一般将样本数量的倒数作为该类别的权重，但是样本之间能提供的信息可能存在重合，简单通过样本数量判断权重会存在问题</p><ul><li>提出了一种计算有效样本的方法</li><li>用有效样本数代替原始的样本频率，再用其倒数对损失进行加权</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295283511.png" alt="enter description here"></p><p>有效样本的定义:</p><ul><li>样本的有效数据量是样本的期望体积，一个新的采样数据只能存在两种情况</li><li>新样本存在于之前样本中的概率为p</li><li>新样本不存在于之前样本中的概率为1-p</li></ul><p>提出有效样本的计算公式:</p><script type="math/tex; mode=display">E_{n}=\frac{1-\beta^{n}}{1-\beta}</script><p>当n=1时，<script type="math/tex">E_1=\frac{1-\beta^1}{1-\beta}=1</script><br>假设当n=k-1时成立,即<script type="math/tex">E_{k-1}=\frac{1-\beta^{k-1}}{1-\beta}</script>，<br>设样本的体积为K，已经采样的样本体积为<script type="math/tex">E_{k-1}</script>。则<script type="math/tex">p=\frac{E_{k-1}}{K}</script>,经过k次采样后，第k次采样时有以下情况:</p><ul><li>第k次采样和之前样本存在重叠的情况,则样本体积为<script type="math/tex">E_{k-1}</script></li><li>第k次采样是新的有效样本，与之前不存在重叠的情况,则样本体积为<script type="math/tex">E_{k-1}+1</script></li></ul><p>期望体积为:</p><script type="math/tex; mode=display">E_k=pE_{k-1}+(1-p)(E_{k-1}+1)=1+\frac{K-1}{K}E_{k-1}</script><p>其中</p><script type="math/tex; mode=display">\beta=\frac{K-1}{K}$$, 则$$E_k=1+\beta E_{k-1}=1+\beta \frac{1-\beta^{k-1}}{1-\beta}=\frac{1-\beta^{k}}{1-\beta}</script><p>则有效数据量是数据总量n的指数函数，超参数<script type="math/tex">\beta\in[0,1)</script><br>有效数据量<script type="math/tex">E_n</script>具有如下性质:</p><ul><li>当n很大时，有效数据量等于n</li><li>当n为1时，有效数据量为1</li></ul><p>CB-Loss:</p><script type="math/tex; mode=display">CB(p,y)=\frac{1}{E_{n_y}}L(p,y)=\frac{1-\beta}{1-\beta_{n_y}}L(p,y)</script><p>贴一下实验结果：<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295322950.png" alt="enter description here"></p><h1 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h1><h2 id="Feature-Space-Augmentation-for-Long-Tailed-Data"><a href="#Feature-Space-Augmentation-for-Long-Tailed-Data" class="headerlink" title="Feature Space Augmentation for Long-Tailed Data"></a>Feature Space Augmentation for Long-Tailed Data</h2><p><strong>论文链接：</strong><a href="https://arxiv.org/abs/2008.03673">https://arxiv.org/abs/2008.03673</a></p><p>常见的解决方法，如data manipulation和Balanced loss function design在提升长尾数据集模型性能的同时会损害特征表示能力<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295376341.png" alt="enter description here"></p><p>两个假设:</p><ul><li>头部类中类别无关的特征可以让尾部类特征更加丰富</li><li>高级特征空间具有更“线性”的表示，可以提取类通用和类特定的特征，并重新混合生成新的样本</li></ul><p>方法:<br>CAM(Class Activation Map)</p><script type="math/tex; mode=display">M_c(x,y)=\sum_{k}w_k^cf_k(x,y)</script><p>c: class; x,y: pixel position; k: channel; w: weight; f: feature，将M归一化到[0,1]，设定两个阈值。<script type="math/tex">0<\tau_s,\tau_t<1</script>。则类特定特征和类通用特征分别为:</p><ul><li><script type="math/tex; mode=display">M_c^s=sgn(M_c-\tau_s) \bigodot M_c</script></li><li><script type="math/tex; mode=display">M_c^g=sgn(\tau_g-M_c) \bigodot M_c</script></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295358501.png" alt="enter description here"><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295382285.png" alt="enter description here"></p><ul><li>正常训练得到特征提取网络和分类器，使用注意力机制CAM图做特征分解，将特征分为类别无关的特征和类别特定的特征。</li><li>两阶段训练，第一阶段正常训练，负责提取特征和cam，第二阶段是做尾部类的增广训练，分为两步<ul><li>网络输入一张头部类图片和一张尾部类图片，通过分类置信度排序，选择和当前尾部类距离最近的头部类特征，融合头部类中类通用特征和尾部类中类特征特征</li></ul></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295403120.png" alt="enter description here"></p><ul><li>使用增强的特征图微调FC分类器层</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295415204.png" alt="enter description here"></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="长尾优化" scheme="https://oysz2016.github.io/tags/%E9%95%BF%E5%B0%BE%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>简单线性回归</title>
    <link href="https://oysz2016.github.io/post/7fda15c2.html"/>
    <id>https://oysz2016.github.io/post/7fda15c2.html</id>
    <published>2023-09-03T07:03:41.337Z</published>
    <updated>2019-03-17T05:49:12.536Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>&emsp;&emsp;好长一段时间挺想重新系统的学习回顾下机器学习的知识，看了些深度学习的论文和框架后，觉得机器学习的知识确实太重要了。接下来的半年时间，想系统的总结实践下，也算是2019年的第一个flag吧。</p><h2 id="使用单一特征预测响应值"><a href="#使用单一特征预测响应值" class="headerlink" title="使用单一特征预测响应值"></a>使用单一特征预测响应值</h2><p>&emsp;&emsp;这是一种基于自变量值(X)来预测因变量值(Y)的方法。假设X和Y两个变量是线性相关的。线性回归就是尝试寻找一种根据特征或自变量(X)的线性函数来精确预测响应值(Y)。</p><h2 id="怎样找到最佳的拟合线"><a href="#怎样找到最佳的拟合线" class="headerlink" title="怎样找到最佳的拟合线"></a>怎样找到最佳的拟合线</h2><p>&emsp;&emsp;在这个回归任务中，我们将通过找到“最佳拟合线”来最小化预测误差——回归线应该尽量拟合X-Y的分布，即误差是最小的。例如$y_p$是预测值，$y_i$是实际值，这个过程就是使$y_p$和$y_i$之间的关系满足$min\{SUM(y_i-y_p)^2\}$</p><p>&emsp;&emsp;这里以学生分数数据集做这个实验，实验数据如下图所示：</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1547371646833.jpg" alt="enter description here"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ol><li>导入相关库</li><li>导入数据集</li><li>检查缺失数据</li><li>划分数据集</li><li>特征缩放(这里使用简单线性模型的相关库进行)</li></ol><h3 id="通过训练集来训练简单线性回回归模型"><a href="#通过训练集来训练简单线性回回归模型" class="headerlink" title="通过训练集来训练简单线性回回归模型"></a>通过训练集来训练简单线性回回归模型</h3><p>&emsp;&emsp;为了使用模型来训练数据集，这里使用python中的<code>sklearn.linear_model</code>库的<code>LinearRegression</code>类。然后实例化一个<code>LinearRegression</code>类的<code>regressor</code>对象。最后使用<code>LinearRegression</code>类的<code>fit()</code>方法。将<code>regressor</code>对象对数据集进行训练。</p><h3 id="预测结果"><a href="#预测结果" class="headerlink" title="预测结果"></a>预测结果</h3><p>&emsp;&emsp;现在将预测来自测试集的观察结果。将实际的输出保存在向量<code>Y_pred</code>中。使用前一步中训练的回归模型<code>regressor</code>的<code>LinearRegression</code>类的预测方法来对结果进行预测。</p><h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><p>&emsp;&emsp;为了直观的查看线性回归的效果，这里将对结果进行可视化。使用<code>matplotlib.pyplot</code>库对我们的训练结果和测试集结果做散点图，以查看模型的预测效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#python的数据处理库</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一步：数据预处理</span></span><br><span class="line">dataset = pd.read_csv(<span class="string">&#x27;../datasets/studentscores.csv&#x27;</span>)</span><br><span class="line">X = dataset.iloc[ : ,   : <span class="number">1</span> ].values</span><br><span class="line">Y = dataset.iloc[ : , <span class="number">1</span> ].values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = <span class="number">1</span>/<span class="number">4</span>, random_state = <span class="number">0</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步：训练集使用简单线性回归模型来训练</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor = regressor.fit(X_train, Y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步：预测结果</span></span><br><span class="line">Y_pred = regressor.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步：可视化</span></span><br><span class="line"><span class="comment"># 训练集结果可视化</span></span><br><span class="line">plt.scatter(X_train , Y_train, color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.plot(X_train , regressor.predict(X_train), color =<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集结果可视化</span></span><br><span class="line">plt.scatter(X_test , Y_test, color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.plot(X_test , regressor.predict(X_test), color =<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>&emsp;&emsp;训练集上的拟合结果：<br><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1547370582187.jpg" alt="enter description here"></p><p>&emsp;&emsp;测试集上拟合结果<br><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1547370582188.jpg" alt="enter description here"></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://oysz2016.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://oysz2016.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>毕业</title>
    <link href="https://oysz2016.github.io/post/9b1b667b.html"/>
    <id>https://oysz2016.github.io/post/9b1b667b.html</id>
    <published>2023-09-03T07:03:41.333Z</published>
    <updated>2022-11-27T02:15:17.551Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="25f8fa2490eda42dc29daad69fdd94355436c1f9fe26a9d737a862bdf5b72306">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">Here&#39;s something encrypted, password is required to continue reading.</summary>
    
    
    
    <category term="随笔" scheme="https://oysz2016.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="https://oysz2016.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>极简人类史</title>
    <link href="https://oysz2016.github.io/post/ad14e7f5.html"/>
    <id>https://oysz2016.github.io/post/ad14e7f5.html</id>
    <published>2023-09-03T07:03:41.314Z</published>
    <updated>2019-03-17T05:49:07.832Z</updated>
    
    <content type="html"><![CDATA[<p class="description"></p><span id="more"></span><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>最近一段时间开始对历史感兴趣起来，看历史的时候能获得一些力量和安慰，一段历史的厚重感和其中的伟人能让人肃然起敬，现在面临的一些不顺与一段历史中被命运捉弄的人来说也变得不值一提。趁着即将工作的间隙，追完了一部电视剧《大明王朝1566》，看完了《极简人类史》和另一本还在看的《人类简史》。<br>《大明王朝1566》这部剧给我的感受是厚重且华丽，是我看过的最棒的国产剧了。剧中大量运用了黑白 闪回，与前一刻的剧情形成反差，或重复加强的效果，有不少闪回表现的人物内心活动。有一篇关于该剧的评论<a href="https://movie.douban.com/review/1296072/">《大明王朝1566》：太极·利剑·雪</a>，太极那部分分析的很好。<br>《极简人类史》以时间为轴线，主要介绍了人类发展历史中的三个阶段，脉络清晰，书中没有细讲一个国家或一位伟人，但描述了人类历史一步步的发展轨迹，讲述的是一部简洁易懂的人类历史。</p><h2 id="前传"><a href="#前传" class="headerlink" title="前传"></a>前传</h2><p>宇宙史是比人类历史更大的一个范畴。这一部分书中也只是简短的介绍，关于这部分看书中的描述只能有个大致的时间概念和各阶段发生的事情。有许多制作精良的纪录片绝对能带来比看书更好的体验。<br>这里放一个简短的科普视频。</p><iframe id = "video" height="415" width="544" align="MIDDLE" src="//player.bilibili.com/player.html?aid=9340850&cid=15435812&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="false" type="application/x-shockwave-flash" pluginspage="//www.adobe.com/shockwave/download/download.cgi?P1_Prod_Version=ShockwaveFlash"> </iframe><h2 id="采集狩猎时代"><a href="#采集狩猎时代" class="headerlink" title="采集狩猎时代"></a>采集狩猎时代</h2><p>首先看看采集狩猎的定义：</p><blockquote><p>采集狩猎时代是人类历史中这样一个时代：整个人类社会依靠采集或狩猎，而不是通过种植或制造，来获取食物和其他必需品。此时的人类被称为“采集狩猎者”。这个时代也被称作“旧石器时代”。采集狩猎时代是人类历史上的第一个时代，也是迄今为止最长的时代，这是为人类历史奠定基础的时代。<br>采集狩猎者始于25万年前，独特的文化和技术创新，将他们的生活方式与其他非人类物种区分开来</p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1525925266504.jpg" alt="enter description here"></p><p>采集狩猎时代距今很遥远，研究那个时代的学者采用三种截然不同的证据：</p><ol><li>远古社会留下的物质遗迹。如石器、制作品或者事物残渣。如对牙齿的细致研究，可以获知早期人类日常的饮食信息；男女之间骨骼的大小，可以反映两性关系；研究海床和数万年前形成的冰盖中提取的划分和果核样本，考古学家能重构当时的气候和环境变化模型。</li><li>研究现代采集狩猎部落。</li><li>基于现代基因差异进行对比研究。基因研究可以测定现代族群之间的基因差异程度，帮助预估自己族群的历史。</li></ol><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1525925761140.jpg" alt="enter description here"></p><h3 id="采集狩猎的生活方式"><a href="#采集狩猎的生活方式" class="headerlink" title="采集狩猎的生活方式"></a>采集狩猎的生活方式</h3><ul><li>生产力水平低下的，那时人类每天从自然环境中获取的热量很难超过3000卡路里，而这是一个成年人类维持基本生存所必须的能量。</li><li>人口密度低，平均每平方公里不足1人。我查阅了资料，中国人口平均密度目前是每平方公里130人，但人口分布及不平衡。东部沿海地区，每平方公里超过400人，中部地区每平方公里200多人，而西部高原地区，每平方公里不足10人。香港旺角是世界人口最密集的地方，每平方公里有13万多人！！！（真正意义上的寸土寸金）。<h4 id="亲缘关系"><a href="#亲缘关系" class="headerlink" title="亲缘关系"></a>亲缘关系</h4>狩猎时代几乎所有的人类部族都鼓励与外族通婚，能确保邻近族群之间的团结意识和语言之间的相互重叠。<h4 id="生活水平"><a href="#生活水平" class="headerlink" title="生活水平"></a>生活水平</h4>与如今的人们没有私有财产就是贫穷的标志不同，采集狩猎时代随时从周围环境获取生存所需的的物质，不积累财富。因此生活在温带地区的采集狩猎者生活水平相对较高，因为他们饮食多种多样，免受饥荒的困扰。<h4 id="生活闲适，但生命短暂"><a href="#生活闲适，但生命短暂" class="headerlink" title="生活闲适，但生命短暂"></a>生活闲适，但生命短暂</h4>采集狩猎者居住的小型社会使他们和流行疾病隔离开，频繁的迁移活动也避免了招致病害虫的垃圾堆积。但生活艰苦，平均寿命可能低于30岁左右（由于婴儿死亡率高，意外事故和人为暴力）。<br>最终，采集狩猎时代技术发展的足够高潮，使某些地区的一些部落能更加深入，集中的利用当地资源。标志着迈向农耕社会。<h3 id="采集狩猎时代的重大变革"><a href="#采集狩猎时代的重大变革" class="headerlink" title="采集狩猎时代的重大变革"></a>采集狩猎时代的重大变革</h3></li><li>技术变革：出现了新的石器<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1527953031918.jpg" alt="enter description here"></li><li>向非洲以外地区迁徙：向东向西迁徙，来到亚欧大陆更偏南、更温暖的地区。这里面有几个大事件，一个是距今5.5万年至4万年，人类出现在冰河时代的澳大利亚，这被视为技术创新的明显标志，因为抵达澳大利亚大陆需要高超复杂的航海技术。还有一个是距今3万年前出现在西伯利亚，在这里生存需要捕获大型哺乳动物（鹿、马和猛犸），表明掌握了高超的狩猎技术。</li><li>人类对环境的影响：许多大型动物灭绝，刀耕火种</li></ul><h2 id="人类历史的开端"><a href="#人类历史的开端" class="headerlink" title="人类历史的开端"></a>人类历史的开端</h2><p>关于人类历史开端的问题，一直有着争议。主要存在两种假说：</p><ul><li>多地起源模式，这种模式的证据来自对骨骼遗迹的对比研究。</li><li>走出非洲假说，主要依赖于现代人类的基因对比。</li></ul><p>作为一个没有查阅过人类起源的门外汉，如今听到的比较多的关于人类起源的说法是“走出非洲假说”，得益于基因检测技术的发展，这种假说的可信度越来越高。</p><h2 id="农耕时代"><a href="#农耕时代" class="headerlink" title="农耕时代"></a>农耕时代</h2><p>距今1.1万年至1万年前，农耕社会诞生了。直到近250年，工业革命的开始，农耕社会才走向消亡。虽然和长达25万年的狩猎时代相比，农耕时代才延续了1万年。但迄今为止，70%的人类成员都生活在农耕社会。<br>从生态学讲，农业能比采集狩猎更有效率地获取自然界通过光合作用存储的能量与资源。农业通过砍伐森林、使河流改道、开垦山坡和耕种土地，农业耕种者极大地改变了地球的面貌，使其变得更受人类活动控制。</p><h3 id="农耕时代的最早证据"><a href="#农耕时代的最早证据" class="headerlink" title="农耕时代的最早证据"></a>农耕时代的最早证据</h3><p>迄今为止，对农业诞生的源头仍缺乏令人信服的解释。有趣的是在直至公元前1500年，几个“世界区域”——非洲、亚欧大陆、美洲和澳大利亚记忆太平洋各岛屿完全没有联系，但却相继进入了农耕时代。正是大家在没有交流的情况下，相继进入了农耕时代，这推翻了农业是一项绝妙发明的观点。而且现代的狩猎者反对进入农耕时代，有种猜想是早期的农耕者并非心甘情愿的接受这种生活方式，而是被迫接受！</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528164538191.jpg" alt="enter description here"></p><h3 id="总体特点和长期趋势"><a href="#总体特点和长期趋势" class="headerlink" title="总体特点和长期趋势"></a>总体特点和长期趋势</h3><p>农耕时代具有超乎寻常的文化多样性，农耕部落之间共享着一些重要的特征，这些特征确保了农耕时代的延续。</p><ul><li>以村庄为基础的社会构成：都需要家庭内部和家庭之间的协同合作，都需要处理与外部族群之间的关系。</li><li>人口活力增强：世界人口由1万年前的600万增长到1750年现代社会初期的7.7亿</li><li>技术创新加速：本地人口压力、新环境的扩张和不断增长的思想和贸易交流促使农耕技术不断进步。</li><li>农副产品革命：纤维、奶和肥料</li><li>水利技术</li><li>流行性疾病：相比于采集狩猎时代没有流行疾病的优势，农业时代的定居且人口多、与牲畜的密切接触导致流行性疾病的产生。</li><li><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528165676516.jpg" alt="enter description here"></li><li>权力等级：为了控制不断增加的宝贵粮食库存，冲突时有发生，导致了新形势的社会不公，形成了新的权力体系。</li></ul><h3 id="城市出现之前的农业社会"><a href="#城市出现之前的农业社会" class="headerlink" title="城市出现之前的农业社会"></a>城市出现之前的农业社会</h3><p>这个时代已经有了农耕部落，但尚未出现大型城市和国家。在非洲和亚欧大陆地区，这一时期从约公元前8000年延续到公元前3000年。在美洲，这一时期开始的晚，持续时间也更长。太平洋和太平洋岛屿，这一时期延续至现代。</p><ul><li>村庄组成的世界</li><li>等级制度出现：由于部落的扩大，人们需要定义自己与邻里关系。司法、战事、贸易和宗教等都需要人管理。有着政治和经济制度。</li><li>早期妇女地位限制：女性通常没有机会承担专业化的角色，随着部落间竞争加剧，男性开始垄断暴力组织。</li></ul><h3 id="最早的城市和国家"><a href="#最早的城市和国家" class="headerlink" title="最早的城市和国家"></a>最早的城市和国家</h3><p>公元前3000年到公元前500年才是人类历史真正的开始时期。在非洲和亚欧低地区第一批城市和国家出现在公元前3000年左右，美洲出现在公元前1000年，而大洋洲在距今1000年左右，国家才出现在一些海岛（夏威夷和汤加）。国家出现的首要原因是不断增加的人口密度。</p><ul><li>农耕文明：随着国家模式的不断扩张，与其相关的制度和实践也固定下来，称为“农耕文明”</li><li>帝制国家：随着国家规模的扩大，独裁者掌控的众多城镇区域内形成了帝国体制。通过地方统治者直接或间接扩大了征税和管辖的区域。</li></ul><h3 id="农业、城市与帝国"><a href="#农业、城市与帝国" class="headerlink" title="农业、城市与帝国"></a>农业、城市与帝国</h3><p>在公元前500到1000年，随着世界各国人口增多、国家势力和数量的不断增长，交换网络的范围扩大。这期间诞生了很多王朝。</p><ul><li>非洲，亚欧大陆，最早的帝国是创建于公元前6世纪的波斯（现伊朗）王朝。该王朝掌控的区域达到其过往朝代最大疆域面积的5倍。在此后的 1500年里，类似规模的国家被称为帝国。</li><li>美洲，公元后第一个千年里，复杂的城邦体制与初创的帝国出现在中美洲。处于鼎盛时期的墨西哥特奥蒂瓦坎城，拥有超过10万人口，控制着跨越中美洲大部分地区的贸易网络。</li><li>农耕文明以外的地带，人口增长促使了新的阶层结构产生。在亚欧大陆人烟稀少的地区，匈奴人于公元前2世纪创立了帝国。</li></ul><h3 id="现代革命前夕的农业社会"><a href="#现代革命前夕的农业社会" class="headerlink" title="现代革命前夕的农业社会"></a>现代革命前夕的农业社会</h3><p>农耕时代的最后一个阶段，是1000年至1750年。该时期农耕文明传播到以往边缘化的区域，例如北美洲、非洲南部、中国西部地区。这一时期最为重要的变化就是世界主要地区在16世纪实现了统一。在此基础上，第一个全球交换网络诞生了。将数千年来从未来往的区域联系在一起，形成商业和知识协同，为现代社会的兴起发挥了至关重要的作用。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1529392312275.jpg" alt="enter description here"></p><h2 id="我们的世界——近现代"><a href="#我们的世界——近现代" class="headerlink" title="我们的世界——近现代"></a>我们的世界——近现代</h2><p>在人类史的三个时代中，近现代才维持了250年，确实最动荡不安的。近现代的主要特征如下:</p><ul><li>人口增长和生产力提高：1750年至2000年间，世界人口从7.7亿左右增长到近60亿，人均生产量也提高了9倍。</li><li>城镇扩展：在1500年，全球只有约50个城市剧名人口超过10万，到2000年，数千个城市的居民人口超过10万。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1529392663798.jpg" alt="enter description here"></li><li>日益复杂和强大的政府：人口的增长及人们间的相互关系，需要更复杂的管理方式运作。</li><li>日益增大的贫富差距</li><li>女性享有更多机遇</li><li>前现代生活方式的消亡：采集狩猎和农业耕作的生活方式都走向没落。<h3 id="工业革命"><a href="#工业革命" class="headerlink" title="工业革命"></a>工业革命</h3>工业革命从 1750年到1914年。起源于苏格兰发明家詹姆斯·瓦特改良的蒸汽机以及第一列机车。工业革命的技术创新呈现波浪式发展态势，每一波都带来了新的生产力提升技术，并将工业化进程扩展到新的区域。工业革命带来了以下改变：</li><li>经济发展：从1820年至1913年，英国国内生产总值增长了6倍，德国增长了9倍，美国增长了41倍。与此同时，中国和印度等传统农耕社会受到了冲击，中国国内生产总值在世界的占比从33%下降至9%，印度从16%降至8%。</li><li>民主革命：<strong>经济基础决定上层建筑</strong>，产生了如法国大革命等变革。</li><li>文化变革：北美及欧洲大部分地区，大众教育将读写能力传授给大多数民众。所有的宗教传统此时都必须直面现代科学提出的挑战，例如达尔文提出的进化论对宗教的冲击。</li></ul><h3 id="20世纪危机"><a href="#20世纪危机" class="headerlink" title="20世纪危机"></a>20世纪危机</h3><p>从1913到1950年间，世界经济增长缓慢，曾经促进工业革命发展的国际金融业和贸易体系的崩溃是增速减缓的部分原因。各国将经济增长视为零和博弈，排挤市场中其他竞争对手。随后爆发了第一次世界大战，第一次世界大战将工业化战争的惊人规模和破坏力展现得淋漓尽致。在第一次世界大战后，德国出现了以西特勒为首的法西斯政权，俄国出现了由马克思主义指导，决心推翻资本主义的社会主义国家。20世纪30年代期间，第二次世界大战起源于日本和德国妄图创建各自的陆上帝国。第二次世界大战后，欧洲不再主导全球经济体系，美国和苏联成为新的超级大国。</p><h3 id="现代历史"><a href="#现代历史" class="headerlink" title="现代历史"></a>现代历史</h3><p>现代历史从1945年至今，第二次世界大战后，资本主义引擎再次轰鸣，早就了世界历史上最快的经济增速。美国的“马歇尔计划”提供了大规模 的重建援助资金，推动了全球 监管机构，如联合国（1945年）和国际货币基金组织（1947年）的成立，国际经济秩序回复 了稳定。在1945年后的40年间，大约有100个国家从欧洲领主手中取得了 独立，另一批新兴国家涌现于1991年苏联解体之后。<br>最后做一个总结：</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1529402706167.jpg" alt="enter description here"></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="读书笔记" scheme="https://oysz2016.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="读书笔记" scheme="https://oysz2016.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="历史" scheme="https://oysz2016.github.io/tags/%E5%8E%86%E5%8F%B2/"/>
    
  </entry>
  
  <entry>
    <title>数学公式语法——Mathjax教程</title>
    <link href="https://oysz2016.github.io/post/8611e6fb.html"/>
    <id>https://oysz2016.github.io/post/8611e6fb.html</id>
    <published>2023-09-03T07:03:41.305Z</published>
    <updated>2019-03-17T05:49:41.870Z</updated>
    
    <content type="html"><![CDATA[<p class="description"></p><span id="more"></span><p>&emsp;&emsp;在着手写博客前，喜欢在“印象笔记”上记录学习笔记，当时觉得“印象笔记”的富文本编辑器用着还挺顺手。在搭建博客开始学着用Markdown写作后，再看原来在“印象笔记”中的笔记，格式排版真是惨不忍睹，Markdown的使用很大程度上提升了写作效率，也统一了排版。这里顺便推荐一款Markdown的编辑器——<a href="http://soft.xiaoshujiang.com/">小书匠</a>，小书匠支持标准的Markdown语法，也具有强大的语法扩展功能，支持大多数图床和“印象笔记”等第三方存储。<br>&emsp;&emsp;在上一篇博客<a href="https://oysz2016.github.io/post/1b649e52.html">《Relation Networks for Object Detection》论文笔记</a>中由于论文中有不少公式需要介绍，又不想用图片代替影响阅读体验，好在Markdown支持Mathjax语法。但不得不说刚开始使用Mathjax编辑公式，还是很不习惯，几千字的博文，公式编辑花了很长时间。因此用这篇博文总结一下Mathjax的语法，搬砖的过程也让自己熟悉Mathjax。</p><h2 id="Mathjax简介"><a href="#Mathjax简介" class="headerlink" title="Mathjax简介"></a>Mathjax简介</h2><p>&emsp;&emsp;Mathjax是一款运行在浏览器中的开源数学符号渲染引擎，使用MathJax可以方便的在浏览器中显示数学公式，不需要使用图片。</p><h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><ul><li>在正文中同一行插入LaTeX公式用<code>$...$</code>定义<ul><li>例如语句为<code>$\sum_&#123;i=0&#125;^N\int_&#123;a&#125;^&#123;b&#125;g(t,i)\text&#123;d&#125;t$</code></li><li>显示为$\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$</li></ul></li><li>另起一行显示LaTeX公式用<code>$$...$$</code><ul><li>例如语句为<code>$$W_G^&#123;mn&#125;=max\&#123;0,W_G.\xi_G(f_G^m,f_G^n)\&#125;$$</code></li><li>显示为<script type="math/tex">W_G^{mn}=max\{0,W_G.\xi_G(f_G^m,f_G^n)\}</script><h2 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h2></li></ul></li></ul><div class="table-container"><table><thead><tr><th>显示</th><th>命令</th><th>显示</th><th>命令</th></tr></thead><tbody><tr><td>$\alpha$</td><td>\alpha</td><td>$\beta$</td><td>\beta</td></tr><tr><td>$\gamma$</td><td>\gamma</td><td>$\delta$</td><td>\delta</td></tr><tr><td>$\epsilon$</td><td>\epsilon</td><td>$\zeta$</td><td>\zeta</td></tr><tr><td>$\eta$</td><td>\eta</td><td>$\theta$</td><td>\theta</td></tr><tr><td>$\iota$</td><td>\iota</td><td>$\kappa$</td><td>\kappa</td></tr><tr><td>$\lambda$</td><td>\lambda</td><td>$\mu$</td><td>\mu</td></tr><tr><td>$\nu$</td><td>\nu</td><td>$\xi$</td><td>\xi</td></tr><tr><td>$\pi$</td><td>\pi</td><td>$\rho$</td><td>\rho</td></tr><tr><td>$\sigma$</td><td>\sigma</td><td>$\tau$</td><td>\tau</td></tr><tr><td>$\upsilon$</td><td>\upsilon</td><td>$\phi$</td><td>\phi</td></tr><tr><td>$\chi$</td><td>\chi</td><td>$\psi$</td><td>\psi</td></tr><tr><td>$\omega$</td><td>\omega</td></tr></tbody></table></div><ul><li>若需要大写希腊字母，将命令首字母大写即可。<code>$\gamma$</code>呈现为$\Gamma$ <ul><li>若需要斜体希腊字母，将命令前加上var前缀即可。<code>$\varGamma$</code>呈现为$\varGamma$</li></ul></li></ul><h2 id="关系运算符"><a href="#关系运算符" class="headerlink" title="关系运算符"></a>关系运算符</h2><div class="table-container"><table><thead><tr><th>显示</th><th>命令</th><th>显示</th><th>命令</th></tr></thead><tbody><tr><td>$\mid$</td><td>\mid</td><td>$\nmid$</td><td>\nmid</td></tr><tr><td>$\cdot$</td><td>\cdot</td><td>$\leq$</td><td>\leq</td></tr><tr><td>$\geq$</td><td>\geq</td><td>$\neq$</td><td>\neq</td></tr><tr><td>$\approx$</td><td>\approx</td><td>$\equiv$</td><td>\equiv</td></tr><tr><td>$\prec$</td><td>\prec</td><td>$\preceq$</td><td>\preceq</td></tr><tr><td>$\ll$</td><td>\ll</td><td>$\succ$</td><td>\succ</td></tr><tr><td>$\succeq$</td><td>\succeq</td><td>$\gg$</td><td>\gg</td></tr><tr><td>$\sim$</td><td>\sim</td><td>$\simeq$</td><td>\simeq</td></tr><tr><td>$\asymp$</td><td>\asymp</td><td>$\cong$</td><td>\cong</td></tr><tr><td>$\doteq$</td><td>\doteq</td><td>$\propto$</td><td>\propto</td></tr><tr><td>$\models$</td><td>\models</td><td>$\parallel$</td><td>\parallel</td></tr><tr><td>$\bowtie$</td><td>\bowtie</td><td>$\perp$</td><td>\perp</td></tr><tr><td>$\circ$</td><td>\circ</td><td>$\ast$</td><td>\ast</td></tr><tr><td>$\bigodot$</td><td>\bigodot</td><td>$\bigotimes$</td><td>\bigotimes</td></tr><tr><td>$\bigoplus$</td><td>\bigoplus</td><td></td></tr></tbody></table></div><h2 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h2><div class="table-container"><table><thead><tr><th>显示</th><th>命令</th><th>显示</th><th>命令</th></tr></thead><tbody><tr><td>$\pm$</td><td>\pm</td><td>$\mp$</td><td>\mp</td></tr><tr><td>$\times$</td><td>\times</td><td>$\ast$</td><td>\ast</td></tr><tr><td>$\star$</td><td>\star</td><td>$\circ$</td><td>\circ</td></tr><tr><td>$\bullet$</td><td>\bullet</td><td>$\cdot$</td><td>\cdot</td></tr><tr><td>$\div$</td><td>\div</td><td>$\sum$</td><td>\sum</td></tr><tr><td>$\prod$</td><td>\prod</td><td>$\coprod$</td><td>\coprod</td></tr><tr><td>$\oplus$</td><td>\oplus</td><td>$\bigoplus$</td><td>\bigoplus</td></tr><tr><td>$\ominus$</td><td>\ominus</td><td>$\otimes$</td><td>\otimes</td></tr><tr><td>$\bigotimes$</td><td>\bigotimes</td><td>$\oslash$</td><td>\oslash</td></tr><tr><td>$\odot$</td><td>\odot</td><td>$\bigodot$</td><td>\bigodot</td></tr><tr><td>$\diamond$</td><td>\diamond</td><td>$\bigtriangleup$</td><td>\bigtriangleup</td></tr><tr><td>$\bigtriangledown$</td><td>\bigtriangledown</td><td>$\triangleleft$</td><td>\triangleleft$</td></tr><tr><td>$\triangleright$</td><td>\triangleright</td><td>$\triangleright$</td><td>\triangleright</td></tr><tr><td>$\bigcirc$</td><td>\bigcirc</td></tr></tbody></table></div><h2 id="字母修饰"><a href="#字母修饰" class="headerlink" title="字母修饰"></a>字母修饰</h2><h3 id="上下标"><a href="#上下标" class="headerlink" title="上下标"></a>上下标</h3><ul><li>上标:<code>^</code></li><li>下标:<code>_</code></li><li>例如:<code>C_n^2</code>，显示为$C_n^2$<h2 id="矢量"><a href="#矢量" class="headerlink" title="矢量"></a>矢量</h2></li><li><code>\vec</code> a，显示为$\vec a$</li><li><code>\overrightarrow&#123;xy&#125;</code>，显示为:$\overrightarrow{xy}$<h2 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h2></li><li>打印机字体Typewriter：<code>\mathtt&#123;A&#125;</code>显示为$\mathtt{A}$</li><li>黑板粗体字Blackboard Bold：<code>\mathbb&#123;A&#125;</code>呈现为$\mathbb{A}$</li><li>无衬线字体Sans Serif：<code>\mathsf&#123;A&#125;</code>呈现为$\mathsf{A}$</li><li>手写体:<code>\mathscr&#123;A&#125;</code>呈现为$\mathscr{A}$</li><li>罗马字体:<code>\mathrm&#123;A&#125;</code>呈现为$\mathrm{A}$<h2 id="括号"><a href="#括号" class="headerlink" title="括号"></a>括号</h2></li><li>小括号:<code>()</code>，显示为()</li><li>中括号：<code>[]</code>，显示为[]</li><li>尖括号：<code>\langle</code>,<code>\rangle</code>呈现为⟨⟩ </li><li>自适应括号：<code>\left(...\right)</code>能使符号大小与邻近公式相适应<ul><li><code>(\frac&#123;x&#125;&#123;y&#125;)</code>，显示为$(\frac{x}{y})$</li><li><code>\left(\frac&#123;x&#125;&#123;y&#125;\right)</code>，显示为$\left(\frac{x}{y}\right)$<h2 id="求和、极限与积分"><a href="#求和、极限与积分" class="headerlink" title="求和、极限与积分"></a>求和、极限与积分</h2></li></ul></li><li>求和：<code>\sum</code><ul><li>举例：<code>\sum_&#123;i=1&#125;^n&#123;a_i&#125;</code>呈现为$\sum_{i=1}^n{a_i}$</li></ul></li><li>极限：<code>\lim</code><ul><li>举例:<code>\lim_&#123;x\to 0&#125;</code>呈现为$\lim_{x \to 0}$</li></ul></li><li>积分:<code>\int</code><ul><li>举例:<code>\int_0^xf(x)dx</code>呈现为$\int_0^xf(x)dx$</li></ul></li></ul><h2 id="分式与根式"><a href="#分式与根式" class="headerlink" title="分式与根式"></a>分式与根式</h2><ul><li>分式:<code>\frac</code><ul><li>举例:<code>\frac&#123;分子&#125;&#123;分母&#125;</code>呈现为$\frac{分子}{分母}$</li></ul></li><li>根式:<code>\sqrt</code><ul><li>举例：<code>\sqrt[x]&#123;y&#125;</code>呈现为$\sqrt[x]{y}$<h2 id="特殊函数"><a href="#特殊函数" class="headerlink" title="特殊函数"></a>特殊函数</h2></li></ul></li><li><code>\函数名</code><ul><li>举例:<code>\sin x</code>，<code>\ln x</code>，<code>\max(A,B,C)</code>呈现为$sin x$,$ln x$,$max(A,B,C)$<h2 id="空格"><a href="#空格" class="headerlink" title="空格"></a>空格</h2></li></ul></li><li>LaTex语法会忽略空格，需要用转义字符\<ul><li>小空格:<code>a\ b</code>呈现为$a\ b$</li><li>四个空格:<code>a\quad b</code>呈现为$a\quad b$<h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h2><h3 id="基本语法-1"><a href="#基本语法-1" class="headerlink" title="基本语法"></a>基本语法</h3></li></ul></li><li>起始标记<code>\begin&#123;matrix&#125;``，结束标记``\end&#123;matrix&#125;</code></li><li>每一行末尾标记\\，行间元素以$分割</li><li>举例<figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$$\begin<span class="punctuation">&#123;</span>matrix<span class="punctuation">&#125;</span></span><br><span class="line"><span class="number">1</span><span class="variable">&amp;0</span><span class="variable">&amp;0</span>\\</span><br><span class="line"><span class="number">0</span><span class="variable">&amp;1</span><span class="variable">&amp;0</span>\\</span><br><span class="line"><span class="number">0</span><span class="variable">&amp;0</span><span class="variable">&amp;1</span>\\</span><br><span class="line">\end<span class="punctuation">&#123;</span>matrix<span class="punctuation">&#125;</span>$$</span><br></pre></td></tr></table></figure>呈现为:<script type="math/tex">\begin{matrix}1&0&0\\0&1&0\\0&0&1\\\end{matrix}</script><h3 id="矩阵边框"><a href="#矩阵边框" class="headerlink" title="矩阵边框"></a>矩阵边框</h3></li><li>在起始、结束标记处用下列词替换matrix<ul><li>pmatrix：小括号边框</li><li>bmatrix：中括号边框</li><li>Bmatrix：大括号边框</li><li>vmatrix：单竖线边框</li><li>Vmatrix：双竖线边框<h3 id="省略元素"><a href="#省略元素" class="headerlink" title="省略元素"></a>省略元素</h3><ul><li>横省略号：\cdots</li><li>竖省略号：\vdots</li><li>斜省略号：\ddots</li><li>举例<figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml">$$\begin</span><span class="template-variable">&#123;bmatrix&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">&#123;a_&#123;11&#125;</span><span class="language-xml">&#125;&amp;</span><span class="template-variable">&#123;a_&#123;12&#125;</span><span class="language-xml">&#125;&amp;</span><span class="template-variable">&#123;\cdots&#125;</span><span class="language-xml">&amp;</span><span class="template-variable">&#123;a_&#123;1n&#125;</span><span class="language-xml">&#125;\\</span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">&#123;a_&#123;21&#125;</span><span class="language-xml">&#125;&amp;</span><span class="template-variable">&#123;a_&#123;22&#125;</span><span class="language-xml">&#125;&amp;</span><span class="template-variable">&#123;\cdots&#125;</span><span class="language-xml">&amp;</span><span class="template-variable">&#123;a_&#123;2n&#125;</span><span class="language-xml">&#125;\\</span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">&#123;\vdots&#125;</span><span class="language-xml">&amp;</span><span class="template-variable">&#123;\vdots&#125;</span><span class="language-xml">&amp;</span><span class="template-variable">&#123;\ddots&#125;</span><span class="language-xml">&amp;</span><span class="template-variable">&#123;\vdots&#125;</span><span class="language-xml">\\</span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">&#123;a_&#123;m1&#125;</span><span class="language-xml">&#125;&amp;</span><span class="template-variable">&#123;a_&#123;m2&#125;</span><span class="language-xml">&#125;&amp;</span><span class="template-variable">&#123;\cdots&#125;</span><span class="language-xml">&amp;</span><span class="template-variable">&#123;a_&#123;mn&#125;</span><span class="language-xml">&#125;\\</span></span><br><span class="line"><span class="language-xml">\end</span><span class="template-variable">&#123;bmatrix&#125;</span><span class="language-xml">$$</span></span><br></pre></td></tr></table></figure>呈现为:<script type="math/tex">\begin{bmatrix}{a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\{a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{a_{m1}}&{a_{m2}}&{\cdots}&{a_{mn}}\\\end{bmatrix}</script><h2 id="方程组"><a href="#方程组" class="headerlink" title="方程组"></a>方程组</h2></li><li>需要cases环境：起始、结束处以{cases}声明</li><li>举例<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$$\begin&#123;cases&#125;</span><br><span class="line">a_1x+b_1y+<span class="attribute">c_1z</span>=d_1\\</span><br><span class="line">a_2x+b_2y+<span class="attribute">c_2z</span>=d_2\\</span><br><span class="line">a_3x+b_3y+<span class="attribute">c_3z</span>=d_3\\</span><br><span class="line">\end&#123;cases&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{cases}a_1x+b_1y+c_1z=d_1\\a_2x+b_2y+c_2z=d_2\\a_3x+b_3y+c_3z=d_3\\\end{cases}</script></li></ul></li></ul></li></ul><h2 id="公式编号"><a href="#公式编号" class="headerlink" title="公式编号"></a>公式编号</h2><ul><li>用<code>\tag&#123;n&#125;</code>标签</li><li>举例<code>f(x)=x\tag&#123;1&#125;</code>显示为$f(x)=x\tag{1}$</li></ul><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>&emsp;&emsp;以上列举的都是常用的Mathjax语法，以后有用到新的会继续补充。</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Hexo博客搭建教程" scheme="https://oysz2016.github.io/categories/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="Hexo" scheme="https://oysz2016.github.io/tags/Hexo/"/>
    
    <category term="Mathjax" scheme="https://oysz2016.github.io/tags/Mathjax/"/>
    
  </entry>
  
  <entry>
    <title>扩散模型汇总——从DDPM到DALLE2</title>
    <link href="https://oysz2016.github.io/post/d10097db.html"/>
    <id>https://oysz2016.github.io/post/d10097db.html</id>
    <published>2023-09-03T07:03:41.289Z</published>
    <updated>2022-12-25T09:20:29.682Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><p>扩散模型最早是在2015年在<a href="https://arxiv.org/abs/1503.03585">Deep unsupervised learning using nonequilibrium thermodynamics</a>中提出，<strong>其目的是消除对训练图像连续应用的高斯噪声</strong>，可以将其视为一系列<strong>去噪自编码器</strong>。它使用了一种被称为“潜在扩散模型”（latent diffusion model; LDM）的变体。训练自动编码器将图像转换为低维潜在空间。随后在2020年提出的DDPM将扩散模型的思想用于图像生成。</p><p><strong>生成模型</strong>: 给定来自感兴趣分布的观察样本x，生成模型的目标是学习对其真实数据分布$p(x)$进行建模<br><strong>隐变量(latent Variable)</strong>: 对于许多模态，我们可以将我们观察到的数据视为由相关的看不见潜在变量生成的，我们可以用随机变量 z 表示。为什么能用看不见的潜在变量表示，感性的理解可以参考柏拉图洞穴的寓言。在这个寓言中，一群人一生都被锁在一个山洞里，只能看到投射在他们面前的墙上的二维阴影，这是由看不见的三维物体在火前经过而产生的。对于这样的人来说，他们所观察到的一切，实际上都是由他们永远看不到的更高维度的抽象概念决定的。</p><p>生成模型发展到如今有下图中几种流派，从下面的算法结构图可以看出Diffusion model相比其他方法<strong>有比较大的不同:</strong></p><ul><li><strong>Diffusion model的算法过程中的latent Variable维度都是相同的</strong></li><li><strong>存在一个前向和反向的过程</strong></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/1.png" alt=""></p><p>从以上的不同出发，可以推测出Diffusion model包含两个过程：分别是<strong>前向过程</strong>和<strong>反向过程</strong><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/2.png" alt=""></p><h3 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h3><p>前向过程在论文中也称为<strong>扩散过程(diffusion process)</strong>，<strong>是向数据随机添加噪声，直至原始图像整个变成随机噪声的过程</strong>，这个过程记为$x_o~q(x_o)$<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/3.png" alt=""></p><h3 id="反向过程"><a href="#反向过程" class="headerlink" title="反向过程"></a>反向过程</h3><p>反向过程是<strong>前向过程的反转</strong>，反向过程的<strong>目的是将随机噪声的分布，逐渐去噪生成真实的样本。</strong>反向过程实际上也是生成数据的过程。将该过程的表达式为:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/4.png" alt=""></p><h2 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a><br><strong>代码链接:</strong> <a href="https://github.com/hojonathanho/diffusion">https://github.com/hojonathanho/diffusion</a></p><p>在DPM中原始的扩散模型，在反向过程中是用$x_{t-1}$ 预测 $x_{t}$，而<strong>DDPM预测的是从t时刻到t-1时刻添加的噪声</strong>，只要减去添加的噪声，同样也能得到$x_{t}$时刻的特征。</p><p>由于在从噪声恢复到目标图像的过程中，特征维度是一致的，在DDPM中采用的是U-Net的结构，在T步的反向过程中，U-Net模型是参数共享的，为了能告知U-Net模型现在是反向传播的第几步，在每一步反向传播时会增加一个<strong>time embedding</strong>，其实现和transformer中的position embedding相似</p><p>在反向过程中预测的噪声都是符合正态分布的，也就是只用拟合噪声的均值和方差就可以预测出噪声，在DDPM中将方差固定为常数，只预测均值</p><p>DDPM的原理到这里基本就介绍完了。关于论文中扩散模型正向和反向过程都是符合高斯分布的，可以做比较详尽的推理和证明，对正向和反向推理过程感兴趣的同学可以看看论文或者其他blog中的推导</p><h2 id="improved-DDPM"><a href="#improved-DDPM" class="headerlink" title="improved DDPM"></a>improved DDPM</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2102.09672">https://arxiv.org/abs/2102.09672</a><br><strong>代码链接:</strong> <a href="https://github.com/openai/improved-diffusion">https://github.com/openai/improved-diffusion</a><br>从论文的名字可以看出主要是对DDPM做的改进，<strong>主要介绍下和DDPM的差异</strong></p><ul><li>将DDPM中用常数指代的方差，用模型学习了</li><li>将添加噪声的schedule改了，从线性的改成了余弦的<br>作者发现DDPM中线性的噪声schedule在高分辨率的图像生成中表现较好，但对于分辨率比较低的，例如64<em>64和32</em>32的图像任务中表现的不那么好。特别是在扩散过程的后期，最后的几步噪声过大，对样本质量的贡献不大。从Figure 3可以看出，cosine schedule的方法在每一步添加的噪声后相比之前图片都有一些差异，而linear schedule方法，在后期几步差异已经不大了。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/5.png" alt=""><br>我也用coco的数据集试了下，确实cosine比linear要合理些。<br>下图展示的是前向过程，迭代不同次数的结果</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/7.png" alt=""></p><p>下面以mnist数据集为例，展示反向过程，迭代不同次数的结果</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/7_2.png" alt=""></p><h2 id="Diffusion-models-beat-GAN-on-image-Synthesis"><a href="#Diffusion-models-beat-GAN-on-image-Synthesis" class="headerlink" title="Diffusion models beat GAN on image Synthesis"></a>Diffusion models beat GAN on image Synthesis</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2105.05233">https://arxiv.org/abs/2105.05233</a><br><strong>代码链接:</strong> <a href="https://github.com/openai/guided-diffusion">https://github.com/openai/guided-diffusion</a></p><p><strong>和之前方法的差异</strong></p><ul><li><strong>从GAN的实验中得到启发，对扩散模型进行了大量的消融实验，找到了更好的架构更深更宽的模型</strong></li><li><strong>用了classifier guider diffusion</strong></li></ul><h3 id="网络结构消融实验"><a href="#网络结构消融实验" class="headerlink" title="网络结构消融实验"></a>网络结构消融实验</h3><p>文中使用的基础模型是U-Net加一个单头全局注意力模块，以FID为评价指标，在ImageNet128<em>128上进行消融实验。<br>作者从模型的<strong>宽度(channels)</strong>、<strong>深度(depth)</strong>、<strong>注意力头的数量(heads)</strong>、<strong>注意力的分辨率(attention resolutions)</strong>、<em>*使用BigGAN的上/下采样激活(BigGAN-up/downsample)</em></em>、调整残差连接的权重(rescale-resblock)等方面进行了消融实验</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/9.png" alt=""></p><p>从上表中可以看到，加宽和加深网络都能带来明显的提升，增加注意力头的数量、使用多分辨率组合的注意力模块比只使用单头单一分辨率更有助于提升模型表现，BigGAN的上下采样也能提升模型表现。<strong>唯独修改残差连接的权重没有带来提升。</strong></p><p>虽然增加深度能带来模型性能的提升，但也会增加训练时间，并且需要更长时间才能拟合到一个一般结构模型能达到的效果。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/10.png" alt=""></p><p>另外通过Table 1的实验，作者使用了Channels为128，2个残差块，高分辨率的attention和使用BigGan中的上下采样激活。进一步探究注意力头的数量和每个注意力头通道数间的关系<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/11.png" alt=""></p><p>通过上表的结果进一步证明，<strong>注意力头的数量能提升性能，但每个注意力头的通道数并不是越多越好</strong></p><h3 id="Classifier-Guidance"><a href="#Classifier-Guidance" class="headerlink" title="Classifier Guidance"></a>Classifier Guidance</h3><p>作者受目前GAN方法里通常会使用的类别信息辅助图像生成的原理启发，开发了一个<strong>将类别信息引入扩散模型中的方法Classifier Guidance Diffusion</strong>，这个方法通俗的说是会训练一个图片分类器，在扩散模型的生成过程中的中间的latend code会<strong>通过分类器计算得到一个梯度，该梯度会指导扩散模型的迭代过程</strong>。其实这一操作也比较make sense，有一个分类器的存在能更好的告诉U-Net的模型在反向过程生成新图片的时候，当前图片有多像需要生成的物体。<strong>有点类似GAN中存在一个判别器的意思</strong>。</p><p>在论文中提到使用Classifier Guidance的技术<strong>能更好的生成逼真的图像</strong>，同时能加速图像生成的速度。论文中也提到，通过使用Classifier Guidance的track会牺牲掉一部分的多样性，换取图片的真实性</p><h2 id="Classifier-Free-Diffusion-Guidance"><a href="#Classifier-Free-Diffusion-Guidance" class="headerlink" title="Classifier-Free Diffusion Guidance"></a>Classifier-Free Diffusion Guidance</h2><ul><li>论文链接: <a href="https://arxiv.org/abs/2207.12598">https://arxiv.org/abs/2207.12598</a></li></ul><p>这篇论文的主要贡献是优化了Openai在《Diffusion models beat GAN on image Synthesis》中提出的Classifier Guidance，<strong>在Classifier Guidance中提出用另外一个模型做引导，需要用预训练的模型或者额外训练一个模型。不仅成本比较高而且训练的过程是不可控的。</strong></p><p>而这篇论文的方法研究的是没有分类器，也可以用生成模型自己做引导，所以起名叫“Classifier-Free Diffusion Guidance”。具体来说在该方法中联合训练了conditional和unconditional的扩散模型，并且结合了两个模型的score estimates，以实现样本质量和多样性之间的均衡。下图的Algorithms1和Algorithms2详细描述了Classifier-Free的做法。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/12.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/13.png" alt=""></p><p>最终模型的输出为下面的公式:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/14.png" alt=""><br>最终的输出为有条件生成的输出减去无条件生成的输出，看到这里个人感觉和之前介绍的两篇关于因果分析的论文<a href="https://oysz2016.github.io/post/b4822109.html">《Unbiased Scene Graph Generation from Biased Training》</a>和<a href="https://oysz2016.github.io/post/3a13345e.html">《Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect》</a>思路十分相似，<strong>可以将无条件生成的输出看作是偏差，用正常训练的网络减去有偏差的网络能得到想要的输出</strong>。回到这篇论文的思路，有条件生成的可以看作是用了和图片匹配的文本对c，而无条件生成将其中的文本对c置为了空集。其中w是超参数，用来条件有条件和无条件生成两者的比例，实验部分有该参数对性能的详细对比，以及和其他方法的对比</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/15.png" alt=""></p><p><strong>从下面的结果图可以看出Classifier-Free Guidance相比不用non-guided的方法多样性会有些损失，但图像的真实性和色彩饱和度是要更好的</strong></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/16.png" alt=""></p><p>值得一提的是，虽然Classifier-Free Guidance的方法没有引入新的模型，但方法本身仍然是”<strong>昂贵的</strong>“，因为训练的时候需要生成两个输出。在扩散模型本身就很慢的情况下，会进一步增加耗时</p><h2 id="GLIDE"><a href="#GLIDE" class="headerlink" title="GLIDE"></a>GLIDE</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2112.10741">https://arxiv.org/abs/2112.10741</a><br><strong>代码链接:</strong> <a href="https://github.com/openai/glide-text2im">https://github.com/openai/glide-text2im</a><br>在前面扩散模型的一系列进展之后，尤其是当guidance技术之后证明扩散模型也能生成高质量的图像后。Openai开始探索文本条件下的图像生成，并在这篇论文里对比了两种不同的guidance策略，分别是通过<strong>CLIP引导</strong>和<strong>classifier-free的引导</strong>。验证了classifier-free的方式生成的图片更真实，与提示的文本有更好的相关性。并且使用classifier-free的引导的GLIDE模型在35亿参数的情况下优于120亿参数的DALL-E模型</p><p>该方法沿袭了Openai一贯的做法，什么模块效果好就用什么，然后进一步增加模型的参数量和数据量。具体而言:</p><ul><li>使用了更大的模型，其中模型的结构和《Classifier-Free Diffusion Guidance》方法中的模型结构一样，不过增大了通道数，参数量达到了35亿</li><li>更多的数据，和Dalle相同的图像-文本对</li><li>更充分的训练，2048的batch size，迭代了250万次</li></ul><p><strong>GLIDE最大的贡献是开始用文本作为条件引导图像的生成</strong>，下图是其训练过程，和之前工作差异主要有以下几点：</p><ul><li><strong>分词后将文本送入transformer（bert），生成文本的embedding</strong></li><li><strong>文本embedding中最后一个token的特征作为扩散模型中classifier-free guidance中的条件c</strong></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/17.gif" alt=""></p><p>GLIDE的效果确实十分惊艳，图片非常真实而且有很多细节。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/18.png" alt=""></p><h2 id="DALLE2"><a href="#DALLE2" class="headerlink" title="DALLE2"></a>DALLE2</h2><p><strong>论文链接:</strong> <a href="https://cdn.openai.com/papers/dall-e-2.pdf">https://cdn.openai.com/papers/dall-e-2.pdf</a><br><strong>代码链接:</strong> <a href="https://github.com/lucidrains/DALLE2-pytorch">https://github.com/lucidrains/DALLE2-pytorch</a><br>如果说前面所提到的方法将扩散模型优化到比同期gan模型指标还要好，让研究人员看到了扩散模型在生成领域的前景，那么Dalle2则将扩散模型引入了公众视野。</p><p>在GLIDE取得成功之后，Openai又进一步在GLIDE上加了一些track，成为了Dalle2。dalle2的结构如下图所示：</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/19.png" alt=""></p><p>上面的图中将有一条分割线，分割线的上半部分代表CLIP模型，下半部分代表DALLE2。<br>为了更好的理解DALLE2，回顾一下CLIP模型，CLIP模型是有一个图像-文本对，文本通过一个text encoder得到文本特征，图像通过image encoder得到图像特征。他们两者就是一对正样本，而该文本跟其他的图像就构成负样本。<br>在Dalle2中CLIP模型没有经过进一步的训练，主要用处是用来根据文本生成文本特征，然后prior根据文本特征生成对应的图像特征，<strong>这一步很有意思，在论文中作者认为显式将图像特征建模出来，再用图像特征生成图像，会比直接通过文本特征生成图像效果要好。</strong></p><p>方法:<br>dalle2使用的数据和CLIP，Dalle，GLIDE一样，都是图像文本对(x，y)。x代表图像，y代表图像对应的文本，$z_i$代表CLIP模型输出的图像特征，$z_t$代表CLIP模型输出的文本特征。则Dalle2的网络结构由两部分组成:</p><ul><li>prior: 根据文本y生成图像特征$z_i$</li><li>decoder: 使用prior生成的$z_i$(对应的文本y，y可有可无)，生成图像x</li></ul><script type="math/tex; mode=display">P(x|y)=P(x，z_i|y)=P(x|z_i，y)P(z_i|y)</script><p>作者用公式证明了可以通过两阶段方式生成图像的原因。$P(X|Y)$代表要用文本生成图像，可以等价于$P(x，z_i|y)$，因为可以认为x和图像特征$z_i$是一一对应的，可以根据链式法则等价于$P(z_i|y)$。</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder部分的模型结构和GLIDE基本一致，使用了CLIP模型作为guidance，也使用了classifier-free guidance，并且classifier-free中的guidance有两种，一种是CLIP模型，另外一种是文本。<br>使用了级联式的生成，即生成的图片先从64<em>64到256</em>256，再到1024*1024</p><h3 id="Prior"><a href="#Prior" class="headerlink" title="Prior"></a>Prior</h3><p>prior模型的任务是从文本特征生成图像特征，这里有两种常用的方法</p><ul><li>Auto-regressive</li><li>扩散模型</li></ul><p>但是自回归的模型训练效率比较低，所以DALLE2的方法中使用的是扩散模型。在prior模型里也是用到了classifier-free guidance。在模型实现上使用的是transformer的decoder，模型的输入非常多，包含文本、CLIP的text embedding、扩散模型中常见的time step的embedding，还有加过噪声之后CLIP的image embedding；输出则预测没有噪声的CLIP的image embedding。和之前扩散模型不一样的地方在于没有使用DDPM中预测噪声的方式，而是直接还原每一步的图像</p><p>最后再贴一贴惊艳的效果<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/20.png" alt=""></p><h2 id="笔者总结"><a href="#笔者总结" class="headerlink" title="笔者总结"></a>笔者总结</h2><p>上面总结了各种各样的扩散模型，总体来说扩散模型的可解释性比GAN要好很多，也有很多数学公式可以证明。发展到如今，扩散模型慢慢的接替了GAN在生成领域头把交椅的地位。并且随着Dalle2的提出确实带来了无穷的想象力。在人工智能没有普及的年代，就有讨论随着人工智能的发展有哪些职业会被取代，但是基本上大家都觉得创造性的工作，是无法被取代的，因为创造性的工作没有固定的模式，大多数需要灵感。但目前来看扩散模型已经具有了一定的创造性，相信对大部分的认知都是有一定的冲击，也说明扩散模型确实是一个很有趣的研究方向。</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="扩散模型" scheme="https://oysz2016.github.io/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="生成模型" scheme="https://oysz2016.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>如何阅读一本书</title>
    <link href="https://oysz2016.github.io/post/43b16a18.html"/>
    <id>https://oysz2016.github.io/post/43b16a18.html</id>
    <published>2023-09-03T07:03:41.285Z</published>
    <updated>2019-03-17T05:49:31.052Z</updated>
    
    <content type="html"><![CDATA[<p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/封面.png?imageMogr2/thumbnail/!60p" alt="enter description here"><span id="more"></span></p><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>&emsp;&emsp;趁着五一的闲暇，在家里读完了《如何阅读一本书》。这本书很偏学术风，逻辑性很强，前两章引出了阅读的四个层次，往后的章节都是对前两章的补充。每一章节的结束基本都有对整章内容的总结。<br>&emsp;&emsp;作者在开篇提到，收音机、电视取代了以往由书本所提供的部分功能，就像照片取代了图画或艺术设计的部分功能一样。但不可否认，书籍的作用是不可取代的。这本书的初版是在1940年出版，距今有70多年。相比较于这版书出版的年代，90后的一代可以称为随着互联网成长起来的一代，因此拥有而且能适应更多的资讯获取方式，各种公众号以及自媒体会将整理好的信息推送给用户。利用碎片时间获取信息很有必要，也是一个良性的发展。然而很多有用的知识，是需要潜下心来系统的去看书，才能获取的。10多年的教育使大多数人都具备了阅读的能力，并且对如何去阅读一本书有了自己的习惯。我常常觉得我的习惯不太好，为了能在有限的时间，用更优质的方法去选择和阅读一本值得读的书。因此在五一读的第一本书便是用来纠正自己阅读习惯的。下面分享一下这本书的读书笔记。</p><h2 id="阅读的活力与艺术"><a href="#阅读的活力与艺术" class="headerlink" title="阅读的活力与艺术"></a>阅读的活力与艺术</h2><p>&emsp;&emsp;阅读是一件说简单也简单，说复杂也复杂的活动。单从阅读能为我们带来什么这个角度来说，无非有这几点：理解力、娱乐和资讯。在阅读之前带着好的目标能有事半功倍的效果，</p><h3 id="主动的阅读：如何做一个自我要求的阅读者"><a href="#主动的阅读：如何做一个自我要求的阅读者" class="headerlink" title="主动的阅读：如何做一个自我要求的阅读者"></a>主动的阅读：如何做一个自我要求的阅读者</h3><ul><li>阅读是一件可以主动的事</li><li>阅读越主动，效果越好<br>&emsp;&emsp;以棒球赛为例，作者=投球手；读者=捕手。区别在于，读书不止有读的好与读的差两种状态。而更靠近读的好还是读得差，取决于阅读时多么主动，以及投入多少心思和阅读技巧。<br>阅读的时候，让自己昏昏入睡比保持清醒要容易的多。在阅读时处于哪种状态，主要取决于一个人的阅读目标。作者指出，许多人都能清楚的区分想要从书中获益还是取乐，但最后仍然无法按目标阅读。原因在于不知道怎样做一个自我要求的阅读者。在阅读一本书时想打瞌睡，并不是不想努力，而是不知道如何努力。要学会主动阅读<br><strong>主动阅读的基础</strong>：阅读时提出问题，并尝试自己解答</li><li>这本书再谈论什么</li><li>哪些部分详细描述了</li><li>这本书的观点对吗</li><li>这本书的信息跟你有什么关系</li></ul><p>&emsp;&emsp;<strong>如何让一本书真正属于自己</strong>：学会做笔记，阅读一本书像是与作者对话。而笔记能表达自己与作者之间相异或相同的观点。做笔记的方法：</p><ul><li>画底线</li><li>标记重点符号</li><li>在空白处编号，理清作者的思路</li><li>圈出关键字或句子</li><li>在书的空白处做笔记</li></ul><p>&emsp;&emsp;<strong>培养阅读习惯</strong>:习惯是第二天性，让阅读变得自然。任何原创性的东西都有规则和技巧可循，如绘画和雕塑。<br>&emsp;&emsp;<strong>从许多规则中养成一个习惯</strong>：任何技能在不熟练的时候，都显得笨手笨脚，熟悉了后能将复杂的步骤连贯起来，变得优美且和谐。</p><h3 id="阅读的目标：为了获得资讯而读，以及求得理解而读"><a href="#阅读的目标：为了获得资讯而读，以及求得理解而读" class="headerlink" title="阅读的目标：为了获得资讯而读，以及求得理解而读"></a>阅读的目标：为了获得资讯而读，以及求得理解而读</h3><p>&emsp;&emsp;作者鼓励如果想要从一本书获取到有用的东西，而不是打发时间，最好是为了提升理解力和资讯。事实上，在获取到理解力与资讯的同时，就具有了消遣的效果。<br>&emsp;&emsp;<strong>阅读这个词可以区分为两种含义：</strong></p><ul><li>阅读与自己理解力相近的读物，如报纸：能增加资讯，却不能增加理解力</li><li>阅读的书籍或听的演讲，对方水平远高于自己:理解更多的事情（<strong>知乎上关于知识的定义</strong>）</li></ul><h3 id="阅读就是学习"><a href="#阅读就是学习" class="headerlink" title="阅读就是学习"></a>阅读就是学习</h3><p>&emsp;&emsp;法国文艺复兴时期的人文主义思想家蒙田说：初学者的无知在于未学，而学者的无知在学后。后者被英国诗人亚历山大.蒲伯称为书呆子，读的广却读不通。要避免都的多就是读的好的观点，要学会选择值得阅读的书籍，在读书时思考，还要运用感觉和想象力。</p><h3 id="老师的出席与缺席"><a href="#老师的出席与缺席" class="headerlink" title="老师的出席与缺席"></a>老师的出席与缺席</h3><p>&emsp;&emsp;阅读一本书只能靠自己，更像是跟着一位缺席的老师学习，因此阅读书籍时更需要主动，也要通过各种方式运用书籍和作者交流。</p><h2 id="阅读的层次"><a href="#阅读的层次" class="headerlink" title="阅读的层次"></a>阅读的层次</h2><p>&emsp;&emsp;阅读的层次是递进的，而且的向下包含的。四种阅读层次：</p><ul><li>初级阅读（elementary reading）:能认字，这是基础，能明白每个字的意思，才能明白句子背后的含义。</li><li>检视阅读（inspectional reading）略读，在很短时间读完一本书</li><li>分析阅读（analytical reading），全盘完整的阅读，寻求理解</li><li>主题阅读(syntopical reading)，在阅读时，比较很多相关的书<br>对于中国的国情，每一个受过义务教育的人都能精通初级阅读。因此主要看作者分享的后续阅读方法。</li></ul><h2 id="检视阅读"><a href="#检视阅读" class="headerlink" title="检视阅读"></a>检视阅读</h2><p>&emsp;&emsp;精通了初级阅读才能熟练的检视阅读，检视阅读一共有两种：</p><ul><li>有系统的粗读：目标是快速发现一本书值不值得多花时间阅读。略读是以最小的时间代价了解作者为什么写这样一本书以及这本书是否对自己有用。关于略读的一些建议：1.先看书名页，看序 2.研究目录页 3.如果有索引，也要检阅下 4.出版者的介绍 5.挑几个跟主题相关的篇章看看 6.翻看几页</li><li>粗浅的阅读：并不是说草草的读完了事，而是遇到不懂的地方，记录下来，先略过。集中精神读完弄得懂的地方。享受读书的快乐，洞察全书的意义。稍后再专心研究不懂的地方。</li></ul><p>&emsp;&emsp;阅读的速度很重要，不同的书籍用不同的速度阅读。<strong>训练阅读速度的方法</strong>：眼睛一次只能读一个字或句子，但大脑能在一瞥之间掌握一个句子或段落。要学会跟着大脑的快速运转看书，而不是眼部的慢动作。训练自己快速阅读的方式：用手指着读，手移动的速度稍比自己的阅读速度快，强迫自己以更快的速度阅读。</p><h2 id="分析阅读"><a href="#分析阅读" class="headerlink" title="分析阅读"></a>分析阅读</h2><p><strong>书籍分类的重要性</strong>：在阅读前要知道自己读的是那一类书<br><strong>书名与内容的关系</strong>：阅读书名可以让读者在阅读之前，获得一些基本的资讯</p><p>&emsp;&emsp;分析阅读的规则：</p><ul><li><p>分析阅读的第一阶段：找出一本书在谈些什么</p><ul><li>依照书的种类与主题分析</li><li>使用一个单一的句子或最多几句话叙述整本书的内容。</li><li>将书中重要篇章列举出来，说明是如何按照顺序则称一个整体的架构<br>后两条规则可以帮助写作，因为写作和阅读是一体两面的事情，一个作品应该有整体感，逻辑清晰，前后连贯。</li><li>找出作者要问的问题，或作者想要解决的问题</li></ul></li><li><p>分析阅读的第二阶段：诠释一本书的内容规则</p><ul><li>诠释作者使用的关键字，与作者达成共识</li><li>从最重要的句子中抓出作者的重要主旨</li><li>找出作者的论述 ，重新架构这些论述的前因后果，以明白作者的主张</li><li>确定作者已经解决了哪些问题，还有哪些是未解决的</li></ul></li><li><p>分析阅读的第三阶段：评价一本书的规则</p><ul><li>除非已经能诠释书的架构，否则不要轻易批评</li><li>证明作者知识的不足</li><li>证明作者错误的地方</li><li>证明作者的不合逻辑</li><li>证明作者的分析与理由是不完整的</li></ul></li></ul><h2 id="主题阅读：阅读的最终目标"><a href="#主题阅读：阅读的最终目标" class="headerlink" title="主题阅读：阅读的最终目标"></a>主题阅读：阅读的最终目标</h2><p>&emsp;&emsp;主题阅读的准备阶段：</p><ul><li>针对要研究的主题，参考图书馆目录以及书中的索引。找到与主题相关的书籍</li><li>浏览整理出的书单，确定哪些与要研究的主题相关。明确各个书与主题的相关程度。</li></ul><p>&emsp;&emsp;在讨论某个主题的书时，所涉及到的往往不止是一本书。主题阅读的五个步骤：</p><ul><li>找到相关章节，关心的重点应该在具体的内容，而不是整本书</li><li>分析阅读中说与作者达成共识。但面对不同的作者描述同样的观点，会有不同的字眼。需要找出他们之间的共识。</li><li>建立一个主旨，列出一连串的问题，从书中找答案</li><li>理清主要及次要的议题。将作者针对不同各个问题的不同意见整理在各个议题旁。</li><li>将问题与议题按顺序排列，以凸显主题。</li></ul><h2 id="阅读与心智的成长："><a href="#阅读与心智的成长：" class="headerlink" title="阅读与心智的成长："></a>阅读与心智的成长：</h2><p>&emsp;&emsp;好书能带来心智的成长，提升阅读能力，能教会你了解这个世界以及自己。不仅是读的更好，还更懂得生命；变得有智慧与知识，对人类生命中永恒的真理有更深刻的认识。在读书的时候不光要会阅读，也要能分辨哪些书能给自己带来成长。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>不要被书中的难点吓到，90%的书知道个大概就好，想要了解的地方再深究；</li><li>阅读时要系统的总结，梳理文章的内容</li><li>学会做笔记，包括画重点，记录章节，页数以及自己针对某段话的思考</li><li>选择优质的书籍，不要将时间浪费在不好的书上</li><li>阅读时有以下重点：<ul><li>带着问题和目的主动的阅读</li><li>整理文章的脉络</li><li>与“书本”交流，时常问自己这本书这一章是解决什么问题，哪些观点对或者不对，值得借鉴和应该避免的错误</li><li>将书本引入自己的知识结构中</li></ul></li></ul><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img data-src=&quot;https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/封面.png?imageMogr2/thumbnail/!60p&quot; alt=&quot;enter description here&quot;&gt;</summary>
    
    
    
    <category term="读书笔记" scheme="https://oysz2016.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="读书笔记" scheme="https://oysz2016.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>基于Github和Hexo的个人博客搭建</title>
    <link href="https://oysz2016.github.io/post/c57581c4.html"/>
    <id>https://oysz2016.github.io/post/c57581c4.html</id>
    <published>2023-09-03T07:03:41.275Z</published>
    <updated>2019-03-17T05:49:02.600Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>&emsp;&emsp;去年起看了很多大牛的博客，也萌生了搭建个人博客的想法，为什么搭建博客，总结下来有以下好处：</p><ul><li><strong>书写是为了更好的思考</strong></li><li><strong>激励自己持续学习</strong></li><li><strong>尝试持之以恒的去做一些事情</strong><span id="more"></span></li></ul><p>&emsp;&emsp;这次趁着大论文盲审结果还没出来和导师还没反馈小论文修改意见的间隙，花了一个周末的时间搭建好了自己的个人博客。在此将搭建过程作为自己的第一篇博客记录下来。</p><h2 id="Hexo简介"><a href="#Hexo简介" class="headerlink" title="Hexo简介"></a>Hexo简介</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/1524460444%281%29.jpg-chuli" alt="enter image description here"><br>&emsp;&emsp;Hexo 是一个基于 Node.js 的静态博客程序，可以方便的生成静态网页托管在github和Heroku上。Hexo有着丰富的主题，可以定制多种样式。<br>hexo特性：</p><ul><li>速度快：Hexo基于Node.js，支持多进程，几百篇文章也可以秒生成；</li><li>撰写工具丰富：支持GitHub Flavored Markdown和所有Octopress的插件；</li><li>扩展性强： Hexo支持EJS、Swig和Stylus。通过插件支持Haml、Jade和Less。</li></ul><p>&emsp;&emsp;使用hexo时，有以下常用命令：<br><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">npm</span> install hexo -g <span class="comment">#安装</span></span><br><span class="line"><span class="built_in">npm</span> install hexo -g <span class="comment">#安装  </span></span><br><span class="line"><span class="built_in">npm</span> update hexo -g <span class="comment">#升级  </span></span><br><span class="line">hexo init <span class="comment">#初始化</span></span><br><span class="line">hexo n <span class="string">&quot;我的博客&quot;</span> == hexo <span class="keyword">new</span> <span class="string">&quot;我的博客&quot;</span> <span class="comment">#新建文章</span></span><br><span class="line">hexo p == hexo publish</span><br><span class="line">hexo g == hexo generate<span class="comment">#生成</span></span><br><span class="line">hexo s == hexo server <span class="comment">#启动服务预览</span></span><br><span class="line">hexo d == hexo deploy<span class="comment">#部署</span></span><br><span class="line">hexo <span class="keyword">new</span> 创建文章</span><br></pre></td></tr></table></figure></p><h2 id="环境搭建-node-hexo-git"><a href="#环境搭建-node-hexo-git" class="headerlink" title="环境搭建(node,hexo,git)"></a>环境搭建(node,hexo,git)</h2><p>&emsp;&emsp;前面说过hexo是基于node.js，因此需要先安装<a href="https://nodejs.org/en/">node.js</a>。git的配置这里就不再赘述，可以参考廖雪峰老师的<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/">Git教程</a>。安装好Git后，需要将其与自己的GitHub账号关联上。安装好node.js后，仅需一步即可安装hexo的相关套件。在命令行输入:</p><pre><code>npm install hexo -g hexo-cli</code></pre><p>&emsp;&emsp;到这一步就安装好了所需的所用环境。</p><h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>&emsp;&emsp;在搭建自己的博客前，需要设置一个博客的根目录。使用命令行切换到该根目录下，输入：</p><pre><code>hexo init blog</code></pre><p>&emsp;&emsp;等待片刻，成功后会提示<code>INFO  Start blogging with Hexo!</code>初始化成功后，目录如下：</p><pre><code>.├── _config.yml├── package.json├── scaffolds├── source|   ├── _drafts|   └── _posts└── themes</code></pre><p>&emsp;&emsp;source的_posts目录下会自带一篇题为“Hello World”的示例文章，直接执行以下操作可以看到网站初步的模样:</p><pre><code>$ hexo generate# 启动本地服务器$ hexo server# 在浏览器输入 http://localhost:4000/就可以看见网页和模板了,若端口号被占用，可输入hexo server -p 4001改为其他端口号。</code></pre><p>&emsp;&emsp;访问<a href="http://localhost:4000/，界面如下：">http://localhost:4000/，界面如下：</a><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%8D%9A%E5%AE%A2%E5%88%9D%E5%A7%8B%E8%A7%86%E5%9B%BE.png-chuli" alt="enter image description here"></p><h2 id="部署及配置博客"><a href="#部署及配置博客" class="headerlink" title="部署及配置博客"></a>部署及配置博客</h2><h3 id="配置SSH"><a href="#配置SSH" class="headerlink" title="配置SSH"></a>配置SSH</h3><p>&emsp;&emsp;在上一步看到了网站的默认效果，此时需要将该博客部署到Github上，登陆Github，创建名为your_name.github.io(your_name替换成你的用户名)的仓库。重新打开CMD,输入：</p><pre><code>ssh-keygen -t rsa -C &quot;Github的注册邮箱地址&quot;</code></pre><p>&emsp;&emsp;一路Enter，得到信息：<code>Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub.</code>根据保存的路径找到id_rsa.pub文件，用编辑器打开，复制所有内容，然后<a href="https://github.com/login?return_to=https://github.com/settings/ssh">Sign in to GitHub</a>，按以下步骤配置SSH：<br>&emsp;&emsp;New SSH key ——&gt;Title：blog ——&gt; Key：输入刚才复制的—— &gt;Add SSH key</p><h3 id="配置博客"><a href="#配置博客" class="headerlink" title="配置博客"></a>配置博客</h3><p>&emsp;&emsp;在blog目录下，用编辑器打开_config.yml，修改其中的配置信息。<br>&emsp;&emsp;修改网站中的相关信息 ：<br><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">title:</span> <span class="meta">#标题</span></span><br><span class="line"><span class="symbol">subtitle:</span> <span class="meta">#副标题</span></span><br><span class="line"><span class="symbol">description:</span> <span class="meta">#站点描述</span></span><br><span class="line"><span class="symbol">author:</span> <span class="meta">#作者</span></span><br><span class="line"><span class="symbol">language:</span> <span class="built_in">zh</span>-Hans</span><br><span class="line"><span class="symbol">email:</span>  <span class="meta">#电子邮箱</span></span><br><span class="line"><span class="symbol">timezone:</span> Asia/Shanghai</span><br></pre></td></tr></table></figure><br>&emsp;&emsp;配置部署仓库</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">deploy:</span> </span><br><span class="line"><span class="symbol">  type:</span> git</span><br><span class="line"><span class="symbol">  repo:</span> 刚刚github创库地址.git</span><br><span class="line"><span class="symbol">  branch:</span> master</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;特别提醒，在每个参数的：后都要加一个空格。以上操作完成后，执行：</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo clean <span class="meta">#清除缓存 网页正常情况下可以忽略此条命令</span></span><br><span class="line">hexo generate <span class="meta">#生成</span></span><br><span class="line">hexo <span class="keyword">server</span> <span class="meta">#启动服务预览，非必要，可本地浏览网页</span></span><br><span class="line">hexo deploy <span class="meta">#部署发布</span></span><br></pre></td></tr></table></figure><p>&emsp;&emsp;得到提示信息<code>INFO  Deploy done: git</code>表示成功发布到Github上。然后在浏览器里输入your_name.github.io就可以访问刚刚配置好的博客了。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>&emsp;&emsp;到此为止，最基本的hexo+github搭建博客完结。hexo有许多优美简洁的主题，网上也有许多关于主题美化的教程，可以根据自己的喜好添加各种或实用或酷炫的功能。</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;去年起看了很多大牛的博客，也萌生了搭建个人博客的想法，为什么搭建博客，总结下来有以下好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;书写是为了更好的思考&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;激励自己持续学习&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;尝试持之以恒的去做一些事情&lt;/strong&gt;</summary>
    
    
    
    <category term="Hexo博客搭建教程" scheme="https://oysz2016.github.io/categories/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="hexo" scheme="https://oysz2016.github.io/tags/hexo/"/>
    
    <category term="Github" scheme="https://oysz2016.github.io/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>《Unbiased Scene Graph Generation from Biased Training》论文笔记</title>
    <link href="https://oysz2016.github.io/post/b4822109.html"/>
    <id>https://oysz2016.github.io/post/b4822109.html</id>
    <published>2023-09-03T07:03:41.271Z</published>
    <updated>2023-01-01T04:01:59.574Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><h1 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h1><p>场景图生成: 描述目标检测的物体之间，具有怎样的关系</p><h1 id="之前算法存在的问题"><a href="#之前算法存在的问题" class="headerlink" title="之前算法存在的问题"></a>之前算法存在的问题</h1><ul><li>数据集中关系词存在严重的偏见，原因有以下几点:<ol><li>标注时,倾向于简单的关系</li><li>日常生活中确实有些事物的关联性比较多</li><li>语法习惯的问题</li></ol></li></ul><ul><li>往往通过union box和两个物体的类别就预测了两个物体的关系，几乎没有使用visual feature，也就预测不出有意义的关系</li></ul><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><ol><li>为了让模型预测更有意义的关系，用了一个causal inference中的概念，即Total Direct Effect（TDE）来取代单纯的网络log-likelihood。在预测关系时更关注visual feature.</li></ol><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-144828@2x.png" alt="enter description here"></p><ol><li>提出了新的评测方法mR@K:把所有谓语类别的Recall单独计算，然后求均值，这样所有类别就一样重要了</li></ol><h2 id="Biased-Training-Models-in-Causal-Graph"><a href="#Biased-Training-Models-in-Causal-Graph" class="headerlink" title="Biased Training Models in Causal Graph"></a>Biased Training Models in Causal Graph</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143055@2x.png" alt="有偏差的训练框架"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-142859%402x.png" alt="训练简化图"></p><p>节点I: Input Image&amp;Backbone.Backbone部分使用Faster rcnn预训练好的模型，并frozen bockbone的参数。输出检测目标的bounding boxes和图像的特征图.<br>Link I-&gt;X:目标的特征，通过ROI Align提取目标对应的特征<script type="math/tex">R={r_i}</script>，获取目标粗略的分类结果<script type="math/tex">L={l_i}</script>.和MOTIFS和VCTree一样，使用以下方式，编码视觉上下文特征.</p><blockquote><p>MOTIFS中使用双向LSTM，VCTree中使用双向TreeLSTM，早期工作如VTransE中使用全连接层</p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-145409@2x.png" alt="enter description here"></p><p>节点X：目标特征。获取一组目标的特征<script type="math/tex">x_e=(x_i,x_j)</script><br>Link X-Z: 获取对应目标fine-tuned的类别，从<script type="math/tex">x_i</script>解码:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-145415@2x.png" alt="enter description here"></p><p>节点Z:目标类别，one-hot的向量<br>Link X-&gt;Y: SGG的目标特征输入<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-145423@2x.png" alt="enter description here"></p><p>Link:Z-&gt;Y：SGG的目标类别输入<br>Link:I-&gt;Y:SGG的视觉特征输入<br>节点Y:输出关系词汇<br>Training loss：使用交叉熵损失预测label，为了避免预测Y只使用单一输入的信息，尤其是只使用Z的信息，进一步 使用auxiliary cross-entropy losses, 让每一个分支分别预测y</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/1280X1280.png" alt="enter description here"></p><h2 id="Unbiased-Prediction-by-Causal-Effects"><a href="#Unbiased-Prediction-by-Causal-Effects" class="headerlink" title="Unbiased Prediction by Causal Effects"></a>Unbiased Prediction by Causal Effects</h2><p>机器学习中常见的解决长尾问题的方法:</p><ul><li>数据增强/重新采样</li><li>对数据平衡改进的loss</li><li>从无偏见中分离出有偏见的部分<br>与上述方法的区别是不需要额外训练或层来建模偏差，通过构建两种因果图将原有模型和偏差分离开。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143048@2x.png" alt="enter description here"></li></ul><h3 id="Origin-amp-Intervention-amp-Counterfactual"><a href="#Origin-amp-Intervention-amp-Counterfactual" class="headerlink" title="Origin&amp;Intervention&amp;Counterfactual"></a>Origin&amp;Intervention&amp;Counterfactual</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143107@2x.png" alt="enter description here"></p><ul><li>ntervention:清除因果图中某个节点的输入，并将其置为某个值，公式为:$do(X= \tilde x)$.某节点被干预后，需要该节点输入的其他节点也会受影响</li><li>Counterfactual:让某个节点被干预后，其他需要输入的节点还假设该节点未被干预<br>总结: Counterfactual图实际上抹除了因果图像中object feature。只用image+object label预测两个目标间的关系。</li></ul><h3 id="Total-Direct-Effect-TDE"><a href="#Total-Direct-Effect-TDE" class="headerlink" title="Total Direct Effect (TDE)"></a>Total Direct Effect (TDE)</h3><p>根据两个因果图:</p><ul><li>原始因果图</li><li>Counterfactual因果图(可以认为是偏见)<br>消除偏见:<script type="math/tex; mode=display">TDE=Y_x(u)-Y_{\tilde x,z}(u)</script></li></ul><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143120@2x.png" alt="enter description here"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143154@2x.png" alt="enter description here"></p><h1 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h1><p>对比了几种常用的优化长尾问题的方法</p><ul><li>Focal</li><li>Reweight</li><li>Resample</li><li>X2Y: 直接通过X的输出预测Y</li><li>X2Y-Tr：切断其他分支的联系，只使用X预测Y</li><li>TE:<script type="math/tex">TE=Y_x(u)-Y_{\tilde x}(u)</script></li><li>NIE:<script type="math/tex">NIE=TDE-TE</script></li><li>TDE</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143120@2x.png" alt="enter description here"></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="因果图" scheme="https://oysz2016.github.io/tags/%E5%9B%A0%E6%9E%9C%E5%9B%BE/"/>
    
    <category term="场景图生成" scheme="https://oysz2016.github.io/tags/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>因果图的思想优化长尾问题</title>
    <link href="https://oysz2016.github.io/post/3a13345e.html"/>
    <id>https://oysz2016.github.io/post/3a13345e.html</id>
    <published>2023-09-03T07:03:41.259Z</published>
    <updated>2023-01-01T04:21:49.320Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2009.12991">https://arxiv.org/abs/2009.12991</a><br><strong>代码链接:</strong> <a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch">https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch</a><br>这篇文章和之前有介绍过的一篇场景图生成的文章<a href="https://arxiv.org/abs/2002.11949">Unbiased Scene Graph Generation from Biased Training</a>思想比较类似, 作者也是同一个团队。贴一个<a href="https://oysz2016.github.io/post/b4822109.html">传送门</a>，方便感兴趣的同学浏览。<br>之所以说思想类似，是因为这两篇文章不仅都用来解决长尾问题，而且都用到了<strong>因果图</strong>的的思想。在看这篇论文的时候，有些内容读起来还是很难理解的，查阅了一些<strong>统计学</strong>和<strong>因果关系</strong>的相关概念才觉得清晰了些。这些理论也非常有意思，看完后我觉得在阅读这篇文章前还是很有必要学习的。在写这篇blog的时候，我也尽量把需要用到的背景知识整理出来，方便理解。</p><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><h3 id="因果关系"><a href="#因果关系" class="headerlink" title="因果关系"></a>因果关系</h3><p>关于因果关系，在<a href="https://zhuanlan.zhihu.com/p/111306353">https://zhuanlan.zhihu.com/p/111306353</a>中有详细的背景知识。为了方便阅读和加深自己的理解。我在这里精简的引用下，更详细的内容可以去阅读大佬的知乎文章。</p><blockquote><p>《The Book of Why》中将因果理论的探索类比成一个向上的阶梯，包含三个层级：Seeing，Doing和Imaging。<br><strong>第一层级: Association 关联</strong>，对应的是大多数机器学习算法和动物。该层级强调基于被动的观察（passive observation）来预测，通过观察来寻找规律，并非真正的因果。<strong>其本质就是条件概率P(Y|X)，在观察到X的条件下Y发生的概率，也是传统机器学习里被广泛应用的。</strong><br><strong>第二层级：Intervention干预</strong>, 干预指的是消除因果关系中的混杂影响，推导出真正的因果关系，如随机对照试验。<br><strong>第三层级：Counterfactual 反事实</strong>，counterfactual和干预intervention区分的关键在于“hindsight”(事后来看)，即反事实强调在对结果已知观测的基础上再对反事实的问题进行解答。</p><h3 id="因果推断中的变量"><a href="#因果推断中的变量" class="headerlink" title="因果推断中的变量"></a>因果推断中的变量</h3><p>因果推断中有一些专业术语用来表示在因果中不同变量的角色，主要分为<a href="https://zh.wikipedia.org/wiki/%E5%B9%B2%E6%93%BE%E5%9B%A0%E7%B4%A0">混淆变量(confounder)</a>，<a href="https://zh.wikipedia.org/wiki/%E4%B8%AD%E4%BB%8B%E8%AE%8A%E9%A0%85">中介变项(mediator)</a>，<a href="https://zh.wikipedia.org/wiki/%E5%B0%8D%E6%92%9E%E5%9B%A0%E5%AD%90">对撞因子(collider)</a><br><strong>混淆变量</strong>比较通俗的例子是老年人因为退休了会更有时间晨炼，但老年人却比年轻人更容易得癌症，如果不控制年龄的分布，就会得到晨炼的人容易得癌症。这里的年龄就是混淆变量，需要被控制<br><strong>中介效应</strong>指的是从一个变量到另一个变量，中间会有些其他的变量带来影响。比如吃药能带来疾病的好转，可能是药本身起了作用，也可能是心理安慰。比较极端的例子是是不是就会出现用面粉做假药的新闻，对于患者而言，药本身可能没起到作用，但可能由于吃了“药”，觉得自己马上会好，比较积极的心态带来了身体的好转。<br><strong>对撞因子</strong>指的是同时被两个以上变量影响的因素，而这些影响对撞因子的变量之间不见得有因果关系。例如在NBA球员中，会发现身高比较高的人得分率并没有很高，这是因为身高矮的人能进NBA必然是用其他优势弥补了劣势。<strong>身高</strong>和<strong>得分率</strong>之间并没有明显的因果关系，而他们都决定能不能<strong>进NBA</strong>。仔细思考就会发现对撞因子的例子很容易造成幸存者偏差。</p><h3 id="Propensity-Score"><a href="#Propensity-Score" class="headerlink" title="Propensity Score"></a>Propensity Score</h3><p><strong>倾向评分匹配(Propensity Score Matching)</strong> 是一种统计学的方法，指的是在<strong>观察研究中</strong>，由于种种原因，<strong>数据偏差（bias）</strong> 和 <strong>混杂变量（confounding variable）</strong> 较多，<strong>倾向评分匹配的方法正是为了减少这些偏差和混杂变量的影响，以便对实验组和对照组进行更合理的比较。</strong><br>为了能较好说明这个方法的价值，可以用理科里学的随机对照实验(Randomized Controlled Trial data)做对比。随机对照试验在样本量足够的情况下是很科学的评判变量对结果影响的实验方法，但很多时候是不符合科研伦理的，比如要研究吸烟是否有害健康，如果招收大量人员，然后随机分配到吸烟组和不吸烟组，这种实验设计不太容易实现，而且也存在危害测试人员健康的可能。而这个研究课题其实很容易通过<strong>观察的研究数据</strong>进行实验，面对观察的研究数据，如果不加调整，很容易获得错误的结论，比如拿吸烟组健康状况最好的一些人和不吸烟组健康状况最不好的一些人作对比，得出吸烟对于健康并无负面影响的结论。从统计学角度分析原因，这是因为观察研究并未采用随机分组的方法，无法基于大数定理的作用，在实验组和对照组之间削弱混杂变量的影响，很容易产生系统性的偏差。<strong>倾向评分匹配</strong>就是用来解决这个问题，消除组别之间的干扰因素。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>Re-Balanced Training:</strong> re-blance的方法主要有两种，分别是re-sampling和re-weighting。<br><strong>Hard Example Mining:</strong> 不关注于每个类别样本数量的先验分布，而是关注于难样本用于缓解长尾问题，代表方法是focal loss<br><strong>Transfer Learning/Two-Stage Approach:</strong> 作者总结的这类工作的特点是将头部类的knowledge转移到尾部类，用以改善长尾问题。其中比较有代表性的是<a href="https://arxiv.org/abs/1910.09217">Decoupling</a>算法，和受Decoupling启发的<a href="https://arxiv.org/abs/1912.02413">BBN</a>算法。BBN在之前的<a href="https://oysz2016.github.io/post/b4822109.html">关于长尾问题的文章</a>中也有过总结。<br><strong>Causal Inference:</strong> 因果图推理作者主要列举了一些这方面的著作，当然也提到了自己在场景图生成中的文章</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>作者将视角聚焦于梯度优化器中常用的track——<strong>Momentum</strong>。在讲解作者是怎么做的之前，让我们回顾下Momentum。Momentum的思想是累积一个历史的梯度信息用来加速优化器，好处主要有以下两点:</p><ul><li>每次梯度更新的时候，不仅考虑了当前梯度的方向，同时也考虑了之前更新的方向，在梯度优化时，不会抖动的那么随意</li><li>Momentum相当于给梯度优化的方向施加了一个惯性，参数优化时容易突破局部最优解，更可能找到全局最优解.</li></ul></blockquote><p><strong>上面是一些比较定性的分析，关于Momentum为什么能work?</strong><a href="https://distill.pub/2017/momentum/">https://distill.pub/2017/momentum/</a>做了非常好的可视化，可以直观的感受到Momentum为梯度优化带来的改变<br>下面的两张图分别是没有Momentum和使用合理参数设置Momentum对模型优化带来的差异，可以看到Momentum能提高网络训练的稳定性，并且同样的迭代次数更容易收敛到全局最优。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/1.png" alt=""><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/2.png" alt=""><br>回归正题，对于长尾分布的数据集，正是由于Momentum会受之前梯度信息的影响，Momentum所产生的惯性，会带来马太效应，即模型的优化方向会倾向于让模型对头部类的效果更好。<br>作者将Momentum对网络的作用用因果图抽象了出来，如figure 1(a)所示<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/3.png" alt=""><br>上图中，<strong>X代表backbone提取到的特征</strong>，<strong>M代表优化器中的动量</strong>，<strong>Y代表预测结果</strong>，<strong>D代表动量所产生的惯性，由于是长尾数据集，这里的D特指对头部类优化的惯性，而在balanced的数据集中，D对每个类别的贡献是一样的</strong>。图中的箭头表示彼此的影响，例如，X-&gt;Y以为着，Y的得出收到X的影响。从因果图中的关系可以看出<strong>节点M和节点D分别代表混淆变量(confounder)和中介效应(mediator)</strong>。<br><strong>M-&gt;X</strong>代表的是特征图X的是在动量M的影响下训练的,figure 1(b)中可视化了动量M对不同类别的影响，可以发现头部类在动量中占比较大。<br>知道了混淆变量和中介效应之后，需要做的就是<strong>消除这些变量对模型带来的偏见</strong>。和<a href="https://arxiv.org/abs/2002.11949">Unbiased Scene Graph Generation from Biased Training</a>中类似，接下来需要构建TDE(Total Direct Effect)用于消除偏见，公式如下:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/10_4.png" alt=""><br>和公式对应的因果图如Figure 3<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/4.png" alt=""><br>在计算TDE时需要抹除掉混淆变量M对X的影响，但没有办法得到M的分布。在<a href="http://bayes.cs.ucla.edu/jsm-july2012-pdf.pdf">Causal inference in statistics: A primer</a>的书中有提到<strong>Inverse Probability Weighting</strong>的公式，这个公式给出了一种思路，即没有办法得到M的分布时，可以看M和X有没有一一对应关系。在这个方法里，M和X确实是有对应关系的。所以可以将对X的采样看成是对M的近似<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/5.png" alt=""><br>这里将weights和features的通道/维度划分成k组，可以认为是做了k倍的细粒度采样。这样的好处是通过multi-head多重采样能更好的近似。<br>M能够做近似之后，还需要考虑 <strong>倾向评分(Propensity Score)</strong> 的影响，在这个问题中需要对所有类别做归一化的统一分布，也就是考虑每个类别的模长。下面就是得到的Propensity Score的公式, 其中第一项是类别感知的, 其中第一项是class-specific，第二项是class-agnostic。需要第二项的原因是因为从Figure 1(b)中可以看出x也具有bias。</p><script type="math/tex; mode=display">g(i, x^k; w^k_i)=||x^k|| \cdot ||w_i^k|| + \gamma||x^k||</script><p>则公式2中的第一项$P (Y = i|do(X = x))$可以表示为<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/6.png" alt=""><br>公式2中的第二项和第一项的区别在于使用了空数据$x_0$替代x。而其他项保持不变，这一部分是构建反事实的因果图，相当于让网络仅通过M和D得到Y，而x没起到作用。可以把这部分看作是偏差。<br>最终的TDE如下式：<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/7.png" alt=""></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>和其他方法的对比如下:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/8.png" alt=""><br>关于本文方法和其他方法的差异，可以用下图表示。</p><ul><li>在baseline的数据集上有问题是由于训练数据是长尾的，而测试数据是balanced的，存在<strong>分布不匹配</strong>。</li><li><strong>One-stage Re-balancing</strong>的方法本质是<strong>改变了训练数据的分布，这种方式会带来错误的模型建模</strong>；</li><li><strong>Two-stage Re-balancing</strong>的方法是第一阶段先通过原始的数据对模型建模，第二阶段再优化分类器，<strong>对分类器边界做调整</strong>，所以能work</li><li>而本文的方法是在测试的时候将分布做了移动<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/9.png" alt=""><br><strong>优势:</strong></li><li><strong>不需要复杂的stage训练方式</strong></li><li><strong>可适用于多个任务，如图片分类，检测之类</strong></li><li><strong>不需要依赖数据的分布, 感觉这个优势对于online的训练比较有意义, 因为其他的训练方式其实都可以获取到数据的数据分布。</strong><h2 id="笔者总结"><a href="#笔者总结" class="headerlink" title="笔者总结"></a>笔者总结</h2>要使用作者提供的 <a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch">https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch</a> 代码中的<strong>CausalNormClassifier</strong>总结起来有几个要点, 为方便理解，下面要点中的超链接会索引到具体的代码:</li><li>训练的时候需要用<a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch/blob/9726439a702614e99a02e2ba321ec4e56491239e/classification/models/CausalNormClassifier.py#L54">multi-head normalized classifier</a></li><li>训练时需要记录<a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch/blob/9726439a702614e99a02e2ba321ec4e56491239e/classification/run_networks.py#L215">移动的平均特征</a></li><li>测试的时候需要用<strong>counterfactual TDE inference</strong>，即<a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch/blob/9726439a702614e99a02e2ba321ec4e56491239e/classification/models/CausalNormClassifier.py#L43">去除具有头部类倾向的部分</a><br>让我们回顾上面的图7，在作者<a href="https://zhuanlan.zhihu.com/p/259569655">博客</a>的评论部分，有非常简洁的总结, 在这里引用下：<blockquote><ol><li>decouple两阶段都是在train过程中，一阶段长尾分布下训练representation + classifier；二阶段直接通过暴力resample来调整classifier。</li><li>de-confound也可以看做两阶段，一阶段在train过程中，通过重采样和normalized的措施来训练representation + classifier；二阶段放在了test过程中，用一阶段中统计的bias来缓解测试中的class bias，得到TDE。<br>这两篇文章都很巧妙的使用了因果图。虽然很多trick可解释性确实不太强，不过细细思考起来，一些算法流程对整体算法的影响还是比较make sence。私以为在如果算法框架中有些trick能带来收益的同时，也会让带来一些问题负面的影响，就很适合用因果图的思想消除负面影响，不过这确实很考验对于问题的抽象能力和对因果图的理解<blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></li></ol></blockquote></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="长尾优化" scheme="https://oysz2016.github.io/tags/%E9%95%BF%E5%B0%BE%E4%BC%98%E5%8C%96/"/>
    
    <category term="因果图" scheme="https://oysz2016.github.io/tags/%E5%9B%A0%E6%9E%9C%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络的结构与相关算法</title>
    <link href="https://oysz2016.github.io/post/4ec18e58.html"/>
    <id>https://oysz2016.github.io/post/4ec18e58.html</id>
    <published>2023-09-03T07:03:41.253Z</published>
    <updated>2019-03-17T05:49:17.418Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>&emsp;&emsp;早在上世纪50年代，美国神经生物学家David Hubel通过研究猫和猴子的瞳孔区域与大脑皮层神经元的对应关系就发现视觉系统的信息处理方式是分级的。这一发现，促成了神经网络在图像处理上的发展。<br>&emsp;&emsp;神经网络的发展史可以分为三个阶段，第一个阶段是Frank Rosenblatt提出的感知机模型[1]，感知机模型的逻辑简单有效，但不能处理异或等非线性问题。第二个阶段是Rumelhart等提出的反向传播算法[2]，该算法使用梯度更新权值，使多层神经网络的训练成为可能。第三个阶段得益于计算机硬件的发展和大数据时代的到来，促进了深度神经网络的发展。<span id="more"></span><br>在上一篇博客<a href="https://oysz2016.github.io/2018/04/24/%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">什么是深度学习</a>中介绍了深度学习的三种网络模型，其中卷积神经网络在图像处理上有着诸多突破性的进展。由于我对卷积神经网络较为熟悉，下面将根据神经网络的三个发展阶段阶段分析讨论卷积神经网络的发展史。</p><h2 id="传统神经网络"><a href="#传统神经网络" class="headerlink" title="传统神经网络"></a>传统神经网络</h2><h3 id="感知机与多层网络"><a href="#感知机与多层网络" class="headerlink" title="感知机与多层网络"></a>感知机与多层网络</h3><p>&emsp;&emsp;感知机（Perceptron）由两层神经元组成，是一种二分类的线性分类模型，也是最简单的单层前馈神经网络（Feedforward Neural Network）模型。感知机的提出受到生物神经元的启发，神经元在处理突触传递而来的电信号后，若产生的刺激大于一定的阈值，则神经元被激活，感知机也具有类似的结构。假设输入空间是χ⊆R^n，输出空间是y={+1,-1}，x和y分别属于两个空间，则由感知机表示的由输入空间到输出空间的函数为：f(x)=sign(w∙x+b)<br>&emsp;&emsp;其中w和b是感知机模型的参数，w∈R^n称为权重(Weight)，b∈R^n称为偏置(Bias)，w∙x表示两个向量间的内积。<br>&emsp;&emsp;Minsky和Papert已证明若决策区域类型是线性可分的，则感知机一定会学习到收敛的参数权重w和偏置b，否则感知机会发生震荡[3]（fluctuation）。因此，感知机在线性可分数据中表现良好，如果设定足够的迭代次数，能很好的处理近似线性可分的数据。但如果对非线性可分的数据，如异或问题，单层感知机不能有效的解决。由于不能用一条直线划分样本空间，有学者想到用多条直线来划分样本，多层感知机就是这样一个模型。多层感知机结构如图1所示，相比较于单层感知机，多层感知机增加了隐藏层的层数。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%BB%93%E6%9E%84%E5%9B%BE.jpg-chuli" alt="多层感知机结构图"></p><div align = center>图1 多层感知机结构图</div>&emsp;&emsp;随着隐藏层数的增加，感知机的分类能力如表1所示。<div align = center>表1 感知机分类能力比较</div><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%88%86%E7%B1%BB%E8%83%BD%E5%8A%9B%E6%AF%94%E8%BE%832.png-chuli" alt="enter image description here"></p><p>&emsp;&emsp;由表可知，在异或问题中，无隐层的感知机不能解决异或问题，引入了隐层后，异或问题得到解决，而随着层数越多，对于异或问题的拟合会越来越好。这说明，在感知机中随着隐藏层层数的增多，决策区域可以拟合任意的区域，因此理论上多层感知机可以解决任何线性或非线性的分类问题。但是，Minsky和Papert提出隐藏层的权重和偏置参数无法训练，这是由于隐藏层不存在期望的输出，无法通过单层感知机的训练方式训练多层感知机[4]。</p><h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p>&emsp;&emsp;如何训练多层感知机的难点在很长一段时间没有得到解决，要训练多层网络，需要更有效的学习算法。反向传播（BackPropagation，BP）算法[1]是训练多层网络的常用方法，该方法用链式法则对网络中所有权重和偏置计算损失函数的梯度，将梯度反馈给随机梯度下降或其它最优化算法，用来更新权值以最小化损失函数。在网络中，正向传播和反向传播的过程如图2所示。在这个例子中输入图片经过网络正向传播后得到的分类是狗，与实际类别的人脸不符，此时会将误差逐层反向传播，修正各个层的权重和偏置参数后，再进行正向传播，反复迭代，直至网络的参数能正确的分类输入的图片。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/BP%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B.jpg-chuli" alt="enter image description here"></p><p><div align = center>图2 BP网络训练过程</div><br>&emsp;&emsp;反向传播算法的主要步骤如下：</p><ul><li>随机初始化多层网络的权重和偏置参数，将训练数据送入多层网络的输入层，经过隐藏层和输出层，得到输出结果。完成网络的前向传播过程；</li><li>计算输出层实际值和输出值间的偏差，根据反向传播算法中的链式法则，得到每个隐藏层的误差，根据每层的误差调整各层的参数。完成网络的反向传播过程；</li><li>不断迭代前两步中的正向传播和反向传播过程，直至网络收敛。</li></ul><p>&emsp;&emsp;由于还不熟悉markdown的公式编辑，这里省去反向传播的推导过程，感兴趣的朋友可以阅读周志华教授的《机器学习》<code>第五章神经网络</code>里面有详尽的推导过程。</p><h2 id="卷积神经网络的基本思想"><a href="#卷积神经网络的基本思想" class="headerlink" title="卷积神经网络的基本思想"></a>卷积神经网络的基本思想</h2><p>&emsp;&emsp;在BP神经网络中，每一层都是全连接的，参数数量随着网络宽度和深度增加会指数级增长。多层网络结合BP算法对输入数据虽然有强大的表示能力，但巨大的参数一方面限制了每层能够容纳的最大神经元数量，另一方面也限制了神经网络的深度。受到动物视觉皮层中感受野的启发，效仿这种结构的卷积神经网络具有局部连接和参数共享的特点，可以有效的减少网络的相关参数数量，优化网络的训练速度。</p><h3 id="局部连接"><a href="#局部连接" class="headerlink" title="局部连接"></a>局部连接</h3><p>&emsp;&emsp;Hubel和Wiesel在二十世纪五十年代和六十年代的研究表明，猫和猴子的视觉皮层中的神经元只响应特定的某些区域的刺激。将这种视觉刺激影响单个神经元反应的区域称为感受野（receptive field），相邻神经元细胞具有相同或相似的感受野[5]。正是由于发现了感受野等功能在猫的视觉神经中枢中的作用，催生了日本学者福岛邦彦提出带卷积和下采样层的多层卷积神经网络[6-8]。<br>&emsp;&emsp;当我们在处理一副图像时，其输入往往是高维的。传统的神经网络将下一层神经元连接到上一层所有神经元。这种方式随着网络层数的增加，参数数量会爆炸式增加，在实际运用中，会无法训练网络。卷积神经网络中采取的做法是将每个神经元连接到上一层的部分神经元。这种连接的空间范围是一个超参数，称为神经元的感受野，感受野实际上是神经元映射到输入图像矩阵空间的大小。<br>&emsp;&emsp;局部连接的实现方式是引入卷积层，通过卷积层对应局部的图像，每一层的神经元组合在一起对应图像的全局信息。如图3所示，在网络的第m层，每个神经元感受野大小为3，能连接到上一层的3个神经元。m+1层与m层类似。随着层数增加，神经元相对于输入层的感受野会越来越大。每个神经元不会响应感受野以外神经元的变化。受启发于动物的视觉神经元只响应局部信息，这样的结构确保了卷积神经网络只响应上一层局部神经元的变化，起到过滤作用的同时，减少了网络参数。而且随着层数的增加，这种过滤作用会越来越全局。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E5%B1%80%E9%83%A8%E8%BF%9E%E6%8E%A5.png-chuli?imageMogr2/thumbnail/!50p" alt="局部连接"></p><p><div align = center>图3 局部连接</div></p><h3 id="权值共享"><a href="#权值共享" class="headerlink" title="权值共享"></a>权值共享</h3><p>&emsp;&emsp;卷积的优点除了局部连接外还有权值共享。如图4所示，假设第m-1层有5个神经元，m层有3个神经元，对第m-1层的特征进行卷积，得到第m层共有3个单元的输出特征图。虽然第m层每个神经元都与第m-1层中的3个神经元连接，但同一组卷积操作的权重参数相同。在这个例子中，通过权值共享，将9个参数较少到了3个。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E6%9D%83%E5%80%BC%E5%85%B1%E4%BA%AB.png-chuli?imageMogr2/thumbnail/!70p" alt="权值共享"></p><p><div align = center>图4 权值共享</div><br>&emsp;&emsp;卷积神经网络中权值共享的实现方式是让同一个卷积核去卷积整张图像，生成一整张特征图[9]。在卷积操作中，同一个卷积核内，所有神经元共享相同权值，权值共享的策略可以很大程度上降低网络需要计算的参数数量。通过权值共享，不仅大大增加了参数的训练效率，而且提取的特征在一定程度上具有位置不变性，加强了特征对输入图像的表达能力。</p><h2 id="卷积神经网络结构"><a href="#卷积神经网络结构" class="headerlink" title="卷积神经网络结构"></a>卷积神经网络结构</h2><p>&emsp;&emsp;卷积神经网络是一种层次模型（Hierarchical Model），其输入是RGB图像，视频，音频等数据。卷积神经网络通过一系列卷积（Convolution）操作，非线性激活函数（Non-linear Activation Function），池化（Pooling）操作层层堆叠，逐层从原始数据获取高层语义信息[10]。<br>如图5所示，在结构上，卷积神经网络分类器有四种类型的网络层：卷积层、池化层、全连接层和分类器。各层次之间的有如下约束：<br>（1）多个卷积（C）和池化（S）层，将上一层的输出图像与本层权重W做卷积得到各个C层，然后经过下采样得到S层。<br>（2）全连接层：全连接层的输入是最后一个卷积池化层的输出，其输出是一个N维的列向量，维度对应类别的个数。<br>（3）分类器：p_1，p_2，p_n的具体数值代表输入图像属于各类别的概率，分类器根据提取到的特征向量将检测目标划分到合适的类中。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%86%E7%B1%BB%E5%99%A8.png-chuli" alt="卷积神经网络分类器"></p><p><div align = center>图5 卷积神经网络分类器</div></p><h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p>&emsp;&emsp;图片有着固有的特性，这意味着，图像的一部分特征与其他部分相似，对一张图片学习到的一部分特征可以用于其他部分。卷积操作受启发于这种特性，具体操作如图6所示，输入图片大小为5×5，经过卷积核大小为3×3的卷积后，原来的输入空间映射到3×3的区域。再经过一次相同大小的卷积核后，图片大小变为了1×1。可见，卷积层逐层提取特征的方式，能从庞大的像素矩阵中，提取到对图像更有代表性的特征。卷积层最重要的是卷积核的设计，卷积核有几个参数:大小、步长、数目、边界填充。这些参数会对卷积的效果带来很大的影响。若卷积核设计的较大，如AlexNet[11]中使用的11×11和5×5的卷积核，其感受野很大，能覆盖图像更大的区域，对图像的“抽象”能力会较好，但较大的卷积核也会带来参数过多的负面影响。卷积核的步长指卷积每次滑动的距离，在一定程度上影响了特征提取的好坏。每一层网络的多个卷积核保证了提取到的特征是图像的多个方面，但卷积核的数量也不是越多越好，过多的卷积核会增加参数数量，计算复杂的同时容易过拟合。边界填充可用于卷积核与图像尺寸不匹配时，填充图像缺失区域。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E5%8D%B7%E7%A7%AF.png-chuli" alt="卷积示意图"></p><p><div align = center>图6 卷积示意图</div></p><h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p>&emsp;&emsp;卷积后的特征依然十分巨大，不仅带来计算性能的下降，也会产生过拟合。于是产生了对一块区域特征进行聚合统计的想法.例如，可以计算图像在某一块区域内的最大值或平均值代替这一块区域的特征，在降低特征维度的同时能使提取到的特征更具有代表性，还会使得处理过后的特征图谱拥有更大的感受野，这种用部分特征代替整体特征的操作称为池化[10]（Pooling）。常用的池化方法如下：最大值池化（Max Pooling）；均值池化（Mean Pooling）；随机池化（Random Pooling）。池化操作具有以下优良特性：<br>（1）平移不变性（Translation Invariant）。无论是哪种池化方式，提取的都是局部特征。池化操作会模糊特征的具体位置，图像发生了平移后，依然能产生相同的特征。<br>（2）特征降维（Feature Dimension Reduction），池化操作将一个局部区域的特征进一步抽象，池化中的一个元素对应输入数据中的一个区域，可以减少参数数量，降低维度。<br>&emsp;&emsp;池化操作的功能是减小特征空间的大小，以减小网络中的参数和计算量，从而避免过度拟合。如图7所示，224×224×64经过大小为2×2，步长为2的池化核，变成了112×112×64，使得特征图谱减少为原来的1/2。图7中池化方式是最大池化，即将一个区域内的最大值表示为这个区域的池化结果。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E6%B1%A0%E5%8C%96%E7%A4%BA%E6%84%8F%E5%9B%BE.png-chuli" alt="池化示意图"></p><p><div align = center>图7 池化示意图</div></p><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>&emsp;&emsp;前面讨论的卷积层，池化层等操作是将原始数据映射到特征空间，使得到的特征矩阵越来越抽象并对特征有良好的表达能力。Softmax分类器要求输入是列向量，需要全连接层将卷积和池化的输出映射到线性可分空间。<br>全连接层可以聚合卷积和池化操作得到的高阶特征，并且可以简化参数模型，一定程度的减少神经元数量和训练参数。为了能用反向传播算法训练网络，全连接层要求图片有固定的输入尺寸。因此早期网络中，需要对不同尺寸的图片进行裁剪或拉伸，这种操作会带来图片信息的失真和损失。在第三章讨论的感兴趣（Region of Interest）池化方法，可以很好的解决这一问题。<br>&emsp;&emsp;卷积层是由全连接层发展而来，全连接层可以用特殊的卷积层表示，对于前一层全连接的全连接层可以用卷积核大小为1×1的卷积层替代，而对于前一层是卷积的全连接层可以用对上一层所有输入全局卷积的卷积层替代。在全连接层中可以认为每个神经元的感受野是整个图像。全连接层隐藏层节点数越多，模型拟合能力越强，但参数冗余会带来过拟合的风险而且会降低效率。对于这个问题，一般的做法是采用正则化（Regularization）技术，如L1、L2范式。还有通过Dropout随机舍弃一些神经元，来减少权重连接，然后增强网络模型在缺失个体连接信息情况下的鲁棒性[10]。</p><h3 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h3><p>&emsp;&emsp;经过全连接层将特征映射到线性空间后，最后还需要将实例数据划分到合适的分类中。分类器有多种，常用的有支持向量机和Softmax回归，此处以Softmax为例子。Softmax函数用于将多个神经元的输出映射到(0,1)之间，转化为概率问题，从而处理多分类问题。如图8所示，Softmax层的输入分别是3、1和-3，在经过Softmax层后分别映射为0.85、0.12和0.03，三个值的累加和为1，其数值可以理解为概率，则属于y_1类的概率最大为0.85。这幅图是Softmax的通俗理解，具体推导过程可以参考<a href="http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92">这篇文章</a>。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/softmax%E5%B1%82.png-chuli" alt="Softmax层"></p><p><div align = center>图8 Softmax层</div></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]    Rumelhart D E, Hinton G E, Williams R J. Learning representations by back-propagating errors [J]. Nature, 1986, 323(6088): 533-536.<br>[2]    Hinton G E, Osindero S, Teh Y W. A fast learning algorithm for deep belief nets [J]. Neural Computation, 2006, 18(7): 1527-1554.<br>[3]    Minsky M, Papert S. Perceptrons: An introduction to computational geometry [J]. 1969, 75(3): 3356-3362.<br>[4]    Minsky M L, Papert S A. Perceptrons (expanded edition) mit press [J]. 1988.<br>[5]    刘建立, 沈菁, 王蕾, 等. 织物纹理的简单视神经细胞感受野的选择特性 [J]. 计算机工程与应用, 2014, 50(1): 185-190.<br>[6]    Fukushima K. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position [J]. Biological Cybernetics, 1980, 36(4): 193-202.<br>[7]    Fukushima K. Neocognitron: A hierarchical neural network capable of visual pattern recognition [J]. Neural Networks, 1988, 1(2): 119-130.<br>[8]    Fukushima K, Miyake S. Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position [J]. Pattern Recognition, 1982, 15(6): 455-469.<br>[9]    曹婷. 一种基于改进卷积神经网络的目标识别方法研究 [D]. 湖南大学, 2016.<br>[10]    LeCun Y, Boser B, Denker J S, et al. Backpropagation applied to handwritten zip code recognition [J]. Neural computation, 1989, 1(4): 541-551.<br>[11]    Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks [C]. Proceedings of 26th Annual Conference on Neural Information Processing Systems, Nevada:NIPS, 2012: 1097-1105.</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;早在上世纪50年代，美国神经生物学家David Hubel通过研究猫和猴子的瞳孔区域与大脑皮层神经元的对应关系就发现视觉系统的信息处理方式是分级的。这一发现，促成了神经网络在图像处理上的发展。&lt;br&gt;&amp;emsp;&amp;emsp;神经网络的发展史可以分为三个阶段，第一个阶段是Frank Rosenblatt提出的感知机模型[1]，感知机模型的逻辑简单有效，但不能处理异或等非线性问题。第二个阶段是Rumelhart等提出的反向传播算法[2]，该算法使用梯度更新权值，使多层神经网络的训练成为可能。第三个阶段得益于计算机硬件的发展和大数据时代的到来，促进了深度神经网络的发展。</summary>
    
    
    
    <category term="深度学习笔记" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="卷积神经网络" scheme="https://oysz2016.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="BP算法" scheme="https://oysz2016.github.io/tags/BP%E7%AE%97%E6%B3%95/"/>
    
    <category term="感知机" scheme="https://oysz2016.github.io/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    
    <category term="卷积" scheme="https://oysz2016.github.io/tags/%E5%8D%B7%E7%A7%AF/"/>
    
    <category term="池化" scheme="https://oysz2016.github.io/tags/%E6%B1%A0%E5%8C%96/"/>
    
    <category term="全连接" scheme="https://oysz2016.github.io/tags/%E5%85%A8%E8%BF%9E%E6%8E%A5/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络模型解读汇总——LeNet5，AlexNet、ZFNet、VGG16、GoogLeNet和ResNet</title>
    <link href="https://oysz2016.github.io/post/d86a012b.html"/>
    <id>https://oysz2016.github.io/post/d86a012b.html</id>
    <published>2023-09-03T07:03:41.230Z</published>
    <updated>2019-03-17T05:49:23.698Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;在我的<a href="https://oysz2016.github.io/">个人博客</a>上一篇<a href="https://oysz2016.github.io/2018/04/25/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/">博文</a>中分析了卷积神经网络的结构与相关算法，知道了这些基本原理之后。这篇博文主要介绍在卷积神经网络的发展历程中一些经典的网络模型。</p><h2 id="LeNet5"><a href="#LeNet5" class="headerlink" title="LeNet5"></a>LeNet5</h2><p>&emsp;&emsp;LeCun等将BP算法应用到多层神经网络中，提出LeNet-5模型[1]（<a href="http://yann.lecun.com/exdb/lenet/index.html">效果和paper见此处</a>），并将其用于手写数字识别，卷积神经网络才算正式提出。LeNet-5的网络模型如图1所示。网络模型具体参数如图2所示。<br><span id="more"></span><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/LeNet5网络模型.png-chuli" alt="enter description here"></p><p><div align = center>图1 LeNet-5网络模型</div></p><p><div align = center>表1 LeNet-5具体参数</div><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/LeNet5具体参数.png-chuli" alt="enter description here"><br><strong>输入</strong>：32<em>32的手写字体图片，这些手写字体包含0~9数字，也就是相当于10个类别的图片;<br><em>*输出</em></em>：分类结果，0~9之间。<br>&emsp;&emsp;从输入输出可以知道，改网络解决的是一个十分类的问题，分类器使用的Softamx回归。</p><ul><li><strong>C1</strong>：卷积核参数如表所示。卷积的后的尺寸计算公式为：<script type="math/tex; mode=display">outHeight=(inHeight+2pad-filterHeight)/strides[1]+1</script><script type="math/tex; mode=display">outWidth=(inWidth+2pad-filterHidth)/strides[2] +1</script>&emsp;&emsp;因此，经过C1卷积层后，每个特征图大小为32-5+1=28，这一层输出的神经元个数为28<em>28</em>6=784。而这一层卷积操作的参数个数为5<em>5</em>1<em>6+6=156，其中参数个数与神经元个数无关，只与卷积核大小（此处为5</em>5），卷积核数量（此处为6，上一层图像默认深度为1）；</li><li><strong>S2</strong>：输入为28<em>28</em>6，该网络使用最大池化进行下采样，池化大小为2<em>2，经过池化操作后输出神经元个数为14</em>14*6；</li><li><strong>C3</strong>：经过C3层后，输出为10<em>10</em>16，参数个数为5<em>5</em>6*16+16=2416个参数；</li><li><strong>S4</strong>：输入为10<em>10</em>16，参数与S2层一致，池化后输出神经元个数为5<em>5</em>16；</li><li><strong>C5</strong>：经过C5层后，输出为1<em>1</em>120，参数个数为5<em>5</em>16<em>120+120=48120个参数。（这一层的卷积大小为5</em>5，图像的输入大小也为5*5，可等效为全连接层）；</li><li><strong>F6</strong>：输出为1<em>1</em>84，参数个数为1<em>1</em>120*84+84=10164<br>参数总量：60856</li></ul><p>&emsp;&emsp;从表1的具体参数可以看出，LeNet的网络结构十分简单且单一，卷积层C1、C3和C5层除了输出维数外采用的是相同的参数，池化层S2和S4采用的也是相同的参数</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>&emsp;&emsp;2012年Krizhevsky使用卷积神经网络在ILSVRC 2012图像分类大赛上夺冠，提出了AlexNet模型[2]（<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">论文地址</a>）。这篇文章凭借着诸多创新的方法，促使了之后的神经网络研究浪潮。AlexNet网络的提出对于卷积神经网络具有里程碑式的意义，相比较于LeNet5的改进有以下几点</p><ol><li>数据增强</li></ol><ul><li>水平翻转</li><li>随机裁剪、平移变换</li><li>颜色光照变换</li></ul><ol><li>Dropout： Dropout方法和数据增强一样，都是防止过拟合的。简单的说，dropout能按照一定的概率将神经元从网络中丢弃。一个很形象的解释如图2所示，左图为dropout前，右图为dropout后。dropout能在一定程度上防止网络过拟合，并且能加快网络的训练速度。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/dropout.png-chuli" alt="enter description here"><div align = center>图2 Dropout示意图</div></li><li>ReLU激活函数：ReLu具有一些优良特性，在为网络引入非线性的同时，也能引入稀疏性。稀疏性可以选择性激活和分布式激活神经元，能学习到相对稀疏的特征，起到自动化解离的效果。此外，ReLu的导数曲线在输入大于0时，函数的导数为1，这种特性能保证在输入大于0时梯度不衰减，从而避免或抑制网络训练时的梯度消失现象，网络模型的收敛速度会相对稳定[10]。</li><li>Local Response Normalization：Local Response Normalization要硬翻译的话是局部响应归一化，简称LRN，实际就是利用临近的数据做归一化。这个策略贡献了1.2%的Top-5错误率。</li><li>Overlapping Pooling：Overlapping的意思是有重叠，即Pooling的步长比Pooling Kernel的对应边要小。这个策略贡献了0.3%的Top-5错误率。</li><li>多GPU并行：这个太重要了，入坑了后发现深度学习真是“炼丹”的学科。得益于计算机硬件的发展，在我自己训练时，Gpu大概能比Cpu快一个数量级以上。能极大的加快网络训练。<br>AlextNet的网络结构如图3所示，具体参数如表2所示。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/AlexNet网络模型.png-chuli" alt="enter description here"><br><div align = center>图3 AlexNet网络模型</div><br><div align = center>表2 AlexNet具体参数</div><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/AlexNet具体参数2.png-chuli" alt="enter description here"><br><strong>输入</strong>：224<em>224</em>3（RGB图像），图像会经过预处理变为227<em>227</em>3;<br><strong>输出</strong>：使用的是ImageNet数据集，该数据集有1000个类，因此输出的类别也是1000个。<br>&emsp;&emsp;从输入输出可以知道，改网络解决的是一个十分类的问题，分类器使用的Softamx回归。<ul><li><strong>conv1</strong>：输出为55<em>55</em>96，参数个数为11<em>11</em>3*96+96=34944</li><li><strong>pool1</strong>：输出为27<em>27</em>96；</li><li><strong>conv2</strong>：输出为27<em>27</em>256，参数个数为5<em>5</em>96*256+256=614656</li><li><strong>pool2</strong>：输出为13<em>13</em>256；</li><li><strong>conv3</strong>：输出为13<em>13</em>384，参数个数为3<em>3</em>256*384+384=885120</li><li><strong>conv4</strong>：输出为13<em>13</em>384，参数个数为3<em>3</em>384*384+384=1327488</li><li><strong>conv5</strong>：输出为13<em>13</em>256，参数个数为3<em>3</em>384*256+256=884992</li><li><strong>pool3</strong>：输出为6<em>6</em>256；</li><li><strong>fc6</strong>：输出为1<em>1</em>4096，参数个数为1<em>1</em>256*4096+4096=1052672</li><li><strong>fc7</strong>：输出为1<em>1</em>4096，参数个数为1<em>1</em>4096*4096+4096=16781312<br>参数总量：21581184</li></ul></li></ol><p>&emsp;&emsp;通过对比LeNet-5和AlexNet的网络结构可以看出，AlexNet具有更深的网络结构，更多的参数。</p><h2 id="ZFNet"><a href="#ZFNet" class="headerlink" title="ZFNet"></a>ZFNet</h2><p>&emsp;&emsp;ZFNet[3]（<a href="https://arxiv.org/pdf/1311.2901.pdf">论文地址</a>）是由纽约大学的Matthew Zeiler和Rob Fergus所设计，该网络在AlexNet上进行了微小的改进，但这篇文章主要贡献在于在一定程度上解释了卷积神经网络为什么有效，以及如何提高网络的性能。该网络的贡献在于：</p><ul><li>使用了反卷积网络，可视化了特征图。通过特征图证明了浅层网络学习到了图像的边缘、颜色和纹理特征，高层网络学习到了图像的抽象特征；</li><li>根据特征可视化，提出AlexNet第一个卷积层卷积核太大，导致提取到的特征模糊；</li><li>通过几组遮挡实验，对比分析找出了图像的关键部位；</li><li>论证了更深的网络模型，具有更好的性能。</li></ul><p>&emsp;&emsp;ZFNet的网络模型如图4所示，具体参数如表3所示。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ZFNet网络模型.jpg-chuli" alt="enter description here"> <div align = center>图4 ZFNet网络模型</div><br>   <div align = center>表3 ZFNet具体参数</div><br> <img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ZFNet具体参数.png-chuli" alt="enter description here"><br>&emsp;&emsp;ZFNet的网络模型与AlexNet十分相似，这里就不列举每一层的输入输出了。</p><h2 id="VGG16"><a href="#VGG16" class="headerlink" title="VGG16"></a>VGG16</h2><p>&emsp;&emsp;VGGNet[4]是由牛津大学计算机视觉组和Google DeepMind项目的研究员共同研发的卷积神经网络模型，包含VGG16和VGG19两种模型，其网络模型如图5所示，<a href="http://ethereon.github.io/netscope/#/gist/dc5003de6943ea5a6b8b">也可以点击此处链接</a>查看网络模型。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/Vgg16网络结构2.jpg-chuli" alt="enter description here"><div align = center>图5 VGG16网络模型</div><br>&emsp;&emsp;从网络模型可以看出，VGG16相比AlexNet类的模型具有较深的深度，通过反复堆叠3<em>3的卷积层和2</em>2的池化层，VGG16构建了较深层次的网络结构，整个网络的卷积核使用了一致的3<em>3的尺寸，最大池化层尺寸也一致为2</em>2。与AlexNet主要有以下不同：</p><ul><li>Vgg16有16层网络，AlexNet只有8层；</li><li>在训练和测试时使用了多尺度做数据增强。</li></ul><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p>&emsp;&emsp;GoogLeNet[5]（<a href="https://arxiv.org/pdf/1409.4842.pdf">论文地址</a>）进一步增加了网络模型的深度和宽度，但是单纯的在VGG16的基础上增加网络的宽度深度会带来以下缺陷：</p><ul><li>过多的参数容易引起过拟合；</li><li>层数的加深，容易引起梯度消失现象。</li></ul><p>&emsp;&emsp;GoogLeNet的提出受到论文Network in Network（NIN）的启发，NIN有两个贡献：</p><ul><li>提出多层感知卷积层：使用卷积层后加上多层感知机，增强网络提取特征的能力。普通的卷积层和多层感知卷积层的结构图如图6所示，Mlpconv相当于在一般的卷积层后加了一个1*1的卷积层；<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/多层感知机.png-chuli" alt="enter description here"><div align = center>图6 普通卷积层和多层感知卷积层结构图</div></li><li>提出了全局平均池化替代全连接层，从上文计算的LeNet5，AlexNet网络各层的参数数量发现，全连接层具有大量的参数。使用全局平均池化替代全连接层，能很大程度减少参数空间，便于加深网络，还能防止过拟合。</li></ul><p>&emsp;&emsp;GoogLeNet根据Mlpconv的思想提出了Inception结构，该结构有两个版本，图7是Inception的naive版。该结构巧妙的将1<em>1、3</em>3和5*5三种卷积核和最大池化层结合起来作为一层结构。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/Inception_naive.png-chuli" alt="enter description here"></p><p><div align = center>图7 Inception结构的naive版</div><br>&emsp;&emsp;然而Inception的naive版中5<em>5的卷积核会带来很大的计算量，因此采用了与NIN类似的结构，在原始的卷积层之后加上了1</em>1卷积层，最终版本的Inception如图8所示。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/Inception.png-chuli" alt="图8 降维后的Inception模块"></p><p><div align = center>图8 降维后的Inception模块</div><br>&emsp;&emsp;GoogLeNet的模型结构如图9所示，详细参数如表4所示。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GoogLeNet网络模型.jpg-chuli" alt="enter description here"></p><p><div align = center>图9 GoogLeNet模型结构</div></p><p><div align = center>表4 GoogLeNet具体参数</div><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GoogleNet具体参数.png-chuli" alt="enter description here"></p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>&emsp;&emsp;卷积神经网络模型的发展历程一次次证明加深网络的深度和宽度能得到更好的效果，但是后来的研究发现，网络层次较深的网络模型的效果反而会不如较浅层的网络，称为“退化”现象，如图10所示。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/退化现象.jpg-chuli" alt="enter description here"></p><p><div align = center>图10 退化现象</div><br>&emsp;&emsp;退化现象产生的原因在于当模型的结构变得复杂时，随机梯度下降的优化变得更加困难，导致网络模型的效果反而不如浅层网络。针对这个问题，MSRA何凯明团队提出了Residual Networks<a href="[论文地址](https://arxiv.org/pdf/1512.03385.pdf">6</a>)。该网络具有Residual结构如图11所示。</p><p> <img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/Residual结构.png-chuli" alt="enter description here"></p><p><div align = center>图11 Residual 结构</div><br>&emsp;&emsp;ResNet的基本思想是引入了能够跳过一层或多层的“shortcut connection”，即增加一个identity mapping（恒等映射），将原始所需要学的函数H(x)转换成F(x)+x，而作者认为这两种表达的效果相同，但是优化的难度却并不相同，作者假设F(x)的优化 会比H(x)简单的多。这一想法也是源于图像处理中的残差向量编码，通过一个reformulation，将一个问题分解成多个尺度直接的残差问题，能够很好的起到优化训练的效果。<br>&emsp;&emsp;这个Residual block通过shortcut connection实现，通过shortcut将这个block的输入和输出进行一个element-wise的加叠，这个简单的加法并不会给网络增加额外的参数和计算量，同时却可以大大增加模型的训练速度、提高训练效果，并且当模型的层数加深时，这个简单的结构能够很好的解决退化问题。<br>&emsp;&emsp;首先构建了一个18层和一个34层的plain网络，即将所有层进行简单的铺叠，然后构建了一个18层和一个34层的residual网络，仅仅是在plain上插入了shortcut，而且这两个网络的参数量、计算量相同，并且和之前有很好效果的VGG-19相比，计算量要小很多。（36亿FLOPs VS 196亿FLOPs，FLOPs即每秒浮点运算次数。）这也是作者反复强调的地方，也是这个模型最大的优势所在。<br>&emsp;&emsp;模型构建好后进行实验，在plain上观测到明显的退化现象，而且ResNet上不仅没有退化，34层网络的效果反而比18层的更好，而且不仅如此，ResNet的收敛速度比plain的要快得多。<br>对于shortcut的方式，作者提出了三个策略：</p><ul><li>使用恒等映射，如果residual block的输入输出维度不一致，对增加的维度用0来填充；</li><li>在block输入输出维度一致时使用恒等映射，不一致时使用线性投影以保证维度一致；</li><li>对于所有的block均使用线性投影。<br>ResNet论文的最后探讨了阻碍网络更深的瓶颈问题，如图12所示，论文中用三个1x1,3x3,1x1的卷积层代替前面说的两个3x3卷积层，第一个1x1用来降低维度，第三个1x1用来增加维度，这样可以保证中间的3x3卷积层拥有比较小的输入输出维度。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/更深的residual block.png-chuli" alt="enter description here"><br><div align = center>图12 更深的residual block</div><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>[1] Lecun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.<br>[2] Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems. Curran Associates Inc. 2012:1097-1105.<br>[3] Zeiler M D, Fergus R. Visualizing and Understanding Convolutional Networks[J]. 2013, 8689:818-833.<br>[4] Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.<br>[5] Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2015:1-9.<br>[6] He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]// Computer Vision and Pattern Recognition. IEEE, 2016:770-778.</li></ul><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;emsp;&amp;emsp;在我的&lt;a href=&quot;https://oysz2016.github.io/&quot;&gt;个人博客&lt;/a&gt;上一篇&lt;a href=&quot;https://oysz2016.github.io/2018/04/25/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/&quot;&gt;博文&lt;/a&gt;中分析了卷积神经网络的结构与相关算法，知道了这些基本原理之后。这篇博文主要介绍在卷积神经网络的发展历程中一些经典的网络模型。&lt;/p&gt;
&lt;h2 id=&quot;LeNet5&quot;&gt;&lt;a href=&quot;#LeNet5&quot; class=&quot;headerlink&quot; title=&quot;LeNet5&quot;&gt;&lt;/a&gt;LeNet5&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;LeCun等将BP算法应用到多层神经网络中，提出LeNet-5模型[1]（&lt;a href=&quot;http://yann.lecun.com/exdb/lenet/index.html&quot;&gt;效果和paper见此处&lt;/a&gt;），并将其用于手写数字识别，卷积神经网络才算正式提出。LeNet-5的网络模型如图1所示。网络模型具体参数如图2所示。&lt;br&gt;</summary>
    
    
    
    <category term="深度学习笔记" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="卷积神经网络" scheme="https://oysz2016.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="LeNet5" scheme="https://oysz2016.github.io/tags/LeNet5/"/>
    
    <category term="AlexNet" scheme="https://oysz2016.github.io/tags/AlexNet/"/>
    
    <category term="ZFNet" scheme="https://oysz2016.github.io/tags/ZFNet/"/>
    
    <category term="VGG16" scheme="https://oysz2016.github.io/tags/VGG16/"/>
    
    <category term="GoogLeNet" scheme="https://oysz2016.github.io/tags/GoogLeNet/"/>
    
    <category term="ResNet" scheme="https://oysz2016.github.io/tags/ResNet/"/>
    
  </entry>
  
  <entry>
    <title>半监督学习汇总——检测</title>
    <link href="https://oysz2016.github.io/post/bcf25936.html"/>
    <id>https://oysz2016.github.io/post/bcf25936.html</id>
    <published>2023-09-03T07:03:41.210Z</published>
    <updated>2023-02-18T03:17:47.797Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span><br>上一篇文章中介绍了<a href="https://oysz2016.github.io/post/18365ad2.html#more">半监督学习——分类</a>的一些文章，今天来看一下半监督学习应用在检测领域的一些思路。</p><h1 id="STAC"><a href="#STAC" class="headerlink" title="STAC"></a>STAC</h1><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2005.04757">https://arxiv.org/abs/2005.04757</a></p><p><strong>代码链接:</strong><a href="https://github.com/google-research/ssl_detection/">https://github.com/google-research/ssl_detection/</a></p><p>之前的半监督学习(Semi-supervised learning, SSL)基本应用在分类领域，而STAC是在目标检测领域应用半监督学习的论文, 该细分领域为半监督目标检测(Semi-supervised Object Detection, SS-OD)。<strong>其思想和分类中使用的一致性学习比较相似，也分为teacher网络和student网络</strong>，思想如下图所示</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/1.png" alt=""></p><p>流程如下:</p><ul><li>用有标签数据训练一个teacher模型</li><li>用训练好的teacher模型在无标签的数据上生成伪标签(包含bbox，label)</li><li>对无标签图片应用强数据增广，在几何变换的增广时，bbox也要做相应的增广</li><li>训练时计算无标签的损失和有监督的损失</li></ul><p>下图是应用的一些增广的可视化<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/2.png" alt=""></p><p>下图对比了有监督的模型，增加RandAugment和使用STAC性能的变化<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/3.png" alt=""></p><h1 id="soft-teacher"><a href="#soft-teacher" class="headerlink" title="soft teacher"></a>soft teacher</h1><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2106.09018">https://arxiv.org/abs/2106.09018</a></p><p><strong>代码链接:</strong><a href="https://github.com/microsoft/SoftTeacher">https://github.com/microsoft/SoftTeacher</a></p><p>soft teacher可以看作是对STAC的改进，STAC的方法简单有效，但是teacher模型和student模型是分阶段训练的。由于student模型会依赖于teacher模型在无标签数据上生成的伪标签，若teacher模型生成了错误且置信度高的伪标签，会造成误差的传递，这也是分阶段训练方式的通病。因此，soft teacher认为端到端训练能取得更好的效果。</p><p>soft teacher的网络结构如下图所示:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/4.png" alt=""></p><p>teacher模型和student模型采用完全相同的结构，其流程为:</p><ul><li>对于无标签数据，弱数据增广后使用teacher模型，nms后得到检测结果</li><li>soft teacher改进的原因主要是考虑到错误的伪标签对student网络学习非常不利，因此使用了”soft”, 具体而言是使用了可靠性度量作为伪标签的权重，而不是所有的伪标签同等重要。可靠性度量采用的是检测的分数</li><li>对于分类和边框回归两个任务，分别使用了两种度量方式保证生成的伪标签的可靠性<ul><li>对于分类分支，使用分类置信度作为判断标准，设置较高的阈值能提升分类标签的可靠性</li><li>对于检测分支, 采用的是box jittering的方式，具体而言，会在候选框周围生成一定偏移量的新bbox，并将新bbox通过teacher模型修正bbox的结果。将该过程重复n次，计算n次修正之后bbox的方差，公式如下。方差越小代表bbox的可信度越高，figuer3(c)的图可以反映方差和bbox预测效果的关系。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/5.png" alt=""></li></ul></li></ul><p>如下图(b)所示, 候选框的定位精度和分类置信度没有很强的相关性，这点在很多检测论文中都有提及并且有优化，例如IOU Net, yolox中的Decoupled head<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/6.png" alt=""></p><p>从实验结果可以看出，相比STAC有较大提升<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/7.png" alt=""></p><h1 id="Dense-teacher"><a href="#Dense-teacher" class="headerlink" title="Dense teacher"></a>Dense teacher</h1><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2106.09018">https://arxiv.org/abs/2106.09018</a></p><p><strong>代码链接:</strong><a href="https://github.com/microsoft/SoftTeacher">https://github.com/microsoft/SoftTeacher</a><br>之前基于伪标签分类具有以下问题:</p><ul><li>Thresholding: 阈值用来区分teacher模型在无标签数据上生成的分类和bbox结果应该如何取舍。作为预定义的数值，阈值非常难选取。设置的较大，会导致召回难以提升；设置的较小，精度会难以提升。</li><li>NMS: NMS的问题和threshold相似，都是预定义的值。设置的较大，会过滤掉一些稍微密集的正样本，较小会有些假正例被保留下来</li><li>Label Assign: </li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/8.png" alt=""></p><p>说了之前伪标签这么多的缺点，接下来看看dense teacher是怎么做的。<br>在dense teacher中提出了密集伪标签(dense pseudo-label, DPL),区别于之前伪标签和有标注数据形式一致。伪标签类似FCOS的中间产物，具体而言将feature map通过cls head输出logits, 再通过sigmoid生成dense label。生成的sigmoid数值是连续的，因此需要使用Quality Focal Loss(Generalized Focal Loss)作为损失函数监督student模型训练。<strong>需要注意的是dense label并不是feautre输出的是每个点的分类和回归结果，类似FCOS的训练方式</strong>。dense label如下图中绿色区域</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/9.png" alt=""></p><p>DPL中有丰富的信息，但由于没有阈值的过滤，也保留了很多低分的信息。直观上看，这些低分信息对模型帮助很小，这部分区域可以看作难区分的负样本，文中也证明了保留该信息作为监督信号会损害student模型的学习。因此根据teacher模型的特征丰富度得分(Feature Richness Score, FRS),划分learning region和suppressing region。具体的，通过FRS选取top k%的像素作为学习区域，其他区域被抑制为0.</p><p>该方法相比之前方法也有一定提升，但由于伪标签的生成和FCOS相似，不太适用于rcnn等两阶段基于anchor的方法</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/10.png" alt=""></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>由于目标检测包含了分类和定位两个任务，因此目标检测场景下的半监督学习和分类场景还是非常相似的。整体思路是使用伪标签/一致性的方法对无标签数据监督。STAC将分类中半监督的方法借鉴过来，在检测领域的半监督取得了很好的效果；而soft teacher进一步思考伪标签的合理性，避免teacher模型误差的传播，区别于分类模型，在额外的定位任务上思考如何评估定位的准确性；Dense teacher则思考半监督学习和检测任务中的阈值对模型整体效果的影响，整体思路也借鉴了anchor free的FCOS模型的思想，进一步提升了半监督带来的收益。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="目标检测" scheme="https://oysz2016.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    <category term="半监督" scheme="https://oysz2016.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3/"/>
    
  </entry>
  
  <entry>
    <title>半监督学习汇总——分类</title>
    <link href="https://oysz2016.github.io/post/18365ad2.html"/>
    <id>https://oysz2016.github.io/post/18365ad2.html</id>
    <published>2023-09-03T07:03:41.191Z</published>
    <updated>2023-02-18T03:17:16.009Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>半监督学习是机器学习中很常用的一种算法，之前有了解些大概，但没系统性的看过该领域的文章。趁着有空总结了下该领域的文章，由于文章比较多，整理完会分成两部分:</p><ul><li><strong>半监督学习——分类</strong></li><li><strong>半监督学习——检测</strong><br>这篇文章是分类，对半监督学习感兴趣的朋友可以期待下。</li></ul><h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><p><strong>监督算法:</strong> 监督算法的训练数据是有标签的，目的是让模型能正确预测数据的标签</p><p><strong>无监督算法:</strong> 训练数据没有标签，根据数据自身的特性设计模型进行分类</p><p><strong>半监督算法:</strong> 半监督算法是介于无监督和监督算法之间的一种算法类型。其特点是有少量有标签的数据，以及大量的无标签数据，可以得到只用有标签数据训练更好的结果</p><h1 id="π-model"><a href="#π-model" class="headerlink" title="π model"></a>π model</h1><p>π model网络结构如下图所示。因为是半监督学习，既有有标签数据，也有无标签数据，π model将网络分为了两部分:</p><ul><li>第一部分为图中使用交叉熵损失。该部分为有label的数据$x_i$，$z_i$为模型预测结果，用于和$y_i$比对，使用corss entropy</li><li>第二部分使用MSE损失。该部分为无label的数据$x_i$，$x_i$会经过数据增广和模型中的drop out，这两部分都是为了引入一定的数据差异。同一个输入经过两次增广和dropout之后得到特征$z_i$和$\widetilde{z_i}$，使用MSE loss，目的是使增广前后的特征尽可能的相似。<br>两个损失会经过weighted sum配置合适的权重，得到最终的loss</li></ul><p>下面是π model的伪代码</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/2.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/3.png" alt=""></p><h1 id="Temporal-ensembling"><a href="#Temporal-ensembling" class="headerlink" title="Temporal ensembling"></a>Temporal ensembling</h1><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/1610.02242">https://arxiv.org/abs/1610.02242</a></p><p><strong>代码链接:</strong> <a href="https://github.com/ferretj/temporal-ensembling">https://github.com/ferretj/temporal-ensembling</a><br>Temporal ensembling的方法和π model非常相似，核心思想都是想利用好有监督的数据和无监督的数据，改进的原因是在π model中使用了两次模型增广和推理，耗时比较长，在Temporal ensembling中使用了<strong>Temporal</strong>的模型，具体而言是使用了之前epoch得到的特征结果去做对比，模型结构如下图:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/4.png" alt=""></p><p>$\widetilde{z_i}$为之前epoch模型保存的特征，$z_i$为当前模型的推理结果，并且会保存给下一个epoch使用</p><p>伪代码如下:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/5.png" alt=""><br>注意, 在更新特征z时，不光用到了当前的特征，也会考虑到之前所有的特征，具体可以看伪代码最后几行</p><p>在大量半监督数据上做了消融实验，验证了Temporal ensembling的有效性</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/6.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/7.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/8.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/9.png" alt=""></p><h1 id="Mean-teacher"><a href="#Mean-teacher" class="headerlink" title="Mean teacher"></a>Mean teacher</h1><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/1703.01780">https://arxiv.org/abs/1703.01780</a></p><p><strong>代码链接:</strong> <a href="https://github.com/CuriousAI/mean-teacher">https://github.com/CuriousAI/mean-teacher</a></p><p>mean teacher的思想和Temporal ensembling也非常相似。改进的原因是在Temporal ensembling中每个epoch才会更新一次对比的特征，在大型的数据集上训练时，用于对比的特征更新的非常慢，时间成本也很高，因此在mean teacher中使用更新模型的方式取代原来更新特征的方式。思想如下图所示</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/10.png" alt=""></p><p>student model即当前模型，teacher model为更新的模型，会参考当前模型和之前模型的权重，得到一个新的模型。更新的方式用公式</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/11.png" alt=""></p><p>目的也是使student mode和teacher model预测出的特征一致</p><p>mean teache和之前的工作π model以及Temporal ensembling进行了对比</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/12.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/13.png" alt=""></p><h1 id="MixMatch"><a href="#MixMatch" class="headerlink" title="MixMatch"></a>MixMatch</h1><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/1905.02249">https://arxiv.org/abs/1905.02249</a></p><p><strong>代码链接:</strong> <a href="https://github.com/YU1ut/MixMatch-pytorch">https://github.com/YU1ut/MixMatch-pytorch</a></p><p>mixmatch是最小化熵的方法，和前面介绍的三篇一致性正则化法有些差异。最小化熵方法的思想基于机器学习中的一个共识。即分类器的边界边际分布的高密度区域(说人话就是不能从类中心去划分边界)。因此强迫分类器对未标记数据做出低熵预测。实现方法是在损失函数中增加一项，最小化$p_{model}(y|x)$对应的熵。</p><p>mixmatch的思想在下图中伪代码展示的很清楚</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/14.png" alt=""></p><p>一个batch中有B份有标签的数据X和B份无标签的数据U</p><ul><li>将有标签的数据增广一次</li><li>将无标签的数据增广K次</li><li>对无标签的数据增广后的结果模型推理后分类，对K个结果取平均，得到模型预测的标签</li><li>使用temperature sharpening算法，对上一步得到的标签后处理。sharpening算法公式如下:</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/15.png" alt=""></p><ul><li>增强后带标签的数据组成一个batch，即$\widetilde{X}$</li><li>增强后无标签的数据，和预测出的标签组成K个batch，即$\widetilde{U}$</li><li>将$\widetilde{X}$和$\widetilde{U}$混合，得到新的数据集W</li><li>应用mixup将增广后有标签的数据$\widetilde{X}$和新的数据W混合得到$X^{‘}$</li><li>应用mixup将增广后无标签的数据$\widetilde{U}$和新的数据W混合得到$U^{‘}$</li></ul><p>得到两个新的数据集后就可以按mixup混合的思路进行训练</p><p>sharpening的大致流程如下图所示</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/16.png" alt=""></p><p>接下来是实验结果部分:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/17.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/18.png" alt=""></p><h1 id="ReMixMatch"><a href="#ReMixMatch" class="headerlink" title="ReMixMatch"></a>ReMixMatch</h1><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/1911.09785">https://arxiv.org/abs/1911.09785</a></p><p><strong>代码链接:</strong><a href="https://github.com/google-research/remixmatch">https://github.com/google-research/remixmatch</a></p><p>从名字上可以看出ReMixMatch是MixMatch的改进版本。MixMatch中比较重要的思想是猜测无标签数据的标签，使用最小化熵的方法做训练。<br>ReMixMatch的改进主要包含两部分: Distribution Alignment和Augmentation Anchor。伪代码如下:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/19.png" alt=""></p><ul><li><strong>Distribution Alignment</strong>。考虑到猜测无标签数据的label有可能存在噪声，因此考虑到使用有标签数据的标签分布，对无标签的猜测进行对齐。对应代码中的第7行。如下图所示，$q_b$对应Label guess。$\widetilde{p}(y)$是一个运行平均版本的无标签猜测，$p(y)$是有标签数据的标签分布。对齐之后的标签猜测如下公式<script type="math/tex; mode=display">q_b=Normalize(q_b*\frac{p(y)}{\widetilde{p}(y)})</script></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/20.png" alt=""></p><ul><li><strong>Augmentation Anchor</strong>。考虑到简单数据增广得到的预测结果会比复杂增广得到的预测结果要更加可信。因此对于一张图片，首先会进行弱增广，再进行多次强增广。弱增广和强增广共同使用一个标签进行mixup和模型训练</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/21.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/22.png" alt=""></p><p>接下来是实验结果部分:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/23.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/24.png" alt=""></p><h1 id="FixMatch"><a href="#FixMatch" class="headerlink" title="FixMatch"></a>FixMatch</h1><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2001.07685">https://arxiv.org/abs/2001.07685</a></p><p><strong>代码链接:</strong><a href="https://github.com/google-research/fixmatch">https://github.com/google-research/fixmatch</a></p><p>在FixMatch中没有采用MixMatch系列中有标签数据和无标签数据猜测标签后互相mixup作为训练样本的方法，本身思想和mean teacher及之前方法更相似。<strong>FixMatch中的Fix主要强调的是混合两种数据增广方式，分别是弱增广和强增广</strong><br>弱数据增广的方式:</p><ul><li>平移</li><li>反转</li><li>平移&amp;反转<br>强数据增广:</li><li>cutout</li><li>random augment</li><li>control theory augment<br>其中若数据增广和强数据增广的前两种方式都比较常见，这里详细说下control theory augment数据增广方法:</li><li>该数据增广方法中有18个候选集，例如旋转，色彩变换等</li><li>初始化transform的权重$W=[w_1,w_2,…,w_18]$</li><li>随机选择其中两个增广方法i和j，增广的权重分别为$w_i/(w_i+w_j)$, $w_j/(w_i+w_j)$</li><li>将两个增广图像混合，更新transform的权重$W$, 第一个公式可以看作MAE损失，第二个公式是用动量的方式更新transform的权重。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/30.png" alt=""></li></ul><p>因此可以将control theory augment看作是可以学习的rand augment, 能学习出好的增广方式，使得分类更准确。为什么不用auto augment呢？因为auto augment需要标签作为学习的条件，而半监督中大部分数据都没有标签</p><p>对于无标签的数据，会先经过弱数据增广获取伪标签，只有某一类的置信度大于一定的阈值才会执行为标签的生成（下图中红色部分），生成的伪标签用于监督强增广的输出值。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/25.png" alt=""></p><p>接下来是实验结果部分:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/26.png" alt=""></p><h1 id="Noisy-Student"><a href="#Noisy-Student" class="headerlink" title="Noisy Student"></a>Noisy Student</h1><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/1911.04252">https://arxiv.org/abs/1911.04252</a></p><p><strong>代码链接:</strong><a href="https://github.com/google-research/noisystudent">https://github.com/google-research/noisystudent</a></p><p>Noisy Student和之前的一致性正则化法也非常相似。区别在于强调了在student模型中加入噪声。在这篇文章中有teacher model和student model的概念。teacher是使用有标签数据训练的模型，student是使用teacher模型预测了无标签数据的标签后，用无标签数据训练的模型。</p><p>而Noisy Student强调的是在student模型中加入噪声，teacher模型和student模型可以用不同的模型训练，也可以使用相同的模型。要思考加入噪声的原因，可以假设teacher和student相同结构联合训练的场景，则student模型不加入噪声，则预测结果和通过teacher模型生成的伪标签会完全一致，student模型就失去了更新的动力。所以加入噪声是很有必要的。</p><p>Noisy Student的思想如下图所示:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/27.png" alt=""></p><p>关于student模型使用数据增广和dropout的消融实验如下：</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/28.png" alt=""></p><p>该文章列举了一些消融实验得到的经验，基本都很符合直觉:</p><ul><li>使用性能好的teacher模型能得到更好的结果</li><li>大量的无标签数据是必要的</li><li>在某些场景下，soft伪标签比hard伪标签效果要好</li><li>大的学生模型很重要</li><li>数据均衡对小模型很重要</li><li>有标签数据和无标签数据联合训练效果更好</li><li>无标签数据:有标签数据的比值越大，该方法越有效</li><li>从头开始训练student有时比用teacher初始化student效果要好</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过阅读上面的7篇文章，半监督学习大概可以分成3种训练方式，分别是<strong>一致性正则(Consistency Regularization Model)</strong>, <strong>伪标签模型(Proxy Label Model)</strong>, <strong>一致性正则&amp;伪标签模型</strong></p><ul><li><strong>一致性正则:</strong> 图片增广前后，模型的预测结果应该相同。在该种方法中一般会加入数据增广和dropout，引入随机性，损失函数会使用MSE Loss。例如π model，Temporal ensembling，Mean teacher都是该类型的方法</li><li><strong>伪标签模型:</strong> 先在有标签的数据上做训练，然后预测无标签数据的伪标签；模型的训练包括有标签的训练和伪标签的训练。例如MixMatch、ReMixmatch都是该类型的方法</li></ul><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="半监督" scheme="https://oysz2016.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3/"/>
    
    <category term="分类" scheme="https://oysz2016.github.io/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>二维图像分割之分式分割</title>
    <link href="https://oysz2016.github.io/post/7f9c7afb.html"/>
    <id>https://oysz2016.github.io/post/7f9c7afb.html</id>
    <published>2023-09-03T07:03:41.186Z</published>
    <updated>2019-03-17T05:48:57.690Z</updated>
    
    <content type="html"><![CDATA[<p class="description"></p><span id="more"></span><p>&emsp;&emsp;从上周开始研究各种数学式子的切割，包括分式，竖式和脱式。本着由易到难的原则，开始做分式切割的调研。除了做PPT，写文档外，也将部分调研的结果整理成博文。</p><h2 id="图像数学公式定位的关键问题"><a href="#图像数学公式定位的关键问题" class="headerlink" title="图像数学公式定位的关键问题"></a>图像数学公式定位的关键问题</h2><p>&emsp;&emsp;一般地，公式定位流程如下：</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215085.png" alt="数学公式定位流程"></p><ul><li>预处理：包括灰度化、二值化、去除噪声和倾斜矫正等步骤</li><li>统计版面参数：包括字符的位置、字符的尺寸和相邻字符间的间隔等</li><li>版面分析：标注出字符域、图像域、表格域和图形域等</li><li>行提取：对字符域以行为单位进行划分</li><li>定位孤立数学公式：从提取的行中区分孤立数学公式行</li><li>定位内嵌公式：从非孤立数学公式行中区分出含数学公式的文本行</li></ul><h2 id="公式定位基本算法"><a href="#公式定位基本算法" class="headerlink" title="公式定位基本算法"></a>公式定位基本算法</h2><ol><li>预处理：对图像灰度化、二值化、倾斜校正、去噪等处理，使其更有利于数学公式定位</li><li><p>数学公式字符块提取<br> 1.行提取</p><ul><li>进行联通区域搜索，得到图片中所有连通区域</li><li>合并具有相交或包含关系的连通区域</li><li>根据连通区域宽高度统计直方图得到版面的字符宽、高度阈值<code>threshold_w</code>和<code>threshold_h</code>，据此去除图片中的无关信息，得到公式的候选区域</li><li>根据候选字符的连通区域的位置关系，合并候选字符连通区域，提取本文行及相应参数：行间距<code>Line_d</code>，行内相邻连通区域水平间隔平均值<code>threshold_d</code></li></ul><p>2.行内字符块提取:行内字符块提取是根据<code>threshold_d</code>将文本中的字符连通区域合并成字符块<br>3.后处理：数学公式字符块合并。对于分式，若将分数线与分子分母分别切割，可以根据其上下相邻行为单字符或行内字符均处于具有二维运算结构的运算符作用范围内，且两者之间的垂直距离小于行距<code>Line_d</code>的特点将其合并。</p></li></ol><p>&emsp;&emsp;图像倾斜的影响：当倾斜角增加时，数学公式定位准确率急剧下降。这是由于在数学公式定位算法采用连通区域空间位置关系特征提取文本行，很容易受到倾斜影响，进而影响公式定位准确性，因此<strong>首先需要对拍照图片进行测斜和校正处理</strong>，以确保公式定位算法的鲁棒性。</p><h2 id="公式定位错误的校正方法"><a href="#公式定位错误的校正方法" class="headerlink" title="公式定位错误的校正方法"></a>公式定位错误的校正方法</h2><ul><li>考虑根据图片中印刷体字符的大小，近似判断拍照的远近，从而确定各个阈值的大小</li><li>影响最大的参数有行内相邻连通区域的水平间隔<code>Character_dist</code>,可以考虑将行内连通区域按从左到右排列，然后计算版面中相邻连通区域的间隔，取数量最多的连通区域间隔座位阈值<code>Character_dist</code>。对与行间距阈值<code>Line_d</code>也可以用类似方法</li></ul><h2 id="分式分割的难点"><a href="#分式分割的难点" class="headerlink" title="分式分割的难点"></a>分式分割的难点</h2><h3 id="印刷体"><a href="#印刷体" class="headerlink" title="印刷体"></a>印刷体</h3><p>&emsp;&emsp;对于印刷体分式，如下图，如同印刷体文本的字符分割算法一样。采取水平投影的方式即可找到分割线。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1532749752156.jpg" alt="enter description here"></p><h3 id="手写体"><a href="#手写体" class="headerlink" title="手写体"></a>手写体</h3><p>&emsp;&emsp;分式分割的难点主要集中再手写体分式上。手写体的格式没有印刷体那么规范，分子与分母经常会出现粘合，带分式中也会出现整数与分数线、分数粘合在一起的情况，因此无法直接通过投影拆分分式。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215086.png" alt="enter description here"></p><h2 id="分式分割的处理思路"><a href="#分式分割的处理思路" class="headerlink" title="分式分割的处理思路"></a>分式分割的处理思路</h2><p>&emsp;&emsp;分式分割的关键点是找到分数线，分数线能作为带分数中整数与分数分割的参考，且能作为分数部分的分割线。因此分数线具有重要的参考价值。</p><p>&emsp;&emsp;考虑到手写体中分数线扭曲，各部分粘合的情况调研了许多直线检测的算法。有时间会做系统的整理。在这里线整理几种常用的直线检测算法。</p><h3 id="霍夫变换"><a href="#霍夫变换" class="headerlink" title="霍夫变换"></a>霍夫变换</h3><p>&emsp;&emsp;Hough是最经典也是应用最广泛的直线检测算法，hough使用极坐标的方式表示直线。极坐标下，直线的表达式可定义为：$\left(-\frac{cos\theta}{sin\theta}\right)x+\left(\frac{r}{sin\theta}\right)$，化简可得到$r=xcos\theta+ysin\theta$。对于每一点$(x_0,y_0)$，可以将通过该点的直线定义为$r_\theta=x_0cos\theta+y_0sin\theta$。</p><p>&emsp;&emsp;通过以上的推导，意味着每一对极坐标的参数$(r_\theta, \theta)$代表着一条通过$(x_0,y_0)$的直线。对于每个定点$(x_0,y_0)$，画出通过该点的所有直线并以极坐标表示，会得到一条正弦曲线。因此越多的点具有所描绘的正弦曲线相交，意为着这些点能组成平面内的一条直线。<br>&emsp;&emsp;如下图所示，点$x_1=9$,$y_1=4$，点$x_1=12$，$y_1=$以及$x_1=8$,$y_1=6$所描绘的通过它们的所有直线的正弦曲线。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215087.png" alt="enter description here"><br>hough的做法是追踪图像中每个点对应曲线间的交点，如果这些交点的数量超过了一点的阈值，即认为交点的参数$(r_\theta, \theta)$在原图像中为一条直线。</p><p>&emsp;&emsp;Hough变换的基本思想是利用图像的全局特征将特定形状的边缘连接起来，Hough通过点线的对偶性，将原图像中的点映射到用于累加的参数空间，将在原图像中寻找特定曲线的检测转化为寻找参数空间中的峰值问题。Hough的优点和缺点都来源于全局特征，因为全局特征，Hough提取的曲线受噪声和边界的影响较小，具有较好的鲁棒性，但也会带来效率低的缺点</p><h3 id="LSD算法"><a href="#LSD算法" class="headerlink" title="LSD算法"></a>LSD算法</h3><p>&emsp;&emsp;LSD发表于2012年，算是较新的直线检测算法。与Hough利用全局特征不同，LSD是一种局部提取直线的算法，在线性时间(liner-time)内能得到亚像素级准确度的直线。<br>&emsp;&emsp;LSD算法的流程如下图，LSD算法的核心思想是合并像素生成直线，合并的规则是根据每个像素点的梯度值建立状态列表，并将所有点设置为NOT USED。然后去除列表中梯度最大的点作为直线的第一个点，并将对应的状态设置为USED。再基于区域生长算法，得到line support region。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215203.png" alt="enter description here"></p><p>&emsp;&emsp;上面说了Hough由于全局的属性带来的优缺点，下面也说说LSD这种算法因为局部提取直线带来的缺点。<br>LSD号称是一种无需设置任何参数的算法，但在实际使用中，需要设置采样率，并且区域生长算法中，需要设置梯度角度变化的容忍（tolerance ）值。</p><ul><li>由于LSD算法的每个点都有状态值“NOT USED”和“USED”。因此每个点都智能属于一条直线，遇到相交直线时会出现至少一条直线被分割成多条直线的情况。</li><li>LSD算法在找寻line support region时，用了区域生长算法的思想。会由于线段间的遮挡和局部模糊导致一条直线被割裂成多条。</li></ul><p>&emsp;&emsp;由于是做手写分式分割中的分数线检测，直线往往弯曲而且易被其他线“切断”。因此做实验时，尝试将采样率与区域生长的容忍值调大了不少，让我困惑的是，在我将采样率和容忍值调大后，LSD算法的耗时呈大幅增长，失去了LSD算法的效率优势，这里暂时还不知道是我自己写算法的问题，还是LSD在增大局部搜索范围后效率会显著下降的原因。日后有了确切的结论会来更新，也欢迎大家指正。</p><p>&emsp;&emsp;下面贴几张LSD算法在分数检测的效果图，不同颜色的线段代表检测出的不同直线。可以看到对于扭曲小且没相交直线的情况下，LSD有较好的效果，而一旦干扰多了，一条直线会被分割成多条直线。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215201.png" alt="enter description here"></p><p>&emsp;&emsp;关于直线的论文有很多，还有使用图像分割或提取边缘算法得到图像边缘后，使用动态聚类算法聚合线段，再用直线拟合同一聚类中图像。</p><h2 id="实验与结论"><a href="#实验与结论" class="headerlink" title="实验与结论"></a>实验与结论</h2><p>&emsp;&emsp;不得不说，手写分式可能出现各种各样的情况。例如无法有效判断检测到的那条直线为分数线，对于带分式中整数与分子分母贴合紧密的情况，也暂时没有有效的解决思路。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215088.png" alt="enter description here"></p><p>&emsp;&emsp;最近看了很多图像处理的论文以及思路，很多棘手的任务，论文作者都巧妙的用图像处理的算法解决了。相信关于分式分割的处理，也会有相应的解决办法。事情总是要一步一个脚印的处理。先贴一个图，下图是这周做的真分式和假分式的切割算法，在525张手写分式图片中，正确分割了512张，也算是达到了预期的效果。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215385.png" alt="enter description here"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 毛星云. OpenCV3编程入门[M]. 电子工业出版社, 2015.</p><p>[2] Gioi R G V, Jakubowicz J, Morel J M, et al. LSD: A line segment detector[J]. Image Processing on Line, 2012, 2(4):35-55.</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="图像处理" scheme="https://oysz2016.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
    <category term="图像处理" scheme="https://oysz2016.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    <category term="算法" scheme="https://oysz2016.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>决策树汇总——ID3、C4.5、CART</title>
    <link href="https://oysz2016.github.io/post/5f5a1d32.html"/>
    <id>https://oysz2016.github.io/post/5f5a1d32.html</id>
    <published>2023-09-03T07:03:41.168Z</published>
    <updated>2023-01-15T07:48:24.536Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span><br>决策树是机器学习中的经典算法之一，十大机器学习算法中就包含决策树算法(Decision Tree). 顾名思义，决策树是基于树结构进行决策的，根据算法不同会使用多叉树或者二叉树。其实人类在日常生活中，遇到问题时也会使用决策树进行判断。例如西瓜书里有列举挑选西瓜的例子。</p><p>一颗决策树一般包含一个根结点，若干个内部节点和若干叶节点，根结点是判断的开始，内部节点是一些子判断流程，而叶节点对应于决策的结果。决策树作为机器学习算法的一种，其目的是产生一颗泛化能力强的树，能处理训练集中没有见到的情形，并能正确决策。</p><h2 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h2><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>信息熵是度量样本集合纯度最常用的一种指标，令D中第k类样本的比例为$p_k$, 则D的信息熵为<br>$Ent(D)=-\sum_{k=1}^{|y|} p_klog_2{p_k}$</p><p>信息增益=信息熵-条件熵<br>$Gain(D,a)=Ent(D)-\sum_{v=1}^{V} \frac{D^v}{D}Ent(D^v)$</p><p>以西瓜书中的列子<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%86%B3%E7%AD%96%E6%A0%91%E6%B1%87%E6%80%BB/2.png" alt=""></p><p>17个样本中正例有8个，负例有9个，则根结点的信息熵为:<br>$Ent(D)=-\sum_{k=1}^{2} p_klog_2{p_k}=-(\frac{8}{17}log_2 \frac{8}{17}+\frac{9}{17}log_2 \frac{9}{17})=0.998$</p><p>然后计算相应属性的条件熵，以色泽属性为例，可以分为 $D^1$(色泽=青绿)有6个样本，正反各3个，$D^2$(色泽=乌黑)有6个样本，正4，饭2，$D^3$(色泽=浅白)，有5个样本，正1反4.则3个分支节点的信息熵为：<br>$Ent(D^1)=-(\frac{3}{6}log_2 \frac{3}{6}+\frac{3}{6}log_2 \frac{3}{6})=1$<br>$Ent(D^2)=-(\frac{4}{6}log_2 \frac{4}{6}+\frac{2}{6}log_2 \frac{2}{6})=0.918$<br>$Ent(D^2)=-(\frac{1}{5}log_2 \frac{1}{5}+\frac{4}{5}log_2 \frac{4}{5})=0.722$</p><p>则色泽的信息增益为:</p><p>$Gain(D,a)=Ent(D)-\sum_{v=1}^{V} \frac{D^v}{D}Ent(D^v)=0.998-(\frac{6}{17}<em>1+\frac{6}{17}</em>0.918+\frac{5}{17}*0.722)=0.109$</p><p>也可以得到其他属性的信息增益，在根节点的位置信息增益最大的属性为纹理Gain=0.381.<br>确定第一个分支后，后面的内部节点和叶节点叶通过类似方式递归判断，则生成的决策树为:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%86%B3%E7%AD%96%E6%A0%91%E6%B1%87%E6%80%BB/3.png" alt=""></p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>ID3没有剪纸的策略，容易过拟合</li><li>没有考虑缺失值</li><li>只能处理离散分布的特征</li></ul><h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><p>C4.5在ID3的基础上做了一些改进:</p><ul><li>引入后剪枝缓解过拟合</li><li>可以处理属性中有连续值的情形，具体做法是对于某属性在区间$[a^i, a^{i+1})$，若取任意值产生的划分结果相同，则取该区间的中位点作为划分点</li><li>对于缺失值，C4.5的做法是用没有缺失的样本子集计算信息增益</li></ul><h3 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h3><p>剪枝的目的是防止决策树过拟合，在构建决策树时，为了能正确分类训练样本。决策树的结点划分过程会不断重复，可能导致分的过细，在训练集拟合的很好，但测试集的效果变得很差。所以需要剪枝帮助模型获得泛化性。<br>剪枝有<strong>预剪枝</strong>和<strong>后剪枝</strong>两种，引用下西瓜书里关于两种剪枝方式的差异</p><blockquote><p><strong>预剪枝</strong>是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;<strong>后剪枝</strong>则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点.</p></blockquote><p>判断上述两种剪枝方式是否能带来性能提升的方式是采用验证集，验证剪枝前后的效果。</p><p>预剪枝和后剪枝的例子见下面两图:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%86%B3%E7%AD%96%E6%A0%91%E6%B1%87%E6%80%BB/4.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%86%B3%E7%AD%96%E6%A0%91%E6%B1%87%E6%80%BB/5.png" alt=""></p><h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ul><li>C4.5使用的是多叉树，二叉树效率更高</li><li>只能用于分类</li><li>使用的熵模型有大量耗时的对数运算，连续值还有排序运算</li><li>在构造树的过程中，对数值属性值需要按照其大小排序，从中选择一个分割点。当训练集打到内存无法容纳时，会无法运行</li></ul><h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><h3 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h3><p>CART使用基尼指数作为划分属性的度量<br>$Gini(D)=\sum_{k=1}^{|y|}\sum_{k^{‘} \neq k} p_k p_{k^{‘}}=1- \sum_{k=1}^{|y|}p_k^2$</p><p>$Gini(D)$反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此$Gini(D)$越小，则数据集D的纯度越高</p><p>将其写成和信息增益等价的形式，则基尼指数为</p><p>$ Gini_index(D,a)=\sum_{v=1}^{V} \frac{D^v}{D} Gini(D^v) $</p><p>在是否要划分决策树的判断时，CART会选择使得划分后基尼指数最小的属性作为最优划分属性。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://oysz2016.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="决策树" scheme="https://oysz2016.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
</feed>
