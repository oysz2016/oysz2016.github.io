<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>冲弱&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/ab8296a41c8b88ea9f8a771cd548cb5e</icon>
  <subtitle>人生在勤，不索何获</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://oysz2016.github.io/"/>
  <updated>2022-11-09T13:13:13.828Z</updated>
  <id>https://oysz2016.github.io/</id>
  
  <author>
    <name>冲弱</name>
    <email>ouyang-sz@foxmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《VAE》论文笔记</title>
    <link href="https://oysz2016.github.io/post/63606c55.html"/>
    <id>https://oysz2016.github.io/post/63606c55.html</id>
    <published>2022-11-09T13:08:51.295Z</published>
    <updated>2022-11-09T13:13:13.828Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>论文链接: <a href="https://arxiv.org/pdf/1312.6114.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1312.6114.pdf</a></p><p>论文代码: <a href="https://github.com/devnag/pytorch-generative-adversarial-networks" target="_blank" rel="noopener">https://github.com/devnag/pytorch-generative-adversarial-networks</a></p><h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><h2 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h2><p>auto-encoder是一种无监督的算法，自编码器是一个输入和学习目标相同的神经网络，其结构分为编码器和解码器两部分。其思想是输入x经过encoder生成hidden layer特征z，再将z经过decoder重新预测生成$x^{‘}$<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VAE/v2-ec10f5dbd7197120cb87eaee5dc04e9a_b.png" alt=""></p><h2 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h2><p>VAE和GAN一样都属于生成模型，即从训练数据建模真实数据的分布，再反过来用学到的模型生成新的数据。这一类模型是假设有一个数据集$X=\{x^{(i)}\}_{i=1}^N$, 理想情况下是用$x_i$拟合函数p(x), 通过p(x)能得到$X$之外的数据，但这是理想情况，GAN和VAE采用了不同的方式达到生成模型的效果，之前GAN的论文笔记中有梳理过，GAN是一个对抗网络，通过判别器来判断生成器产生数据的效果。而VAE的方式在下面会详细介绍</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>VAE的思想是对于一个真实样本$x_k$, 假设存在后验分布$P(Z|x_k)$和$x_k$是一一对应的，$P(Z|x_k)$是一个正态分布图分布，能知道该正态分布的均值和方差，就能将其还原回X，在VAE中$P(Z|x_k)$的均值和方差是通过模型计算得到。<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VAE/WX20221010-203445@2x.png" alt=""><br>为了使模型具有生成能力，VAE希望$p(Z_x)$趋向于正态分布<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VAE/WX20221010-204114@2x.png" alt=""></p><p>在VAE中实际上也有两个encoder，分别用来计算均值和方差，在encoder计算的损失中会加入KL Loss，实际上是给encoder部分增加正则项，希望encoder的均值为0.VAE的损失函数会倾向于在训练初期，会降低噪声(KL loss增加)，使得拟合更容易，而在decoder训练的不错时，会增加噪声(KL loss减少)，使得拟合更困难</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>实际上VAE中也有对抗的过程，因为decoder的部分希望没有噪声，而KL loss希望有高斯噪声，两者在训练的时候实际上是对立的。VAE和GAN两种方式实际上各有优缺点，GAN在训练时不稳定，而VAE生成的图像相对GAN会模糊些</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;\assets\js\APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;p&gt;论文链接: &lt;a href=&quot;https://arxiv.org/pdf/1312.6114.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成网络" scheme="https://oysz2016.github.io/tags/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>《NER&amp;RE联合抽取》论文笔记</title>
    <link href="https://oysz2016.github.io/post/f45dd794.html"/>
    <id>https://oysz2016.github.io/post/f45dd794.html</id>
    <published>2022-10-29T01:45:49.641Z</published>
    <updated>2022-10-29T01:46:03.471Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h1 id="背景"><a href="#背景" class="headerlink" title="背景:"></a>背景:</h1><ul><li>NER任务: 预测文本中具有特定意义的实体，如人名，地名等</li><li>RE任务: 多分类任务，通过NER得到了实体之后，预测任意两个实体存在怎样的关系 </li></ul><p><strong>实体关系抽取可以分为两类方法:</strong></p><ul><li>pipeline models:可以任意组合不同的模型和数据，但关系模型使用实体模型的的预测结果作为其输入，会导致实体预测的错误传播到关系预测模型中</li><li>joint models:在一个模型中同时完成实体和关系抽取的任务,增强实体和关系的信息交互</li></ul><p><strong>关系抽取需要考虑的问题:</strong><br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/1.PNG" alt="enter description here"></p><ul><li>SEO: (Single Entity Overlap)，一个实体与多个其他实体有关系</li><li>EPO(Entity Pair Overlap):  两个实体之间有多个关系</li><li>SOO(Subject Object Overlap)/HTO)(Head Tail Overlap): subject和object存在嵌套的情形</li></ul><h1 id="CsRel-A-Novel-Cascade-Binary-Tagging-Framework-for-Relational-Triple-Extraction"><a href="#CsRel-A-Novel-Cascade-Binary-Tagging-Framework-for-Relational-Triple-Extraction" class="headerlink" title="CsRel:A Novel Cascade Binary Tagging Framework for Relational Triple Extraction"></a>CsRel:A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</h1><p><strong>论文:</strong> <a href="https://arxiv.org/pdf/1909.03227.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1909.03227.pdf</a><br><strong>代码:</strong> <a href="https://github.com/weizhepei/CasRel" target="_blank" rel="noopener">https://github.com/weizhepei/CasRel</a><br>CasRel:</p><ul><li>对关系三元组联合建模</li><li>通过下面的公式，可以将句子中含有(s,r,o)三元组的最大似然估计转换为先提取s后，在关系r的前提下，提取对应的o<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/2.PNG" alt="enter description here"></li></ul><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/3.PNG" alt="enter description here"></p><p>Cascade: 先抽取subject, 再抽取对应的关系和object。整个网络分成两步:</p><ul><li>先抽取subject，再抽取对应的关系和object</li><li>对于每一个关系，都要做对应关系的object抽取，如果有N个关系，则有2N个序列</li></ul><p><strong>subject tagger</strong></p><p>采用两个单独的二分类器分别检测subject实体的开始和结束，具体做法是经过bert后输出两个序列，start序列将实体的头token标记为1，end序列将实体的尾token标记为1。如果一个句子中检测出多个实体，则采用nearest的原则解码</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/4.PNG" alt="enter description here"></p><p><strong>relation-specific object taggers</strong><br>将subject的信息作为先验信息，带入到object和关系的抽取中。由于每个span的宽度不同，为了保证x和v的维度一致，需要将subject做max pooling</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/5.PNG" alt="enter description here"></p><h1 id="TPLinker-Single-stage-Joint-Extraction-of-Entities-and-Relations-Through-Token-Pair-Linking"><a href="#TPLinker-Single-stage-Joint-Extraction-of-Entities-and-Relations-Through-Token-Pair-Linking" class="headerlink" title="TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking"></a>TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</h1><p><strong>论文:</strong> <a href="https://arxiv.org/pdf/2010.13415.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2010.13415.pdf</a><br><strong>代码:</strong> <a href="https://github.com/131250208/TPlinker-joint-extraction" target="_blank" rel="noopener">https://github.com/131250208/TPlinker-joint-extraction</a></p><p>之前工作存在的问题</p><ul><li>曝光偏差(exposure bias): 在训练时，grouth truth token作为上下文，训练object和关系。而在预测时，用预测的subject作为输入，导致了训练和测试时的偏差</li><li>误差传播: 错误不可逆有的传播，如果subject未抽取到，则object和关系也将预测不出来</li></ul><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/6.PNG" alt="enter description here"></p><p>TPLinker通过三种类型的span矩阵抽取实体关系三元组，N是序列长度，R是关系的总数</p><ul><li>EH-to-ET: 表示实体的头尾关系，1个N*N的矩阵。如两个实体：New York City:M(New, City) =1; De Blasio:M(De, Blasio) =1，在上图中紫色标记。</li><li>SH-to-OH: 表示subject和object的头部token间的关系，是R个N*N矩阵；如三元组(New York City, mayor,De Blasio):M(New, De)=1，在上图中位红色标记。</li><li>ST-to-OT: 表示subject和object的尾部token间的关系，是R个N*N的矩阵；如三元组(New York City, mayor,De Blasio):M(City, Blasio)=1，在上图中为蓝色标记。</li></ul><p>共有2R+1个矩阵，为了防止稀疏计算，下三角矩阵不参与计算。实体标注不会产生下三角矩阵，但关系可能会存在，若关系矩阵存在于下三角，则将其转置到上三角，并由标记1转换为标记2</p><p><strong>解码过程</strong></p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/7.PNG" alt="enter description here"></p><ol><li>解码EH-to-ET可以得到句子中所有的实体，用实体头token idx作为key，实体作为value，存入字典D中；得到三个实体{New York,New York City,De Blasio};则D={New:(New York,New York City),De:(De Blasio)}</li><li>对每种关系r，解码SH-to-OH得到token对并在D中关联其token idx的实体value；以关系mayor为例，解码SH-to-OH得到(De,New)，关联到D可以知道subject实体为(De Blasio), object实体为(New York, New York City)</li><li>解码ST-to-OT得到E=(City,Blasio), 关联上面的到的subject和object集合可以确认subject为(De Blasio),object为New York City</li></ol><h1 id="PRGC-Potential-Relation-and-Global-Correspondence-Based-Joint-Relational-Triple-Extraction"><a href="#PRGC-Potential-Relation-and-Global-Correspondence-Based-Joint-Relational-Triple-Extraction" class="headerlink" title="PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction"></a>PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction</h1><p><strong>论文:</strong> <a href="https://arxiv.org/pdf/2106.09895.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2106.09895.pdf</a><br><strong>代码:</strong> <a href="https://github.com/hy-struggle/PRGC" target="_blank" rel="noopener">https://github.com/hy-struggle/PRGC</a><br><strong>CsRel缺点:</strong> </p><ul><li>每个subject需要判断大量冗余的关系</li><li>每次只能处理一个subject，工程效率不太友好</li></ul><p><strong>TPLinker缺点:</strong></p><ul><li>构建了大量的关系矩阵，导致标签稀疏和收敛速度慢</li><li>同样存在关系冗余</li></ul><p><strong>将实体和关系抽取建模成三个子任务:</strong></p><ul><li>关系判断(Relation Judgement): 输入为句子的特征向量h,  输出是长度为r的向量。判断句子中可能存在的关系。</li><li>实体提取(Entity Extraction): 输入是句子的特征向量h和可能存在的关系$R_{pot}$。 对于每个候选关系，采用softmax进行两次三分类，第一次确定头实体的BIO标签，第二次确定尾实体的BIO标签。在抽取subject和object时，用到了关系的特征。将关系向量加到对应token向量上，经过全连接层、softmax得到分类概率。</li></ul><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/8.png" alt="enter description here"></p><ul><li>主宾语对齐(Subject-object Alignment):输入是句子的特征向量。构建二维矩阵$M\in R^{n*n}<script type="math/tex">,</script>M_{i,j}$存储的是subject实体首词为第i个token，object实体首词为第j个token的概率。由于通过实体提取获取了句子中在每个关系$r_{i}$相应的实体，则只要subject和object实体的首词能匹配上，则对应的实体也能匹配上<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/9.png" alt="enter description here"></li></ul><h1 id="OneRel-Joint-Entity-and-Relation-Extraction-with-One-Module-in-One-Step"><a href="#OneRel-Joint-Entity-and-Relation-Extraction-with-One-Module-in-One-Step" class="headerlink" title="OneRel: Joint Entity and Relation Extraction with One Module in One Step"></a>OneRel: Joint Entity and Relation Extraction with One Module in One Step</h1><p><strong>论文:</strong> <a href="https://arxiv.org/pdf/2203.05412" target="_blank" rel="noopener">https://arxiv.org/pdf/2203.05412</a><br><strong>代码:</strong> <a href="https://github.com/ssnvxia/OneRel" target="_blank" rel="noopener">https://github.com/ssnvxia/OneRel</a><br>TPLinker存在的问题:</p><ul><li>在构建实体和关系时，引入了1+2R个矩阵，即一个矩阵用来抽取关系，2R个矩阵抽取subject和object实体对头之间的关系，R个矩阵抽取是梯队尾之间的关系。存在较多冗余的信息，而且忽略了三元组中关系实体、头实体和尾实体相互的关系<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/10.png" alt="enter description here"></li></ul><p>提出单模块，单步解码的实体关系联合抽取方法</p><ul><li>Multi-Module Multi-Step: 实体和关系分别建模，串行多步解码，会存在误差传递</li><li>Multi‐Module One‐Step: 实体和关系分别建模，单步解码，最后组装成三元组。存在冗余计算</li><li>One‐Module One‐Step: 用单个模块直接建模头实体，关系，尾实体<br>使用token-pair的方式，用4个标记类型建模三元组</li><li>HB-TB：头实体的开始token与尾实体的开始token 进行连接。</li><li>HB-TE：头实体的开始token与尾实体的结束token 进行连接。</li><li>HE-TE：头实体的结束token与尾实体的结束token 进行连接。</li><li>-：不存在连接关系。</li></ul><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/11.png" alt="enter description here"></p><p>相比TPLinker构建的矩阵数量由1+2R个，减少到R个<br>在(New York State, Contains, New York City)的三元组中, 关系<script type="math/tex">M_r=contains</script>的矩阵中。解码时通过通过HB-TE,HE-TE可以得到头实体New York State，HB-TB, HB-TE可以得到尾实体New York City</p><h1 id="UniRE-A-Unified-Label-Space-for-Entity-Relation-Extraction"><a href="#UniRE-A-Unified-Label-Space-for-Entity-Relation-Extraction" class="headerlink" title="UniRE: A Unified Label Space for Entity Relation Extraction"></a>UniRE: A Unified Label Space for Entity Relation Extraction</h1><p><strong>论文:</strong> <a href="https://arxiv.org/abs/2107.04292" target="_blank" rel="noopener">https://arxiv.org/abs/2107.04292</a><br><strong>代码:</strong> <a href="https://github.com/Receiling/UniRE" target="_blank" rel="noopener">https://github.com/Receiling/UniRE</a></p><ul><li>之前联合学习的方法仍然使用各自的标签空间，并没有将标签空间联合起来</li><li>将sequence labeling调整为关系抽取任务，将NER和RE两个任务放在同一个标签空间进行处理</li></ul><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/12.png" alt="enter description here"></p><ul><li>引入词对关系表，将实体和关系通过该表完整的表示出来（PER：人名实体，GPE：地理位置实体，PER-SOC: 社会关系，ORG-AFF：机构附属关系， PHYS：位置临近关系</li><li>正向关系：表的上三角部分。人名实体 David Perkins 对地理位置实体 California 存在位置临近关系 PHYS ，那 David 对California，Perkins 对 California 都具有 PHYS 关系；</li><li>逆向关系：表的下三角部分。人名实体 doctors 对地理位置实体 village 存在隶属关系 ORG-AFF ，那 doctors 对 village具有 ORG-AFF 关系；</li><li>无向关系：和表对角线对称的。两个人名实体 David Perkins 和 wife 之间存在社会关系 PER-SOC ，这被分解成两个对称关系，David Perkins 对 wife 的正向关系和 wife 对 David Perkins 的逆向关系。实体也可以看做无向的关系，例如，David Perkins 是一个人名实体，那 David 对 David ，David 对Perkins，Perkins 对 David ，Perkins 对 Perkins 都具有标签为 PER 的关系。</li></ul><p>模型的训练转换为预测任意两个单词之间的关系，采用双仿射变换</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/13.png" alt="enter description here"></p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/14.png" alt="enter description here"></p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/15.png" alt="enter description here"></p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/16.png" alt="enter description here"></p><p>损失函数为交叉墒损失</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/17.png" alt="enter description here"></p><p>对该建模的一些约束</p><ul><li><p>对称性: 对称关系具有如下性质，<script type="math/tex">(e_1,e_2,l)</script>和<script type="math/tex">(e_2,e_1,l)</script>是等价的。因此满足该条件的两个关系关于对象线是对称的。实体和无向关系关于对角线是对称的。因此，这些标签对应的概率分数应该关于对角线对称。将标签分为y分为对称标签<script type="math/tex">y_{sym}</script>(实体和无向关系)和非对称标签<script type="math/tex">y_{asym}</script>。对于对称标签，具有如下约束:<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/18.png" alt="enter description here"></p></li><li><p>蕴含性: 给定一个关系，则参与关系的必定是两个实体。相反，给定两个实体，他们两个之间不一定存在关系。则关系的概率分布应该不大于参与该关系的两个实体的概率分布</p></li></ul><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/19.png" alt="enter description here"></p><p>Max为hinge loss。最终的loss</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/20.png" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;\assets\js\APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景:&quot;&gt;&lt;/a&gt;背景:&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;NER任务: 预测文本中
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="NLP" scheme="https://oysz2016.github.io/tags/NLP/"/>
    
      <category term="NER" scheme="https://oysz2016.github.io/tags/NER/"/>
    
      <category term="RE" scheme="https://oysz2016.github.io/tags/RE/"/>
    
  </entry>
  
  <entry>
    <title>长尾优化汇总</title>
    <link href="https://oysz2016.github.io/post/8592ee55.html"/>
    <id>https://oysz2016.github.io/post/8592ee55.html</id>
    <published>2022-10-24T01:14:11.516Z</published>
    <updated>2022-10-24T01:14:11.517Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="经典的深度学习数据集"><a href="#经典的深度学习数据集" class="headerlink" title="经典的深度学习数据集"></a>经典的深度学习数据集</h2><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295054598.png" alt="enter description here"><br>Mnist: 数据规模较小，10个类别<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295071148.png" alt="enter description here"><br>ImageNet: 百万数据量，1000个类别<br>两者的共同点：类别是均匀分布的</p><h2 id="真实场景中的深度学习任务"><a href="#真实场景中的深度学习任务" class="headerlink" title="真实场景中的深度学习任务"></a>真实场景中的深度学习任务</h2><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295017948.png" alt="enter description here"></p><ul><li>类别不平衡是常态<h2 id="长尾问题"><a href="#长尾问题" class="headerlink" title="长尾问题"></a>长尾问题</h2><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295083966.png" alt="enter description here"><br>长尾问题常见的表现及原因 ：</li><li>头部类的效果好，尾部类的效果差</li><li>模型是数据驱动的，头部类的数据多，尾部类的数据少</li><li>尾部类数量少的同时可能造成类别中样本差异较大，网络学习不充分<h1 id="Resampling"><a href="#Resampling" class="headerlink" title="Resampling"></a>Resampling</h1><h2 id="BBN-Bilateral-Branch-Network-with-Cumulative-Learning-for-Long-Tailed-Visual-Recognition"><a href="#BBN-Bilateral-Branch-Network-with-Cumulative-Learning-for-Long-Tailed-Visual-Recognition" class="headerlink" title="BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition"></a>BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition</h2>论文链接：<a href="https://arxiv.org/pdf/1912.02413.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1912.02413.pdf</a><br>two stage finetuning:</li></ul><ul><li>第一阶段在原始不平衡的数据集上训练</li><li>第二阶段以一个很小的学习率使用resampling/reweighting的方法fintune<br>根据two stage fintuning方法比只使用resampling/reweighting好的原因做了假设</li><li>reblance的方法有效的原因在于提升了分类器的性能</li><li>会损害网络学习到的特征<br>为了验证上述假设，将网络拆解成了分类器和特征提取器两部分，分别实验:</li><li>在第一阶段使用交叉熵和resampling/reweighting训练整个网络</li><li>将第一阶段特征提取器的参数固定，使用交叉熵和resampling/reweighting训练分类器<br>论文链接：<a href="https://arxiv.org/pdf/1912.02413.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1912.02413.pdf</a><br>two stage finetuning:</li><li>第一阶段在原始不平衡的数据集上训练</li><li>第二阶段以一个很小的学习率使用resampling/reweighting的方法fintune<br>根据two stage fintuning方法比只使用resampling/reweighting好的原因做了假设</li><li>reblance的方法有效的原因在于提升了分类器的性能</li><li>会损害网络学习到的特征<br>为了验证上述假设，将网络拆解成了分类器和特征提取器两部分，分别实验:</li><li>在第一阶段使用交叉熵和resampling/reweighting训练整个网络</li><li>将第一阶段特征提取器的参数固定，使用交叉熵和resampling/reweighting训练分类器</li></ul><p>设计了一个双分支的网络论文链接：<a href="https://arxiv.org/pdf/1912.02413.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1912.02413.pdf</a><br>two stage finetuning:</p><ul><li>第一阶段在原始不平衡的数据集上训练</li><li>第二阶段以一个很小的学习率使用resampling/reweighting的方法fintune<br>根据two stage fintuning方法比只使用resampling/reweighting好的原因做了假设</li><li>reblance的方法有效的原因在于提升了分类器的性能</li><li>会损害网络学习到的特征<br>为了验证上述假设，将网络拆解成了分类器和特征提取器两部分，分别实验:</li><li>在第一阶段使用交叉熵和resampling/reweighting训练整个网络</li><li><p>将第一阶段特征提取器的参数固定，使用交叉熵和resampling/reweighting训练分类器<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295186969.png" alt="enter description here"><br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295194164.png" alt="enter description here"><br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295201430.png" alt="enter description here"></p></li><li><p>第一个采样器是一个公平的采样器</p></li><li>第二个采样器是一个resample的采样器</li><li>两个分支共享权重，减少参数量的同时，让第二个分支受益于第一个分支中更好的特征</li><li>使用一个adaptor的策略，调节两个分支在网络训练中的权重<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295216148.png" alt="enter description here"><br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295235292.png" alt="enter description here"><h1 id="Cost-sensitive-learning"><a href="#Cost-sensitive-learning" class="headerlink" title="Cost-sensitive learning"></a>Cost-sensitive learning</h1><h2 id="Class-Balanced-Loss-Based-on-Effective-Number-of-Samples"><a href="#Class-Balanced-Loss-Based-on-Effective-Number-of-Samples" class="headerlink" title="Class-Balanced Loss Based on Effective Number of Samples"></a>Class-Balanced Loss Based on Effective Number of Samples</h2>链接:<a href="https://arxiv.org/pdf/1901.05555.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.05555.pdf</a><br>之前方法存在的问题：在reweighting等方法中，一般将样本数量的倒数作为该类别的权重，但是样本之间能提供的信息可能存在重合，简单通过样本数量判断权重会存在问题</li><li>提出了一种计算有效样本的方法</li><li>用有效样本数代替原始的样本频率，再用其倒数对损失进行加权<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295283511.png" alt="enter description here"><br>有效样本的定义:</li><li>样本的有效数据量是样本的期望体积，一个新的采样数据只能存在两种情况<ul><li>新样本存在于之前样本中的概率为p</li><li>新样本不存在于之前样本中的概率为1-p<br>提出有效样本的计算公式:<script type="math/tex; mode=display">E_{n}=\frac{1-\beta^{n}}{1-\beta}</script>当n=1时，<script type="math/tex">E_1=\frac{1-\beta^1}{1-\beta}=1</script><br>假设当n=k-1时成立,即<script type="math/tex">E_{k-1}=\frac{1-\beta^{k-1}}{1-\beta}</script>，<br>设样本的体积为K，已经采样的样本体积为<script type="math/tex">E_{k-1}</script>。则<script type="math/tex">p=\frac{E_{k-1}}{K}</script>,经过k次采样后，第k次采样时有以下情况:</li></ul></li><li>第k次采样和之前样本存在重叠的情况,则样本体积为<script type="math/tex">E_{k-1}</script></li><li>第k次采样是新的有效样本，与之前不存在重叠的情况,则样本体积为<script type="math/tex">E_{k-1}+1</script><br>期望体积为:<script type="math/tex; mode=display">E_k=pE_{k-1}+(1-p)(E_{k-1}+1)=1+\frac{K-1}{K}E_{k-1}</script>其中<script type="math/tex; mode=display">\beta=\frac{K-1}{K}$$, 则$$E_k=1+\beta E_{k-1}=1+\beta \frac{1-\beta^{k-1}}{1-\beta}=\frac{1-\beta^{k}}{1-\beta}</script>则有效数据量是数据总量n的指数函数，超参数<script type="math/tex">\beta\in[0,1)</script><br>有效数据量<script type="math/tex">E_n</script>具有如下性质:</li><li>当n很大时，有效数据量等于n</li><li>当n为1时，有效数据量为1<br>CB-Loss:<script type="math/tex; mode=display">CB(p,y)=\frac{1}{E_{n_y}}L(p,y)=\frac{1-\beta}{1-\beta_{n_y}}L(p,y)</script></li></ul><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295322950.png" alt="enter description here"></p><h1 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h1><h2 id="Feature-Space-Augmentation-for-Long-Tailed-Data"><a href="#Feature-Space-Augmentation-for-Long-Tailed-Data" class="headerlink" title="Feature Space Augmentation for Long-Tailed Data"></a>Feature Space Augmentation for Long-Tailed Data</h2><p>论文链接：<a href="https://arxiv.org/abs/2008.03673" target="_blank" rel="noopener">https://arxiv.org/abs/2008.03673</a><br>常见的解决方法，如data manipulation和Balanced loss function design在提升长尾数据集模型性能的同时会损害特征表示能力<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295376341.png" alt="enter description here"></p><p>两个假设:</p><ul><li>头部类中类别无关的特征可以让尾部类特征更加丰富</li><li>高级特征空间具有更“线性”的表示，可以提取类通用和类特定的特征，并重新混合生成新的样本<br>方法:<br>CAM(Class Activation Map)<script type="math/tex; mode=display">M_c(x,y)=\sum_{k}w_k^cf_k(x,y)</script>c: class; x,y: pixel position; k: channel; w: weight; f: feature，将M归一化到[0,1]，设定两个阈值。<script type="math/tex">0<\tau_s,\tau_t<1</script>。则类特定特征和类通用特征分别为:</li><li><script type="math/tex; mode=display">M_c^s=sgn(M_c-\tau_s) \bigodot M_c</script></li><li><script type="math/tex; mode=display">M_c^g=sgn(\tau_g-M_c) \bigodot M_c</script><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295358501.png" alt="enter description here"><br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295382285.png" alt="enter description here"></li><li>正常训练得到特征提取网络和分类器，使用注意力机制CAM图做特征分解，将特征分为类别无关的特征和类别特定的特征。</li><li>两阶段训练，第一阶段正常训练，负责提取特征和cam，第二阶段是做尾部类的增广训练，分为两步<ul><li>网络输入一张头部类图片和一张尾部类图片，通过分类置信度排序，选择和当前尾部类距离最近的头部类特征，融合头部类中类通用特征和尾部类中类特征特征<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295403120.png" alt="enter description here"></li><li>使用增强的特征图微调FC分类器层<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295415204.png" alt="enter description here"></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;\assets\js\APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;h2 id=&quot;经典的深度学习数据集&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="长尾优化" scheme="https://oysz2016.github.io/tags/%E9%95%BF%E5%B0%BE%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>《Unbiased Scene Graph Generation from Biased Training》论文笔记</title>
    <link href="https://oysz2016.github.io/post/b4822109.html"/>
    <id>https://oysz2016.github.io/post/b4822109.html</id>
    <published>2022-10-15T10:01:52.844Z</published>
    <updated>2022-10-15T10:01:57.863Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h1 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h1><p>场景图生成: 描述目标检测的物体之间，具有怎样的关系</p><h1 id="之前算法存在的问题"><a href="#之前算法存在的问题" class="headerlink" title="之前算法存在的问题"></a>之前算法存在的问题</h1><ul><li>数据集中关系词存在严重的偏见，原因有以下几点:<ol><li>标注时,倾向于简单的关系</li><li>日常生活中确实有些事物的关联性比较多</li><li>语法习惯的问题</li></ol></li></ul><ul><li>往往通过union box和两个物体的类别就预测了两个物体的关系，几乎没有使用visual feature，也就预测不出有意义的关系</li></ul><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><ol><li>为了让模型预测更有意义的关系，用了一个causal inference中的概念，即Total Direct Effect（TDE）来取代单纯的网络log-likelihood。在预测关系时更关注visual feature.</li></ol><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-144828@2x.png" alt="enter description here"></p><ol><li>提出了新的评测方法mR@K:把所有谓语类别的Recall单独计算，然后求均值，这样所有类别就一样重要了</li></ol><h2 id="Biased-Training-Models-in-Causal-Graph"><a href="#Biased-Training-Models-in-Causal-Graph" class="headerlink" title="Biased Training Models in Causal Graph"></a>Biased Training Models in Causal Graph</h2><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143055@2x.png" alt="有偏差的训练框架"></p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-142859%402x.png" alt="训练简化图"></p><p>节点I: Input Image&amp;Backbone.Backbone部分使用Faster rcnn预训练好的模型，并frozen bockbone的参数。输出检测目标的bounding boxes和图像的特征图.<br>Link I-&gt;X:目标的特征，通过ROI Align提取目标对应的特征<script type="math/tex">R={r_i}</script>，获取目标粗略的分类结果<script type="math/tex">L={l_i}</script>.和MOTIFS和VCTree一样，使用以下方式，编码视觉上下文特征.</p><blockquote><p>MOTIFS中使用双向LSTM，VCTree中使用双向TreeLSTM，早期工作如VTransE中使用全连接层</p></blockquote><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-145409@2x.png" alt="enter description here"></p><p>节点X：目标特征。获取一组目标的特征<script type="math/tex">x_e=(x_i,x_j)</script><br>Link X-Z: 获取对应目标fine-tuned的类别，从<script type="math/tex">x_i</script>解码:</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-145415@2x.png" alt="enter description here"></p><p>节点Z:目标类别，one-hot的向量<br>Link X-&gt;Y: SGG的目标特征输入<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-145423@2x.png" alt="enter description here"></p><p>Link:Z-&gt;Y：SGG的目标类别输入<br>Link:I-&gt;Y:SGG的视觉特征输入<br>节点Y:输出关系词汇<br>Training loss：使用交叉熵损失预测label，为了避免预测Y只使用单一输入的信息，尤其是只使用Z的信息，进一步 使用auxiliary cross-entropy losses, 让每一个分支分别预测y</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/1280X1280.png" alt="enter description here"></p><h2 id="Unbiased-Prediction-by-Causal-Effects"><a href="#Unbiased-Prediction-by-Causal-Effects" class="headerlink" title="Unbiased Prediction by Causal Effects"></a>Unbiased Prediction by Causal Effects</h2><p>机器学习中常见的解决长尾问题的方法:</p><ul><li>数据增强/重新采样</li><li>对数据平衡改进的loss</li><li>从无偏见中分离出有偏见的部分<br>与上述方法的区别是不需要额外训练或层来建模偏差，通过构建两种因果图将原有模型和偏差分离开。<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143048@2x.png" alt="enter description here"></li></ul><h3 id="Origin-amp-Intervention-amp-Counterfactual"><a href="#Origin-amp-Intervention-amp-Counterfactual" class="headerlink" title="Origin&amp;Intervention&amp;Counterfactual"></a>Origin&amp;Intervention&amp;Counterfactual</h3><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143107@2x.png" alt="enter description here"></p><ul><li>ntervention:清除因果图中某个节点的输入，并将其置为某个值，公式为:$do(X= \tilde x)$.某节点被干预后，需要该节点输入的其他节点也会受影响</li><li>Counterfactual:让某个节点被干预后，其他需要输入的节点还假设该节点未被干预<br>总结: Counterfactual图实际上抹除了因果图像中object feature。只用image+object label预测两个目标间的关系。</li></ul><h3 id="Total-Direct-Effect-TDE"><a href="#Total-Direct-Effect-TDE" class="headerlink" title="Total Direct Effect (TDE)"></a>Total Direct Effect (TDE)</h3><p>根据两个因果图:</p><ul><li>原始因果图</li><li>Counterfactual因果图(可以认为是偏见)<br>消除偏见:<script type="math/tex; mode=display">TDE=Y_x(u)-Y_{\tilde x,z}(u)</script></li></ul><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143120@2x.png" alt="enter description here"></p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143154@2x.png" alt="enter description here"></p><h1 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h1><p>对比了几种常用的优化长尾问题的方法</p><ul><li>Focal</li><li>Reweight</li><li>Resample</li><li>X2Y: 直接通过X的输出预测Y</li><li>X2Y-Tr：切断其他分支的联系，只使用X预测Y</li><li>TE:<script type="math/tex">TE=Y_x(u)-Y_{\tilde x}(u)</script></li><li>NIE:<script type="math/tex">NIE=TDE-TE</script></li><li>TDE</li></ul><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143120@2x.png" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;\assets\js\APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;h1 id=&quot;任务定义&quot;&gt;&lt;a href=&quot;#任务定义&quot; class=&quot;headerlink&quot; title=&quot;任务定义&quot;&gt;&lt;/a&gt;任务定义&lt;/h1&gt;&lt;p&gt;场景图生成: 描述目标检
      
    
    </summary>
    
      <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="场景图生成" scheme="https://oysz2016.github.io/tags/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>《GAN》论文笔记</title>
    <link href="https://oysz2016.github.io/post/f84aad11.html"/>
    <id>https://oysz2016.github.io/post/f84aad11.html</id>
    <published>2022-10-08T10:00:09.832Z</published>
    <updated>2022-10-08T10:00:14.494Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>论文链接: <a href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf" target="_blank" rel="noopener">https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf</a></p><p>论文代码: <a href="https://github.com/devnag/pytorch-generative-adversarial-networks" target="_blank" rel="noopener">https://github.com/devnag/pytorch-generative-adversarial-networks</a></p><p>顾名思义, GAN是一个对抗网络。具体来说，会有一个生成器(Generator)，和一个判别器(Discirminator)，两个模型互相对抗，共同进步。举一个生活中的例子，例如造假币的人是这里的生成器，查假币的人是判别器，那么生成器的任务就是让造出的假币让判别器以为是真钱一样，而判断器的任务就是能很好的分别出生成器造出的钱是假币。</p><h1 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h1><p>输入一个n维的noise的向量z，一般是随机产生的，例如满足高斯分布/均值分布的噪声，生成器的任务是将向量z生成图片x，可以通过MLP等神经网络</p><h1 id="判别器"><a href="#判别器" class="headerlink" title="判别器"></a>判别器</h1><p>输入是图片数据，输出是一个标量，用于判断输入数据是来自真实数据还是生成器</p><h1 id="训练方式"><a href="#训练方式" class="headerlink" title="训练方式"></a>训练方式</h1><p>通过上面网络的简介知道，需要训练两个网络，所以训练方式目的也是让两个网络都能够有效的学习</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GAN/WX20221006-161252@2x.png" alt="enter description here"></p><ul><li>上式中D是判别器，G是生成器</li><li>先看前一项，$E_{x \sim p_{data}}[logD(x)]$表示从真实数据$p_{data}$中采样样本x，让判别器判断x的来源。在辨别器判断正确的情况下$D_x=1$, 则$logD(x)=0$</li><li>后面一项，$E_{z \sim p_{z}(z)}[log(1-D(G(z)))]$表示从随机噪声数据$p_{z}(z)$中生成噪声z, 先让生成器z生成类似x的数据$G(z)$,再让判别器判断生成的数据来源，在判别器完美的时候，能分出$G(z)$不是真实样本，则$D(G(z))=0$</li><li>在D犯错的时候，$log(D(x)$为负数，因此需要最大化D的损失, 而$G(z)$需要生成的和x尽可能相似，需要最小化G的损失</li><li>整个模型设计用的是博弈论的思想，min和max都需要优化，且互相对抗。感兴趣可以看看minmax算法相关的知识:<a href="https://en.wikipedia.org/wiki/Minimax" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Minimax</a>。在博弈论中，在包含两个或两个以上参与者的非合作博弈中，如果每个参与者都选择了对自己最有利的策略，则最后会达到均衡点，称为纳什均衡(<a href="https://en.wikipedia.org/wiki/Nash_equilibrium" target="_blank" rel="noopener">Nash equilibrium</a>)</li></ul><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GAN//WX20221006-161026@2x.png" alt="enter description here"></p><p>网络的优化过程在图1中有形象化的示例，图1中，蓝色的线是判别器，黑色是真实数据，绿色是生成器</p><ul><li>在图1(a)中，生成器生成的数据和真实数据相差比较大，但判别器判断的也不是太好</li><li>图1(b)更新了判别器，能很好的区分是否是生成器生成的数据</li><li>图1(c)中更新了生成器，生成的数据和真实数据变得很相似</li><li>图1(d)中随着生成器和辨别器都进行优化，最终生成器拟合的数据和真实数据基本一致，而判别器也无法判断数据来源，则模型优化完毕</li></ul><p>代码逻辑如下所示:<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GAN/WX20221006-161122@2x.png" alt="enter description here"></p><ul><li>第一个for循环是迭代的次数</li><li>然后会将判别器循环更新k次，再将生成器更新1次</li></ul><p>其中k是一个超参数，如果判别器更新的太好，生成器就没法玩了，而要是更新的太差，生成器就没有动力继续优化</p><p>算法的原理到这里基本上就整理完了，论文中第四部分章节有关于损失函数正确的理论证明，感兴趣也可以阅读下</p><p>总结</p><h2 id="gan算法实际上训练完后主要使用的是生成器，在方法中其实对生成器已经有了优化，所以为什么还要引入判别器呢？其实光从生成器的优化上，很难通过损失判断生成器的好坏，而引入判别器可以更全局的判断生成器的效果，也通过博弈的方法让生成器能达到更好的效果"><a href="#gan算法实际上训练完后主要使用的是生成器，在方法中其实对生成器已经有了优化，所以为什么还要引入判别器呢？其实光从生成器的优化上，很难通过损失判断生成器的好坏，而引入判别器可以更全局的判断生成器的效果，也通过博弈的方法让生成器能达到更好的效果" class="headerlink" title="gan算法实际上训练完后主要使用的是生成器，在方法中其实对生成器已经有了优化，所以为什么还要引入判别器呢？其实光从生成器的优化上，很难通过损失判断生成器的好坏，而引入判别器可以更全局的判断生成器的效果，也通过博弈的方法让生成器能达到更好的效果"></a>gan算法实际上训练完后主要使用的是生成器，在方法中其实对生成器已经有了优化，所以为什么还要引入判别器呢？其实光从生成器的优化上，很难通过损失判断生成器的好坏，而引入判别器可以更全局的判断生成器的效果，也通过博弈的方法让生成器能达到更好的效果</h2><p>title: 《GAN》论文笔记<br>author: 冲弱<br>email: ouyang-sz@foxmail.com<br>timezone: Asia/Shanghai<br>categories: 论文笔记<br>tags:</p><ul><li>深度学习</li><li>生成网络<br>copyright: true<br>mathjax: true<br>photos:</li><li><a href="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GAN/WX20221006-161541@2x.png" target="_blank" rel="noopener">https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GAN/WX20221006-161541@2x.png</a><br>abbrlink: f84aad11</li></ul><hr><p>论文链接: <a href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf" target="_blank" rel="noopener">https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf</a></p><p>论文代码: <a href="https://github.com/devnag/pytorch-generative-adversarial-networks" target="_blank" rel="noopener">https://github.com/devnag/pytorch-generative-adversarial-networks</a></p><p>顾名思义, GAN是一个对抗网络。具体来说，会有一个生成器(Generator)，和一个判别器(Discirminator)，两个模型互相对抗，共同进步。举一个生活中的例子，例如造假币的人是这里的生成器，查假币的人是判别器，那么生成器的任务就是让造出的假币让判别器以为是真钱一样，而判断器的任务就是能很好的分别出生成器造出的钱是假币。</p><h1 id="生成器-1"><a href="#生成器-1" class="headerlink" title="生成器"></a>生成器</h1><p>输入一个n维的noise的向量z，一般是随机产生的，例如满足高斯分布/均值分布的噪声，生成器的任务是将向量z生成图片x，可以通过MLP等神经网络</p><h1 id="判别器-1"><a href="#判别器-1" class="headerlink" title="判别器"></a>判别器</h1><p>输入是图片数据，输出是一个标量，用于判断输入数据是来自真实数据还是生成器</p><h1 id="训练方式-1"><a href="#训练方式-1" class="headerlink" title="训练方式"></a>训练方式</h1><p>通过上面网络的简介知道，需要训练两个网络，所以训练方式目的也是让两个网络都能够有效的学习</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GAN/WX20221006-161252@2x.png" alt="enter description here"></p><ul><li>上式中D是判别器，G是生成器</li><li>先看前一项，$E_{x \sim p_{data}}[logD(x)]$表示从真实数据$p_{data}$中采样样本x，让判别器判断x的来源。在辨别器判断正确的情况下$D_x=1$, 则$logD(x)=0$</li><li>后面一项，$E_{z \sim p_{z}(z)}[log(1-D(G(z)))]$表示从随机噪声数据$p_{z}(z)$中生成噪声z, 先让生成器z生成类似x的数据$G(z)$,再让判别器判断生成的数据来源，在判别器完美的时候，能分出$G(z)$不是真实样本，则$D(G(z))=0$</li><li>在D犯错的时候，$log(D(x)$为负数，因此需要最大化D的损失, 而$G(z)$需要生成的和x尽可能相似，需要最小化G的损失</li><li>整个模型设计用的是博弈论的思想，min和max都需要优化，且互相对抗。感兴趣可以看看minmax算法相关的知识:<a href="https://en.wikipedia.org/wiki/Minimax" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Minimax</a>。在博弈论中，在包含两个或两个以上参与者的非合作博弈中，如果每个参与者都选择了对自己最有利的策略，则最后会达到均衡点，称为纳什均衡(<a href="https://en.wikipedia.org/wiki/Nash_equilibrium" target="_blank" rel="noopener">Nash equilibrium</a>)</li></ul><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GAN//WX20221006-161026@2x.png" alt="enter description here"></p><p>网络的优化过程在图1中有形象化的示例，图1中，蓝色的线是判别器，黑色是真实数据，绿色是生成器</p><ul><li>在图1(a)中，生成器生成的数据和真实数据相差比较大，但判别器判断的也不是太好</li><li>图1(b)更新了判别器，能很好的区分是否是生成器生成的数据</li><li>图1(c)中更新了生成器，生成的数据和真实数据变得很相似</li><li>图1(d)中随着生成器和辨别器都进行优化，最终生成器拟合的数据和真实数据基本一致，而判别器也无法判断数据来源，则模型优化完毕</li></ul><p>代码逻辑如下所示:<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GAN/WX20221006-161122@2x.png" alt="enter description here"></p><ul><li>第一个for循环是迭代的次数</li><li>然后会将判别器循环更新k次，再将生成器更新1次</li></ul><p>其中k是一个超参数，如果判别器更新的太好，生成器就没法玩了，而要是更新的太差，生成器就没有动力继续优化</p><p>算法的原理到这里基本上就整理完了，论文中第四部分章节有关于损失函数正确的理论证明，感兴趣也可以阅读下</p><p>总结<br>gan算法实际上训练完后主要使用的是生成器，在方法中其实对生成器已经有了优化，所以为什么还要引入判别器呢？其实光从生成器的优化上，很难通过损失判断生成器的好坏，而引入判别器可以更全局的判断生成器的效果，也通过博弈的方法让生成器能达到更好的效果</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;\assets\js\APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;p&gt;论文链接: &lt;a href=&quot;https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成网络" scheme="https://oysz2016.github.io/tags/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>《ReferTransformer》论文笔记</title>
    <link href="https://oysz2016.github.io/post/149f20a.html"/>
    <id>https://oysz2016.github.io/post/149f20a.html</id>
    <published>2022-10-01T10:01:02.652Z</published>
    <updated>2022-10-01T10:01:07.363Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>由于之前看图片相关的论文比较多，而这篇论文偏向于视频理解的任务，为了避免有其他不了解这篇论文领域的同学看的时候比较吃力，在解读这篇论文前先介绍这篇论文的任务。<br><strong>任务定义</strong>: R-VOS(Referring video object segmentation): 给出一种物体对应的语言描述，分割出该物体对应的mask。<br>这篇论文的相关视频在reddit上热度非常高，感兴趣的同学可以看下视频，相信也能对这篇论文应用的方向有更清晰的了解。<a href="https://www.reddit.com/r/MachineLearning/comments/t7qe6b/r_endtoend_referring_video_object_segmentation/?utm_source=share&amp;utm_medium=web2x&amp;context=3" target="_blank" rel="noopener">[R] End-to-End Referring Video Object Segmentation with Multimodal Transformers</a></p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作:"></a>相关工作:</h2><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233717@2x.png" alt="enter description here"></p><h3 id="Bottom-up"><a href="#Bottom-up" class="headerlink" title="Bottom-up"></a>Bottom-up</h3><ul><li><strong>方法:</strong> 如图1(a)所示，用early-feature的方式融合视觉和语言特征，再用FCN decoder目标的掩码。</li><li><strong>缺点:</strong> early-feature的方法无法获得较好的多模态特征，无法为跨模态推理提供明确的知识，并且会遇到由于场景变化而导致预测对象的差异。</li></ul><h3 id="Top-down"><a href="#Top-down" class="headerlink" title="Top-down"></a>Top-down</h3><ul><li><strong>方法:</strong> 如图1(b)所示，两阶段的方式，先对图片/视频中所有的物体做实例分割，将实例分割的结果在视频中关联起来，形成一系列候选，再通过语言模型和Grounding Model筛选出语言描述提到的物体</li><li><strong>缺点:</strong> 相比bottom-up的方式有更好的性能，但整个pipeline由于多阶段的方式太重。例如最近的一些方法，如HTC,CFBI中都需要再ImageNet，COCO，RefCOCO中预训练，然后在R-VOS数据集中fintune。而且将R-VOS的任务拆解为几个子问题分别优化由于误差传递等问题会造成次优的解决方案。</li></ul><h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><p>该论文的方法图1(c):</p><ul><li>将文字的特征和queries的特征融合，作为conditional queries，生成的queries可以只聚焦在文字所提到的目标特征上，可以极大的减少queries的数量(例如detr中的100个)</li><li>考虑到需要从queries的特征中decode出object mask，使用instance-aware dynamic kernels从提取分割的mask特征</li><li>受启发于FPN，设计了CM-FPN(cross modal features pyramid network), 提取多模态的金字塔特征</li></ul><p>网络结构如图2所示，主要由backbone,language as queries,Cross-modal Feature Pyramid Network,Instance Sequence Matching and Loss，inference五部分组成。下面我写的尽量详细点。<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233751@2x.png" alt="enter description here"></p><h3 id="backbone"><a href="#backbone" class="headerlink" title="backbone"></a>backbone</h3><p><strong>visual encoder</strong><br>使用通用的视觉backbone做视觉特征的编码器，例如可以使用经典的ResNet网络或者3D的特征编码器，如Video swin transformer。生成的特征是如下Sequence $F_v=\{F_t\}^T_{t=1}$,其中T表示T桢图像<br><strong>linguistic encoder</strong><br>使用现成的语言模型,如RoBERTa，提取文本的特征$F_e=\{F_i\}^L_{i=1}$，其中L表示L个word。在该任务中需要将$F_e$通过pooling提取句子级别的特征，这是由于该任务使用句子级别的特征和queries融合用于跨模态的特征融合</p><h3 id="language-as-queries"><a href="#language-as-queries" class="headerlink" title="language as queries:"></a>language as queries:</h3><p><strong>transformer encoder</strong>: encoder部分和detr中一样，先用1x1卷积进行<strong>通道数压缩</strong>,再把宽和高压缩为一个维度，将<strong>特征序列化</strong>，再做<strong>位置编码</strong><br><strong>transformer decoder</strong>: decoder部分也和detr比较类似，但有些差异。使用N个object queries提取每帧实例中的特征，区别在于queries在视频帧之间共享权重，好处是可以更灵活的处理长视频，并且对于同样的实例queries学习的特征更鲁棒。将句子级别的特征$F_e$复制N份和每一个object queries一起作为decoder的输入。对于T帧图像，会得到$N_q=T<em>N$的预测集合<br><strong>prediction heads</strong>：具有三个预测头,分别是box head，mask head，class head(二分类，输出是否是文本中提到的目标)<br><strong>dynamic convolution</strong>： 使用动态卷积生成binary segmentation masks<br><em>*illustration of conditional queries</em></em>：得益于transformer的注意力机制，输入object queries和text embedding可以让object queries聚焦于text提到的物体</p><h3 id="cross-model-Feature-Pyramid-Network"><a href="#cross-model-Feature-Pyramid-Network" class="headerlink" title="cross-model Feature Pyramid Network"></a>cross-model Feature Pyramid Network</h3><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233822@2x.png" alt="enter description here"></p><p>该部分可以认为是这篇论文最大的创新点，提出了多模态融合的FPN网络，网络结构图如图4所示，和传统FPN网络的区别在于加入了vision-language fusion模块，该模块的结构类似transformer网络，图像特征作为encoder部分qkv的输入，文本特征作为decoder部分k和v的输入。</p><h3 id="Instance-Sequence-Matching-and-Loss"><a href="#Instance-Sequence-Matching-and-Loss" class="headerlink" title="Instance Sequence Matching and Loss"></a>Instance Sequence Matching and Loss</h3><p>前面提到过，对于T帧图像，N个object queries，会得到$N_q=T*N$个预测结果。由于视频中跨帧的物体在相邻帧间保持相对相同的位置，可以将预测结果看成是N个实例在T帧上的轨迹</p><p>因此可以用instance matching strategy的方式对预测整体监督。将预测集集表示为$y=\{y_i\}^N_{i=1}$，第i个实例的预测表示为:</p><script type="math/tex; mode=display">y_i=\{p^t_i, b^t_i, s^t_i\}^T_{t=1}</script><p>$p^t$表示一个概率的标量，用于表示实例是否在文本中被提到且在当前帧出现的概率；$b^t_i$是一个长度为4的向量，表示实例的坐标，具体定义是目标的中心坐标和宽高；<br>$s^t_i$的维度为(H/4 * W/4),表示实例的分割mask。<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-235737@2x.png" alt="enter description here"></p><p>损失函数部分由3部分组成，分类loss为focal loss，坐标回归loss为L1 Loss和GIOU Loss，分割loss为DICE Loss和binary mask focal loss</p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>在推理时，模型会预测所有帧中对应实例类别的置信度，选择平均置信度最高的实例类别对应的定位和分割结果作为最终的输出结果</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>论文中有非常详细的和其他方法的对比和消融实验<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233839@2x.png" alt="enter description here"></p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233851@2x.png" alt="enter description here"></p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233900@2x.png" alt="enter description here"></p><p>表4中baseline的方法训练和推理阶段使用的都是长度为5帧的视频，baseline的方法不能有效的区分距离较近的相似物体，容易分割出最显著的区域，相比之下，通过表6(a)可以看出，只需要一个object queries就可以获得比baseline更好的效果，也证明了动态卷积对于分割任务是必要的</p><ul><li>baseline的方式使用图像level的特征分割物体，而本文的方法是用图像-文本融合后的多模态特征</li><li>本文将动态卷积核的相对坐标和掩码特征融合起来，有助于模型定位出文本中提到的目标</li></ul><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233911@2x.png" alt="enter description here"></p><p>不同backbone的影响<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233918@2x.png" alt="enter description here"></p><p><strong>object queries数量的影响</strong>，从实验结果来看数量也不是越多越好，5个是实验中的最好效果。<br><strong>训练和测试帧数的影响</strong>，最好的是5帧的参数，但理论上应该帧数增多，效果还会提升，可能是受限于计算资源的影响<br><strong>预测头的影响</strong>，类别，边框回归，分割三个任务都预测，效果是最好的<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-234022@2x.png" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;\assets\js\APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;p&gt;由于之前看图片相关的论文比较多，而这篇论文偏向于视频理解的任务，为了避免有其他不了解这篇论文领域的同学看的时候比较吃力，在解读这篇论文前先介绍这篇论文的任务。&lt;br&gt;&lt;str
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="视频理解" scheme="https://oysz2016.github.io/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/"/>
    
      <category term="多模态" scheme="https://oysz2016.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>《Faster R-CNN》详解</title>
    <link href="https://oysz2016.github.io/post/79b7b6f1.html"/>
    <id>https://oysz2016.github.io/post/79b7b6f1.html</id>
    <published>2019-06-08T06:35:12.157Z</published>
    <updated>2019-06-08T07:20:44.007Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p></p><p class="description"></p><br><a id="more"></a><p></p><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><blockquote><p>最近实在是太忙了，很久没更新博客。想起前几周花了一些功夫给组内的小伙伴们做了个分享，为了做分享时自己能讲清楚，找了不少我认为描述的比较好的图片。Faster R-CNN可以说是目标检测领域的开山之作了，即使现今的顶会论文中，目标检测方面 的论文也有不少是对Faster R-CNN做的改进。这么经典的文章，细节又多如牛毛，真是常读常新。所以搬运到博客上应该也是有一些价值的。</p></blockquote><h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196152.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978195967.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196153.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978195729.jpg" alt="enter description here"></p><blockquote><p>写作的过程是将网状的思考用树状的语法结构转换成线性的文字，阅读则是其逆向过程。读文章时并不用逐字逐句的按文章的书写顺序去读。可以按照合适的方式去更好的还原作者的思考过程。及时的停止读一篇文章，能及时止损，每个人精力和注意力都是宝贵的，不应该把时间花在读了对自己没价值的文章上。</p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196154.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196156.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196019.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196293.jpg" alt="enter description here"></p><blockquote><p>检测任务目前的弊病之一就在于其网络的特征提取层直接从分类网络迁移学习来，并不是很贴合检测任务。</p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196020.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196294.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196021.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196024.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196026.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196314.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978195920.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196289.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196118.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978195946.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196290.jpg" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;
    
    </summary>
    
      <category term="输入分类" scheme="https://oysz2016.github.io/categories/%E8%BE%93%E5%85%A5%E5%88%86%E7%B1%BB/"/>
    
    
      <category term="输入标签" scheme="https://oysz2016.github.io/tags/%E8%BE%93%E5%85%A5%E6%A0%87%E7%AD%BE/"/>
    
  </entry>
  
  <entry>
    <title>《GIoU》论文笔记</title>
    <link href="https://oysz2016.github.io/post/4065a784.html"/>
    <id>https://oysz2016.github.io/post/4065a784.html</id>
    <published>2019-03-17T04:25:09.918Z</published>
    <updated>2019-03-17T05:52:44.516Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p></p><p class="description"></p><br><a id="more"></a><p></p><p>论文链接：<a href="https://arxiv.org/abs/1902.09630" target="_blank" rel="noopener">https://arxiv.org/abs/1902.09630</a><br>代码链接：<a href="https://github.com/generalized-iou" target="_blank" rel="noopener">https://github.com/generalized-iou</a></p><p>&emsp;&emsp;这篇论文出自CVPR2019，算是目前已被录用且公布的位数不多的目标检测相关论文了。这篇论文提出了一种优化边界框的新方式——GIoU(generalized IoU，广义IoU)。<em>目前关于IOU的新用法真是层出不穷，从<a href="https://oysz2016.github.io/post/d76cc2d4.html">Cascade R-CNN</a>到<a href="https://oysz2016.github.io/post/4395ef5a.html">IOU Net</a>再到如今的GIoU，GIoU的方法是这些论文中相对简单的，相信很多朋友了解了这篇文章的远离后，内心的OS都是“总觉得损失函数可以优化，这么简单我怎么没想到呢？”，哈哈，反正我是这样想的了。下面来看看这篇文章所提出的方法吧。</em></p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>&emsp;&emsp;目前目标检测中主流的边界框优化采用的都是BBox的回归损失(MSE loss, L1-smooth loss等)，这些方式计算损失值得方式都是检测框得“代理属性”，而忽略了检测框本身最显著的性质——IoU。如下图所示，在L1及L2范数取到相同的值时，实际上检测效果却是差异巨大的，直接表现就是预测和真实检测框的IoU值变化较大，这说明L1和L2范数不能很好的反映检测效果。</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1552801043208.jpg" alt="enter description here"></p><p>&emsp;&emsp;除了能反映预测检测框与真实检测框的检测效果外，IoU还具有<strong>尺度不变性</strong>。可是既然IOU这么好，为什么之前不直接用IoU呢，这是由于IoU有两个缺点，导致其不太适合做损失函数：</p><ul><li>但检测框与gt之间没有重合时，IoU为0。而在优化损失函数时，梯度为0，意味着无法优化</li><li>在检测框与gt之间IoU相同时，检测的效果也具有较大差异，如下图所示：<br><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1552801042668.jpg" alt="enter description here"></li></ul><p>&emsp;&emsp;基于IoU的优良特性和其作为损失函数时的致命缺点，作者提出了一个新的概念——GIoU</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>&emsp;&emsp;GIoU的定义如下图所示，</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1552801042664.jpg" alt="enter description here"></p><p>&emsp;&emsp;根据定义，GIoU具有如下性质：</p><ul><li>GIoU具有作为一个度量标准的优良性质。包括非负性，同一性，对称性，以及三角不等式的性质</li><li>与IoU相似，具有尺度不变性</li><li>GIoU的值总是小于IoU的值</li><li>对于两个矩形框A和B，0≤IoU(A，B)≤1，而-1≤GIoU≤1</li><li>在A，B没有良好对齐时，会导致C的面积增大，从而使GIoU的值变小，而两个矩形框不重合时，依然可以计算GIoU，一定程度上解决了IoU不适合作为损失函数的原因</li></ul><p>&emsp;&emsp;GIoU作为损失函数时计算方式如下的算法2</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1552801043209.jpg" alt="enter description here"></p><p>&emsp;&emsp;从算法中可以看到和GIoU的计算方式和IoU的步骤基本保持一致，在得到IoU的值后在根据上面的算法1计算GIoU的值。<strong>这里还不太清楚方向传播时，梯度是怎么计算的。等我看看源码再来更新吧</strong></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>&emsp;&emsp;作者分别在几种主流的目标检测算法上做了实验，分别是YoLo、Faster R-CNN和Mask R-CNN。这里贴上在Pascal Voc数据集上的实验结果，如下</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1552801042667.jpg" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1552799016561.jpg" alt="enter description here"></p><p>&emsp;&emsp;实验结果中在YoLo v3上可以看到GIoU相比IoU的损失函数有较大幅度的提升，而在faster r-cnn中GIoU和IoU作为损失函数的区别不大，这里作者给出的解释是faster rcnn的anchor更密集，导致不易出现与gt不重叠的检测框。<em>其实个人认为，anchor多的情形与gt不重叠的检测框才多，更根本的原因应该是RPN网络进行了一次粗检后，滤去了大部分跟gt没有重合的检测框。导致GIoU相比IoU的损失函数提升不明显吧</em></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;GIoU的方法很简单，巧妙的是优化的点。通过广义IoU作为损失函数替代bbox回归还是很有趣的。不过疑惑的是实验结果中的检测AP值都非常低，原生的faster rcnn在pascal voc上的检测效果都不会这么差。从实验对比上GIoU的损失函数相比原始的损失函数在准确率不到40%的效果上来说确实有较大幅度的提升。然而要是换到准确率较高的baseline上呢？这一点还需要实验验证。<br>&emsp;&emsp;另外总感觉这篇论文有点点到即止，没有更多的实验验证bbox作为损失函数存在缺陷的原因。</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="https://oysz2016.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>《Mask TextSpotter》论文笔记</title>
    <link href="https://oysz2016.github.io/post/f222e8ff.html"/>
    <id>https://oysz2016.github.io/post/f222e8ff.html</id>
    <published>2019-02-17T07:18:21.458Z</published>
    <updated>2019-03-17T05:48:16.802Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p></p><p class="description"></p><br><a id="more"></a><p></p><p>&emsp;&emsp;论文链接：<a href="https://arxiv.org/abs/1807.02242" target="_blank" rel="noopener">https://arxiv.org/abs/1807.02242</a><br>&emsp;&emsp;代码链接：<a href="https://github.com/lvpengyuan/masktextspotter.caffe2" target="_blank" rel="noopener">https://github.com/lvpengyuan/masktextspotter.caffe2</a></p><p>&emsp;&emsp;除了自然场景的目标检测外，文本检测也是近年来热门的研究领域。Mask TextSpotter是<strong>ECCV 2018</strong>发表的一篇文本检测文章，具有以下特点：</p><ul><li><strong>端到端的检测+识别框架</strong></li><li><strong>基于Mask R-CNN结构</strong></li><li><strong>在处理不规则的文本形状时，优于之前的方法</strong></li><li><strong>除了文本行检测外，能进行字符分割</strong></li></ul><p>&emsp;&emsp;Mask R-CNN结构是近几年来最优秀的目标检测结构了，该篇文章将其应用到文本检测这一任务上，并且所做的优化算是十分巧妙了。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>&emsp;&emsp;Mask TextSpotter提出了一种名为掩码文本检测的文本检测器，它可以<strong>检测和识别任意形状</strong>的文本。</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1550388479976.jpg" alt="enter description here"></p><p>&emsp;&emsp;上图直观的比较了不同的检测方法其场景文本定位的效果，左图是水平水平文本定位方法，中间是支持倾斜框的文本检测方法，右图是Mask TextSpotter的检测方法。</p><p>&emsp;&emsp;可以看到Mask TextSpotter相比较于其他两种方法能够提供更准确的定位图，为后续的识别提供良好的检测效果。<br>这篇文章总共有四点贡献：</p><ol><li>提出了一种端到端的文本检测加识别模型，具有简单高效的特点</li><li>该方法可以检测和识别各种形状的文本，包括水平文本、定向文本和弯曲文本</li><li>与之前的方法相比，该方法通过语义分割来实现精确的文本检测和识别</li><li>在各种基准的文本检测和定位上都达到了state-of-the-art的效果</li></ol><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>&emsp;&emsp;这部分主要介绍了场景文本检测、识别及结合的发展进程。强调了Mask TextSpotter基于Mask R-CNN，区别在于该方法不仅可以分割文本，也可以进行字符分割。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><p>&emsp;&emsp;Mask TextSpotter的架构如下图所示：</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753793.png" alt="enter description here"></p><p>&emsp;&emsp;该架构由以下4部分组成</p><ul><li>backbone：ResNet50；FPN网络（ top-down 结构）</li><li>生成文本建议：RPN</li><li>边界框回归：Fast R-CNN</li><li>文本和字符实例分割：mask branch</li></ul><p>&emsp;&emsp;首先由RPN生成大量的文本提案，然后将提案的RoI特征输入Fast R-CNN分支和mask分支中，生成准确的文本候选框、文本实例分割图和字符分割图。</p><p>&emsp;&emsp;每部分的细节如下：</p><ul><li><strong>Backbone</strong>:自然场景的文本大小不一，为了提取更高层的语义特征。采用了ResNet-50网络，并用对小目标有较好效果的FPN网络提取特征</li><li><strong>RPN</strong>:RPN网络为Fast R-CNN分支以及mask分支生成文本建议。anchor设置5种尺寸$\{32^2,64^2,128^2,256^2,512^2\}$，FPN网络中有5个层级$\{P_2,P_3,P_4,P_5\}$，3种比例$\{0.5,1,2\}$。区域特征映射方式采用ROI Align。</li><li><strong>Fast R-CNN</strong>:包括分类和回归任务，主要作用是为了后续的检测提供更精确的检测框。</li><li><strong>Mask 分支</strong>：掩码分支有两个任务，分别是全局文本实例分割和字符分割。如下图所示。Mask分支的输入是固定大小的ROI（16<em>64）,经过4个卷积层和一个反卷积层将特征图降维到38个维度（特征图大小：32</em>128）。这38个维度由以下3部分组成：<ul><li>全局文本实例分割</li><li>背景分割</li><li>10个数字，26个字母</li></ul></li></ul><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753811.png" alt="enter description here"></p><h3 id="标签的生成"><a href="#标签的生成" class="headerlink" title="标签的生成"></a>标签的生成</h3><p>&emsp;&emsp;为了满足训练的要求，ground truth要包含$P=P\{p_1,p_2…p_m\}$以及$C=\{c_1=(cc_1,cl_1),c_2=(cc_2,cl_2),…,c_n=(cc_n,cl_n)\}$。其中$p_i$表示一个文本定位的多边形。$cc_j$和$cl_j$分别是字符的类别和定位。<strong>值得一提的是并不要求所有样本都需要有标记$C$。</strong><br>首先用涵盖目标最小水平矩形面积的方法，将多边形转换为水平矩形。然后通过RPN和Fast R-CNN网络生成区域建议。对于具有ground truth P, C(可能不存在)的mask分支，需要生成两种类型的目标映射，以及RPN生成的建议：用于全局实例分割的映射和用于字符实例分割的映射。对于正样本的proposal,首先得到最匹配的水平矩形。相应的多边形及字符（如果有的话）可以进一步得到。在映射$H×W$上调整多边形和字符的proposal一致的公式如下：</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753633.png" alt="enter description here"></p><p>&emsp;&emsp;其中$(B_{x0},B_{y0})$是所有多边形和字符原始的顶点，$(B_x,B_y)$是所有多变形和字符更新后的顶点，$(r_x,r_y)$是rpoposal产生的顶点。</p><p>&emsp;&emsp;然后，全局映射图的生成规则：通过绘制zero-initialized mask的规则多边形；字符边界框的生成：固定所有字符的中心点，并将边缩短到原始边的1/4。如下图所示：</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753814.png" alt="enter description here"></p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>&emsp;&emsp;该部分讨论的是整个框架优化参数时的损失函数组成，正如上面介绍的Mask TextSpotter的框架由4部分组成，除了提取特征的网络外，其余的3部分都是损失函数的组成部分，如下所示：</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753635.png" alt="enter description here"></p><p>&emsp;&emsp;mask分支有两个任务，因此$L_{mask}$的计算如下：</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753768.png" alt="enter description here"></p><p>&emsp;&emsp;其中$L_{global}$为<a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank" rel="noopener">Cross entropy</a>损失，而$L_{char}$为<a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank" rel="noopener">Softmax</a>损失。其中作者设置的超参数$α_1,α_2,β$都为1。<br>&emsp;&emsp;$L_{global}$的计算公式如下：</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753769.png" alt="enter description here"></p><p>&emsp;&emsp;$L_{char}$的计算公式如下：</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753770.png" alt="enter description here"></p><p>&emsp;&emsp;其中T为类别，X为预测的输出，Y为gt，W为权重。W的主要作用是为了样本均衡。不同类别的W计算公式如下：</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753771.png" alt="enter description here"></p><h3 id="推理-测试"><a href="#推理-测试" class="headerlink" title="推理(测试)"></a>推理(测试)</h3><p>&emsp;&emsp;不同于mask 分支的ROI来自于RPN网络，在推理阶段，mask分支的ROI来自于Fast R-CNN网络，而不是RPN网络，这是由于Fast R-CNN的输出更精确。<br>推理阶段可以分为以下几个过程：</p><ol><li>输入测试图片，获得Fast R-CNN的输出，并经过NMS;</li><li>保存下来的proposals被输入mask分支以生成全局映射(gloabal maps)和字符映射(character maps)</li><li><p>通过计算全局映射的map区域获得预测的多边形，通过pixel voting算法生成字符映射序列</p><p><strong>pixel voting</strong>算法的细节如下：</p></li></ol><ol><li>二值化背景图，阈值为192</li><li>根据二值化中的联通与划分所有字符区域</li></ol><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>&emsp;&emsp;作者总共使用了4个数据集，除了SynthText用来预训练以外，其余的三个数据集ICDAR 2013，ICDAR2015，Total_Text均做了测试实验。</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387754029.png" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387754097.png" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387754189.png" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387754099.png" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387754186.png" alt="enter description here"></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753779.png" alt="enter description here"></p><p>&emsp;&emsp;通过以上实验数据比较，Mask TextSpotter水平文本、定向文本和弯曲文本等数据集上的良好性能证明了该方法对文本检测和端到端文本识别的有效性和鲁棒性。</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="https://oysz2016.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="文本检测" scheme="https://oysz2016.github.io/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>简单线性回归</title>
    <link href="https://oysz2016.github.io/post/7fda15c2.html"/>
    <id>https://oysz2016.github.io/post/7fda15c2.html</id>
    <published>2019-01-13T09:10:18.243Z</published>
    <updated>2019-03-17T05:49:12.536Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p></p><p class="description"></p><br><a id="more"></a><p></p><p>&emsp;&emsp;好长一段时间挺想重新系统的学习回顾下机器学习的知识，看了些深度学习的论文和框架后，觉得机器学习的知识确实太重要了。接下来的半年时间，想系统的总结实践下，也算是2019年的第一个flag吧。</p><h2 id="使用单一特征预测响应值"><a href="#使用单一特征预测响应值" class="headerlink" title="使用单一特征预测响应值"></a>使用单一特征预测响应值</h2><p>&emsp;&emsp;这是一种基于自变量值(X)来预测因变量值(Y)的方法。假设X和Y两个变量是线性相关的。线性回归就是尝试寻找一种根据特征或自变量(X)的线性函数来精确预测响应值(Y)。</p><h2 id="怎样找到最佳的拟合线"><a href="#怎样找到最佳的拟合线" class="headerlink" title="怎样找到最佳的拟合线"></a>怎样找到最佳的拟合线</h2><p>&emsp;&emsp;在这个回归任务中，我们将通过找到“最佳拟合线”来最小化预测误差——回归线应该尽量拟合X-Y的分布，即误差是最小的。例如$y_p$是预测值，$y_i$是实际值，这个过程就是使$y_p$和$y_i$之间的关系满足$min\{SUM(y_i-y_p)^2\}$</p><p>&emsp;&emsp;这里以学生分数数据集做这个实验，实验数据如下图所示：</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1547371646833.jpg" alt="enter description here"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ol><li>导入相关库</li><li>导入数据集</li><li>检查缺失数据</li><li>划分数据集</li><li>特征缩放(这里使用简单线性模型的相关库进行)</li></ol><h3 id="通过训练集来训练简单线性回回归模型"><a href="#通过训练集来训练简单线性回回归模型" class="headerlink" title="通过训练集来训练简单线性回回归模型"></a>通过训练集来训练简单线性回回归模型</h3><p>&emsp;&emsp;为了使用模型来训练数据集，这里使用python中的<code>sklearn.linear_model</code>库的<code>LinearRegression</code>类。然后实例化一个<code>LinearRegression</code>类的<code>regressor</code>对象。最后使用<code>LinearRegression</code>类的<code>fit()</code>方法。将<code>regressor</code>对象对数据集进行训练。</p><h3 id="预测结果"><a href="#预测结果" class="headerlink" title="预测结果"></a>预测结果</h3><p>&emsp;&emsp;现在将预测来自测试集的观察结果。将实际的输出保存在向量<code>Y_pred</code>中。使用前一步中训练的回归模型<code>regressor</code>的<code>LinearRegression</code>类的预测方法来对结果进行预测。</p><h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><p>&emsp;&emsp;为了直观的查看线性回归的效果，这里将对结果进行可视化。使用<code>matplotlib.pyplot</code>库对我们的训练结果和测试集结果做散点图，以查看模型的预测效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#python的数据处理库</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一步：数据预处理</span></span><br><span class="line">dataset = pd.read_csv(<span class="string">'../datasets/studentscores.csv'</span>)</span><br><span class="line">X = dataset.iloc[ : ,   : <span class="number">1</span> ].values</span><br><span class="line">Y = dataset.iloc[ : , <span class="number">1</span> ].values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = <span class="number">1</span>/<span class="number">4</span>, random_state = <span class="number">0</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步：训练集使用简单线性回归模型来训练</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor = regressor.fit(X_train, Y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步：预测结果</span></span><br><span class="line">Y_pred = regressor.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步：可视化</span></span><br><span class="line"><span class="comment"># 训练集结果可视化</span></span><br><span class="line">plt.scatter(X_train , Y_train, color = <span class="string">'red'</span>)</span><br><span class="line">plt.plot(X_train , regressor.predict(X_train), color =<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集结果可视化</span></span><br><span class="line">plt.scatter(X_test , Y_test, color = <span class="string">'red'</span>)</span><br><span class="line">plt.plot(X_test , regressor.predict(X_test), color =<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>&emsp;&emsp;训练集上的拟合结果：<br><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1547370582187.jpg" alt="enter description here"></p><p>&emsp;&emsp;测试集上拟合结果<br><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1547370582188.jpg" alt="enter description here"></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://oysz2016.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://oysz2016.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>(IOU-Net)《Acquisition of Localization Confidence for Accurate Object Detection》论文笔记</title>
    <link href="https://oysz2016.github.io/post/4395ef5a.html"/>
    <id>https://oysz2016.github.io/post/4395ef5a.html</id>
    <published>2019-01-12T05:40:24.263Z</published>
    <updated>2019-03-17T05:48:10.514Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p></p><p class="description"></p><br><a id="more"></a><p></p><p>论文链接：<a href="https://arxiv.org/abs/1711.07767" target="_blank" rel="noopener">https://arxiv.org/abs/1711.07767</a><br>论文代码：<a href="https://github.com/ruinmessi/RFBNet" target="_blank" rel="noopener">https://github.com/ruinmessi/RFBNet</a></p><p>&emsp;&emsp;目标检测框架中目前主要依靠边界框回归和非极大值抑制来定位对象，并且非极大值抑制算法去除重复框的依据是候选框的分类置信度，在这个过程中缺少了一个边框筛选的重要参考——<strong>定位置信度</strong>。因此这篇文章提出<strong>IOU-Net</strong>，IOU-Net具有以下优点：</p><ul><li>通过在网络中加入定位置信度，在NMS算法中用定位置信度代替分类置信度作为去重复框的参考</li><li>提出了一种<strong>基于优化的边界框修正</strong>算法</li></ul><h2 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h2><p>&emsp;&emsp;作者先根据实验定性的分析了仅用分类置信度评估检测框是否准确不太合适，缺少分类置信度回带来两个<strong>缺点</strong>：</p><ol><li>分类置信度被用作对检测框排序的度量标准，并且在NMS算法中忽略了定位的精度</li><li>缺少定位置信度，使得广泛使用BBox回归缺少可解释性，这点在《cascade r-cnn》中也有提到多次使用BBox回归可能导致检测框的局部退化。</li></ol><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292464876.jpg" alt="enter description here"></p><p>&emsp;&emsp;上图中，黄色框表示gt，绿色和红色都是是模型预测得到的检测框。如图(a)所示，绿色检测框与红色检测框相比更适合作为最终的预测结果，绿色框与gt的IOU(交并比)更高，定位置信度也更高，但分类置信度却低于红色的检测框，在以往NMS算法中以分类作为检测框排序标准，会保留红色的框，而滤去绿色的，这显然是不够合理的。图(b)比较的是基于优化的边界框修正比基于回归的边界框修正有更好的效果(多次应用基于回归的边界框修正，会导致检测框退化)<br>基于上图中目前目标检测框架的不足，作者IOU-Net所做的两点改进：</p><ol><li>通过在目标检测框架中引入定位置信度。在NMS算法中，预测检测框与gt之间IOU的顺序，代替原来的以分类置信度排序</li><li>提出了一种基于优化的边界框修正算法</li></ol><p>&emsp;&emsp;IOU-Net中的IOU指的就是<strong>网络预测到的检测框与gt之间的IOU</strong></p><h2 id="深入研究目标定位"><a href="#深入研究目标定位" class="headerlink" title="深入研究目标定位"></a>深入研究目标定位</h2><p>&emsp;&emsp;作者在绪论中提到了所作的两点改进，绪论中定性的用图表表明了缺少定位置信度带来的缺点，接下来分别做了几个实验定量的验证所提出方法的有效性。以下实验所采用的训练集是MS-COCO trainval35k,测试集用minival，检测框架用FPN网络。</p><h3 id="类别不匹配和定位精度"><a href="#类别不匹配和定位精度" class="headerlink" title="类别不匹配和定位精度"></a>类别不匹配和定位精度</h3><p>&emsp;&emsp;作者先简单介绍了NMS算法的原理，及关于这方面的研究进展。随后指出NMS算法中，以分类置信度作为检测框的度量标准不太合理。</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292464564.jpg" alt="enter description here"></p><p>&emsp;&emsp;如上图所示，(a)横轴是预测的检测框与gt之间的IOU,纵轴是分类置信度。(b)横轴同样是预测的检测框与gt之间的IOU,纵轴是定位置信度。若以检测框与gt之间的IOU大于0.5作为是否检测到的阈值，作者计算了两张图横纵轴的Pearson correlation(皮尔森相关系数<a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" target="_blank" rel="noopener">[wiki百科]</a>)，结论是(a)的相关的相关系数为0.217，而(b)的相关性为0.617。<br>&emsp;&emsp;作者也总结了分类置信度与定位是否准确关系不大的原因：<strong>目标检测框架中分类和定位的相关性不强的原因在于正样本和负样本的选取方式，例如获取正样本只用其与gt之间的IOU大于指定阈值即可</strong>。关于正样本的生成方式，之前我也想过为什么这样生成，如此的生成方式一定程度上会带来定位不准确的问题。但是相比较于完全使用标注的gt这种方式具有以下的优点：</p><ul><li><strong><em>可以生成多个比例的候选框，覆盖样本的不同尺度，正样本会更好的回归</em></strong></li><li><strong><em>样本均衡一直是目标检测算法的一个难题，这种方式一定程度上可以生成更多的正样本</em></strong></li></ul><p>&emsp;&emsp;很明显正样本的选取方式带来的好处远大于其缺点，作者也表明，定位不准主要是缺少定位置信度的原因</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292464535.jpg" alt="enter description here"></p><p>&emsp;&emsp;传统NMS算法中由于用分类置信度作为MNS算法中检测框的度量标准，会导致与gt有更大IOU的检测框被抑制。如上图所示，蓝色代表传统的NMS算法；黄色表示以定位置信度作为度量标注的NMS算法；绿色表示不用NMS时，理论能生成的最多检测框数量。可以看到，在传统的NMS算法中由于缺少定位置信度，保留的检测框中与gt的IOU在0.9以上的会被抑制。</p><h3 id="BBox回归的非单调性"><a href="#BBox回归的非单调性" class="headerlink" title="BBox回归的非单调性"></a>BBox回归的非单调性</h3><p>&emsp;&emsp;faster rcnn中使用了两次边界框回归，以达到位置精修的目的。但位置精修的次数是不是越多越好，《cascade r-cnn》中也提到了这样的疑惑，作者做了个实验。</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292464267.jpg" alt="enter description here"></p><p>&emsp;&emsp;上图中，横轴代表边界框回归结构的迭代次数，纵轴代表检测精度。从上图可以看到，无论是FPN网络还是Cascade R-CNN网络，在多次使用边界框回归之后，都会出现退化现象。作者解释出现这种情况的是<strong>由于缺少定位置信度，对模型不能进行细粒度(fine-grained)的控制，例如对不同的检测框采用不同的迭代次数</strong>。</p><h2 id="IOU-Net"><a href="#IOU-Net" class="headerlink" title="IOU-Net"></a>IOU-Net</h2><p>&emsp;&emsp;为了定量分析IOU-Net的有效性，接下来作者给出了IOU-Net的框架和NMS算法中怎样用IOU进行预测以及基于回归的边界框精修算法。</p><h3 id="IOU-Net架构"><a href="#IOU-Net架构" class="headerlink" title="IOU-Net架构"></a>IOU-Net架构</h3><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292464269.jpg" alt="enter description here"></p><p>&emsp;&emsp;如上图所示，IOU-Net框架中提特征的网络部分与FPN网络一致，并估计每个边界框的定位精度(IOU)，通过数据扩充生成用于训练IOU-Net的边框和标签，而不是接受来自RPN的建议(说实话这里有点奇怪，RPN网络作用之一是用来位置精修的，不用之前RPN网络中的建议，这个没太理解作者的意思)。细节实施上，作者表明在生成正样本时，从候选集中移除与gt的IOU小于0.5的样本，然后对所有样本统一采样训练，据作者所说这样可以获得更好的性能和鲁棒性</p><h3 id="使用IOU度量的NMS"><a href="#使用IOU度量的NMS" class="headerlink" title="使用IOU度量的NMS"></a>使用IOU度量的NMS</h3><p>&emsp;&emsp;这里作者主要介绍了在NMS去除重复框时，如何使用IOU作为度量标准。伪代码如下所示：</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292464539.jpg" alt="enter description here"></p><p>&emsp;&emsp;特点是采用了基于聚类的规则来更新分类置信度。具体实施过程是当boxi将boxj移除了，会更新boxi的分类置信度为max(si,sj)</p><h3 id="将边界框精修作为一个优化过程"><a href="#将边界框精修作为一个优化过程" class="headerlink" title="将边界框精修作为一个优化过程"></a>将边界框精修作为一个优化过程</h3><p>&emsp;&emsp;边界框精修的数学公式定义如下：</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292463491.jpg" alt="enter description here"></p><p>&emsp;&emsp;其中 $box_{det}$是检测到的边界框，$box_{gt}$是实际的边界框.<code>trainsform</code>是用参数<code>c</code>对边界框变换的转换函数。$crit$是两个边界框的度量标准(这个标准在faster rcnn中是<code>smooth-L1</code>)</p><p>&emsp;&emsp;基于回归的算法用前馈神经网络直接估计最优解c* 。 然而，迭代边界框回归方法易受输入分布变化的影响，并可能导致非单调的回归退化，如图4所示。为了解决这些问题，作者提出一种基于优化的边界框精修方法。 利用IoU-Net作为鲁棒定位精度（IoU）估计器的方法。 此外，IoU估计器可以用作早期停止条件，以通过自适应步骤实现迭代精修。</p><p>&emsp;&emsp;IoU-Net直接估算IoU。 虽然所提出的精确RoI池化层能够计算关于边界框坐标的IoU的梯度，我们可以直接使用梯度上升方法找到方程1的最优解。在算法2中，将IoU的估计视为优化目标，我们迭代地使用计算的梯度更新边界框坐标和最大化检测到的边界框与其匹配的真值之间的IoU。 此外，预测的IoU是每个边界框上的定位置信度的可解释指示符，并且有助于解释所做的转换。</p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292463918.jpg" alt="enter description here"></p><p>&emsp;&emsp;关于算法2中的初始坐标，初始化采用了一次边界框回归。</p><h4 id="precise-ROI-Pooling"><a href="#precise-ROI-Pooling" class="headerlink" title="precise ROI Pooling"></a>precise ROI Pooling</h4><p>&emsp;&emsp;为了细粒度的修正边界框，作者引入了一种新的ROI Pooling方法，以代替之前的ROI Pooling和ROI Align。<strong>关于ROI Pooling到PrROI Pooling我会专门写一篇博文比较其区别，这里就先简单介绍下。</strong></p><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292463702.jpg" alt="enter description here"><br>&emsp;&emsp;如上图，绿色的点代表特征图(feature map)的值，虚线代表目标在映射在特征图中实际的位置。<br>RoI Pooling：ROI Pooling的弊病，在于其在特征图中的边界框只能取整数，因此会从虚线取整到实线，映射回原图时，会带来误差。<br>ROI Align:为了避免ROI pooling中取整带来的误差。ROI Align中会保持浮点数边界不变，将特征图中的值用双线性插值，由绿色的点映射到图中的4个红点。然后再做池化<br>PrRoI Pooling: 如果付出浮点数边界内固定的点来做池化，这样对于目标实际的映射大小不具有适应性。PrRoI Pooling则使用二阶积分来做池化 </p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>&emsp;&emsp;针对作者提到的几个方法，分别做了几组实验。</p><ul><li>下面的表1总结了在不同检测框架中，使用不同NMS算法性能上的变化。可以看到IOU指导的NMS算法，在高IOU指标上性能会优于其他算法</li></ul><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292463704.jpg" alt="enter description here"></p><ul><li>基于优化的边界框精修</li></ul><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292463705.jpg" alt="enter description here"></p><ul><li>耗时实验：耗时还在可以接受的范围</li></ul><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292463508.jpg" alt="enter description here"></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="https://oysz2016.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>《Receptive Field Block Net for Accurate and Fast Object Detection》论文笔记</title>
    <link href="https://oysz2016.github.io/post/f38a4f4.html"/>
    <id>https://oysz2016.github.io/post/f38a4f4.html</id>
    <published>2019-01-06T07:50:46.841Z</published>
    <updated>2019-03-17T05:48:22.182Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p></p><p class="description"></p><br><a id="more"></a><p></p><p>论文链接：<a href="https://arxiv.org/abs/1711.07767" target="_blank" rel="noopener">https://arxiv.org/abs/1711.07767</a><br>论文代码：<a href="https://github.com/ruinmessi/RFBNet" target="_blank" rel="noopener">https://github.com/ruinmessi/RFBNet</a></p><p>&emsp;&emsp;这篇文章是CV领域顶会ECCV2018中关于目标检测的文章,文中以SSD模型为基础提出了RFB结构,强调兼顾速度与性能。说来也巧,因为项目需要,在看这篇论文之前正好看过提出dilated convolution的那篇文章，但是dilated convolution的结构获得更大感受野的方式确实对细粒度的分割会比较好，适用图像分割领域。当我还在想能怎么用在目标检测上时，就看到了RFB网络。虽然作者说是为了兼顾速度与性能将其应用到one stage的SSD上，但我也在two stage的faster rcnn上，复现出了较好的效果。这是一篇我个人很喜欢的文章，实验充分，模拟视觉细胞的结构让我觉得即简单又巧妙。因此简单总结下这篇文章。</p><h2 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h2><p>&emsp;&emsp;作者指出目前图像领域深度学习的发展越来越倾向于用更深的网络以达到更好的效果，然而像ResNet等很深的网络往往具有较大的计算量，导致速度受限。相比之下作者提出的RFB结构具有以下优点：</p><ol><li>模拟了人类视觉系统RFs的大小和离心率设置，增强轻量级CNN网络的特征提取能力</li><li>简单的替换了SSD的最后一级卷积层，在较少的计算增加的情况下，提升了模型的性能</li><li>除了SSD之外，也扩展到了MobileNet中取得了较好的结果，展示了结构的泛化性</li></ol><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>&emsp;&emsp;这部分就不总结了，主要介绍了one stage和two stage的目标检测模型和目前论文中在感受野上做的研究。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="视觉皮层"><a href="#视觉皮层" class="headerlink" title="视觉皮层"></a>视觉皮层</h3><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546754014816.jpg" alt="enter description here"><br>&emsp;&emsp;如上图所示是人类感受野(pRF)的示意图,可以看到有以下规律:</p><ul><li>距离中心越远的pRF越大，即pRF大小与偏心含有正相关的关系</li><li>不同图谱的pRF大小规模不同</li></ul><h3 id="感受野块"><a href="#感受野块" class="headerlink" title="感受野块"></a>感受野块</h3><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546754512995.jpg" alt="enter description here"></p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546759194131.jpg" alt="enter description here"><br>&emsp;&emsp;作者提出的RFB结构的原理如上图所示，该结构的特点有：</p><ul><li>多分支卷积层：根据之前人类感受野(pRF)的示意图，为了仿照不同图谱的pRF大小规模不同，作者提出用不同大小的卷积核以实现多大小的pRF，这一方法应该优于共享固定大小的RFs。这一结构参考了Inception的结构。</li><li>膨胀卷积和池化层：<strong>膨胀卷积的基本意图在于生成分辨率更高的特征图</strong>，在相同计算量的情况下获得更大的感受野。而膨胀卷积核的大小和扩张与pRFs在视觉皮层的大小和偏心具有相似的功能关系。然后再将不同膨胀卷积处理过的层融合起来，以达到视觉皮层中感受野的效果。rFB的结构如下图所示：</li></ul><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546755296592.jpg" alt="enter description here"></p><h3 id="RFB检测框架"><a href="#RFB检测框架" class="headerlink" title="RFB检测框架"></a>RFB检测框架</h3><p>&emsp;&emsp;作者提出的RFB的结构是在SSD的基础上改的，做的修改及替换如下图所示：</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546755756172.jpg" alt="enter description here"></p><ul><li>轻量级的结构：这里主要说的是SSD的有点，这里不赘述</li><li>多尺度结构中的RFB：在原始的SSD中，有着层叠的卷积层，形成一系列空间分辨率连续下降、感受野不断增大的feature map。在作者的实现中，保持了相同的SSD级联结构，<strong>但具有较大感受也的卷积层被RFB结构替代</strong>。作者还指出最后基层卷积层的特征图太小，适合用5X5大小的卷积核。<strong>这部分论文里Fig.4的a图中用的确实是5x5的卷积核，但是给出的代码中却用两个3x3的卷积核替代了，这部分我有点疑惑。</strong></li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>&emsp;&emsp;这一部分就是各种各样的实验图表了，也不赘述。从以下图表可以看到实验结果确实很惊艳，用了RFB结构的网络mAP会有不小的提升。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546756443789.jpg" alt="enter description here"></p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546756466104.jpg" alt="enter description here"></p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546756478399.jpg" alt="enter description here"></p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546757250177.jpg" alt="enter description here"></p><p>&emsp;&emsp;作者还给出了一张目前目标检测算法的准确率和耗时的图片，对比的多是one stage的模型，可以作为参考</p><p><img src="https://user-gold-cdn.xitu.io/2019/1/6/1682321abe0c6f46?w=717&amp;h=489&amp;f=png&amp;s=96823" alt=""></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="https://oysz2016.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>Faster R-CNN代码的caffe封装</title>
    <link href="https://oysz2016.github.io/post/fc7cd866.html"/>
    <id>https://oysz2016.github.io/post/fc7cd866.html</id>
    <published>2019-01-06T03:59:02.718Z</published>
    <updated>2019-03-17T05:48:40.004Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p></p><p class="description"></p><br><a id="more"></a><p></p><p>目前部署服务的主流语言还是C++，因此项目上线前，在需要部署调试的时候需要对Faster R-CNN工程化为<code>C++</code>代码。这篇博文总结的部分主要将python版本的demo.py代码及其相关的部分改写成<code>C++</code>版本。</p><h2 id="封装1"><a href="#封装1" class="headerlink" title="封装1"></a>封装1</h2><p>封装听起来很复杂，其实就是隐藏代码的实现细节，只暴露出对应的接口。Faster R-CNN具有很广泛的流行度了，相关的资料在可以说在目标检测模型里是最多的。<br>为了方便理解，首先给出<code>C++</code>工程demo的目录结构。</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">└── <span class="class"><span class="keyword">lib</span></span></span><br><span class="line">│   │── faster_rcnn.cpp</span><br><span class="line">│   │── faster_rcnn.hpp</span><br><span class="line">│   │── CMakeLists.txt</span><br><span class="line">│—— CMakeLists.txt</span><br><span class="line">│—— main.cpp</span><br></pre></td></tr></table></figure><p>其中faster_rcnn.cpp与faster_rcnn.hpp是对应的demo接口，main.cpp可以直接调用。<br>faster_rcnn.cpp文件如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> FASTER_RCNN_HPP</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FASTER_RCNN_HPP</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;  // for snprintf</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;boost/python.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"caffe/caffe.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"gpu_nms.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/highgui/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/imgproc/imgproc.hpp&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> caffe;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> max(a, b) (((a)&gt;(b)) ? (a) :(b))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> min(a, b) (((a)&lt;(b)) ? (a) :(b))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//background and car</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> class_num=<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * ===  Class  ======================================================================</span></span><br><span class="line"><span class="comment"> *         Name:  Detector</span></span><br><span class="line"><span class="comment"> *  Description:  FasterRCNN CXX Detector</span></span><br><span class="line"><span class="comment"> * =====================================================================================</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Detector</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Detector(<span class="keyword">const</span> <span class="built_in">string</span>&amp; model_file, <span class="keyword">const</span> <span class="built_in">string</span>&amp; weights_file);</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Detect</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; im_name)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">bbox_transform_inv</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">float</span>* box_deltas, <span class="keyword">const</span> <span class="keyword">float</span>* pred_cls, <span class="keyword">float</span>* boxes, <span class="keyword">float</span>* pred, <span class="keyword">int</span> img_height, <span class="keyword">int</span> img_width)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">vis_detections</span><span class="params">(cv::Mat image, <span class="keyword">int</span>* keep, <span class="keyword">int</span> num_out, <span class="keyword">float</span>* sorted_pred_cls, <span class="keyword">float</span> CONF_THRESH)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">boxes_sort</span><span class="params">(<span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">float</span>* pred, <span class="keyword">float</span>* sorted_pred)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;Net&lt;<span class="keyword">float</span>&gt; &gt; net_;</span><br><span class="line">    Detector()&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Using for box sort</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Info</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">float</span> score;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span>* head;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">compare</span><span class="params">(<span class="keyword">const</span> Info&amp; Info1, <span class="keyword">const</span> Info&amp; Info2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Info1.score &gt; Info2.score;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><p>其对应的faster_rcnn.hpp文件为</p><figure class="highlight hpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> FASTER_RCNN_HPP</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FASTER_RCNN_HPP</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;  // for snprintf</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;boost/python.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"caffe/caffe.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"gpu_nms.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/highgui/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/imgproc/imgproc.hpp&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> caffe;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> max(a, b) (((a)&gt;(b)) ? (a) :(b))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> min(a, b) (((a)&lt;(b)) ? (a) :(b))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//background and car</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> class_num=<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * ===  Class  ======================================================================</span></span><br><span class="line"><span class="comment"> *         Name:  Detector</span></span><br><span class="line"><span class="comment"> *  Description:  FasterRCNN CXX Detector</span></span><br><span class="line"><span class="comment"> * =====================================================================================</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Detector</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Detector(<span class="keyword">const</span> <span class="built_in">string</span>&amp; model_file, <span class="keyword">const</span> <span class="built_in">string</span>&amp; weights_file);</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Detect</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; im_name)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">bbox_transform_inv</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">float</span>* box_deltas, <span class="keyword">const</span> <span class="keyword">float</span>* pred_cls, <span class="keyword">float</span>* boxes, <span class="keyword">float</span>* pred, <span class="keyword">int</span> img_height, <span class="keyword">int</span> img_width)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">vis_detections</span><span class="params">(cv::Mat image, <span class="keyword">int</span>* keep, <span class="keyword">int</span> num_out, <span class="keyword">float</span>* sorted_pred_cls, <span class="keyword">float</span> CONF_THRESH)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">boxes_sort</span><span class="params">(<span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">float</span>* pred, <span class="keyword">float</span>* sorted_pred)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;Net&lt;<span class="keyword">float</span>&gt; &gt; net_;</span><br><span class="line">    Detector()&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Using for box sort</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Info</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">float</span> score;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span>* head;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">compare</span><span class="params">(<span class="keyword">const</span> Info&amp; Info1, <span class="keyword">const</span> Info&amp; Info2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Info1.score &gt; Info2.score;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><p>初次在linux上运行C++代码时，真是各种蒙圈，看着Makefile文件也搞不明白其中关键字代表的意义，使用cmake后，感觉容易了很多。如下是lib文件夹底下，即<code>faster_rcnn.cpp</code>对应的<code>CMakeLists.txt</code>文件。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span> (VERSION <span class="number">2.8</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> (SRC_LIST faster_rcnn.cpp)</span><br><span class="line"><span class="keyword">include_directories</span> ( <span class="string">"$&#123;PROJECT_SOURCE_DIR&#125;/../../caffe-fast-rcnn/include"</span></span><br><span class="line">    <span class="string">"$&#123;PROJECT_SOURCE_DIR&#125;/../../lib/nms"</span> </span><br><span class="line">    /usr/local/<span class="keyword">include</span> </span><br><span class="line">    /usr/<span class="keyword">include</span>/python2.<span class="number">7</span></span><br><span class="line">    /usr/local/cuda/<span class="keyword">include</span> )</span><br><span class="line"><span class="keyword">add_library</span>(faster_rcnn SHARED <span class="variable">$&#123;SRC_LIST&#125;</span>)</span><br></pre></td></tr></table></figure><p>然后依次执行<code>cmake.</code>和<code>make</code>就完成编译了。接下来是调用<code>faster_rcnn.cpp</code>接口的<code>main.cpp</code>文件。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"faster_rcnn.hpp"</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">string</span> model_file = <span class="string">"/home/ouyang/Program/py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt"</span>;</span><br><span class="line">    <span class="built_in">string</span> weights_file = <span class="string">"/home/ouyang/Program/py-faster-rcnn/output/faster_rcnn_end2end/voc_2007_trainval/vgg16_faster_rcnn_iter_170000.caffemodel"</span>;</span><br><span class="line">    <span class="keyword">int</span> GPUID=<span class="number">2</span>;</span><br><span class="line">    Caffe::SetDevice(GPUID);</span><br><span class="line">    Caffe::set_mode(Caffe::CPU);</span><br><span class="line">    Detector det = Detector(model_file, weights_file);</span><br><span class="line">    det.Detect(<span class="string">"/home/ouyang/Program/py-faster-rcnn/data/demo/90.jpg"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>里面的pt文件和model文件的路径替换成对应的就好。然后是main.cpp对应的CMakeLists.txt文件。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#This part is used for compile faster_rcnn_demo.cpp</span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span> (VERSION <span class="number">2.8</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">project</span> (main_demo)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(main main.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">include_directories</span> ( <span class="string">"$&#123;PROJECT_SOURCE_DIR&#125;/../caffe-fast-rcnn/include"</span></span><br><span class="line">    <span class="string">"$&#123;PROJECT_SOURCE_DIR&#125;/../lib/nms"</span> </span><br><span class="line">    <span class="string">"$&#123;PROJECT_SOURCE_DIR&#125;/lib"</span> </span><br><span class="line">    /usr/local/<span class="keyword">include</span> </span><br><span class="line">    /usr/<span class="keyword">include</span>/python2.<span class="number">7</span></span><br><span class="line">    /usr/local/cuda/<span class="keyword">include</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(main /home/ouyang/Program/py-faster-rcnn/Cplusplus2/lib/libfaster_rcnn.so</span><br><span class="line">    /home/ouyang/Program/py-faster-rcnn/caffe-fast-rcnn/build/lib/libcaffe.so</span><br><span class="line">    /home/ouyang/Program/py-faster-rcnn/lib/nms/libgpu_nms.so </span><br><span class="line">    /usr/local/lib/libopencv_highgui.so </span><br><span class="line">    /usr/local/lib/libopencv_core.so  </span><br><span class="line">    /usr/local/lib/libopencv_imgproc.so </span><br><span class="line">    /usr/local/lib/libopencv_imgcodecs.so</span><br><span class="line">    /usr/lib/x86_64-linux-gnu/libglog.so</span><br><span class="line">    /usr/lib/x86_64-linux-gnu/libboost_system.so</span><br><span class="line">    /usr/lib/x86_64-linux-gnu/libboost_python.so</span><br><span class="line">    /usr/lib/x86_64-linux-gnu/libglog.so</span><br><span class="line">    /usr/lib/x86_64-linux-gnu/libpython2.<span class="number">7</span>.so</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>相同的执行<code>cmake.</code>和<code>make</code>完成该部分的编译。</p><h2 id="封装2"><a href="#封装2" class="headerlink" title="封装2"></a>封装2</h2><p>上面<code>C++</code>封装的Faster R-CNN结构简单，代码量也少，部署起来很方便。但在项目中也有着致命的弱点，检测速度大概比python版的Faster R-CNN慢了一个数量级。因此很有必要寻求另外的封装方式，这里参考<a href="https://github.com/D-X-Y/caffe-faster-rcnn" target="_blank" rel="noopener">D-X-Y</a>纯<code>C++</code>版的Faster R-CNN代码，该代码将所有代码都改写为了C++版本，不像上一部分的代码中调用了很多python的库。<br>D-X-Y所封装的代码中对应原python版本中demo.py的接口在<code>G:\gitProgram\caffe-faster-rcnn\src\api\FRCNN\frcnn_api.cpp</code>和<code>G:\gitProgram\caffe-faster-rcnn\include\api\FRCNN\frcnn_api.hpp</code>。这里我将所需要的文件整合到一起，目录结构为：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">└── lib</span><br><span class="line">│   │── frcnn_api.cpp</span><br><span class="line">│   │── frcnn_api.hpp</span><br><span class="line">│   │── CMakeLists.txt</span><br><span class="line">│—— CMakeLists.txt</span><br><span class="line">│—— main.cpp</span><br><span class="line">└── include</span><br><span class="line">│   │── api</span><br><span class="line">│   │    api.hpp</span><br><span class="line">│   │   └── FRCNN</span><br><span class="line">│   │   │   │── frcnn_api.hpp</span><br><span class="line">│   │   │   │── rpn_api.hpp</span><br><span class="line">│   │── caffe</span><br><span class="line">│   │   │──proto</span><br><span class="line">│—— libcaffe<span class="selector-class">.so</span>.<span class="number">1.0</span>.<span class="number">0</span></span><br><span class="line">│—— libcaffe.so</span><br><span class="line">│—— test.prototxt</span><br></pre></td></tr></table></figure></p><p>上述代码结构中的<code>include</code>来自D-X-Y代码中的<code>include</code>,<code>/include/caffe/proto</code>来自D-X-Y代码中编译后产生，对应的路径为<code>./caffe-faster-rcnn/.build_release/src/caffe/proto</code>。<br><strong>第二种方式就不贴对应的代码了，在这里我也将两种方式对应的代码放在github上，供大家参考</strong><a href="https://github.com/oysz2016/cpp_faster-rcnn" target="_blank" rel="noopener">代码</a></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;
    
    </summary>
    
      <category term="深度学习笔记" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="代码部署" scheme="https://oysz2016.github.io/tags/%E4%BB%A3%E7%A0%81%E9%83%A8%E7%BD%B2/"/>
    
  </entry>
  
  <entry>
    <title>写博客的第100天&amp;北漂的第30天</title>
    <link href="https://oysz2016.github.io/post/6341d9b.html"/>
    <id>https://oysz2016.github.io/post/6341d9b.html</id>
    <published>2018-08-13T15:32:35.365Z</published>
    <updated>2019-03-17T05:58:43.531Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p class="description"></p><a id="more"></a><h2 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h2><p>&emsp;&emsp;从开始写博客到今天也差不多100天了，100天的时间写了15篇博文，大概一个星期写一篇，不敢说高产，但最起码坚持下来了。回顾这15篇博文，每篇博文从动笔到完成少则1个小时，多则要花大半天收集整理资料，事后还要自己读几遍，以修改文中的漏洞。在这100天里，有时我也在问自己，维护这个网站和对应的公众号，以及花费时间去写博客的价值是什么。其实这个问题在我决定开始搭建属于自己博客时也有思考，并在第一篇博客文对自己这样做的原因做了简短的回答。在生活节奏如此快的今天，自我管理，时间管理变得越来越重要，也应运而生了各种“21天计划”和”100天行动”等活动用来培养一个良好的习惯。我也相信通过21天或者100天的重复一件事情，可以形成对应的习惯。因此，如今到了开始写博客的第100天，我理应比第1天写博客时有了更深的感触。<br>我一直相信，从一个人的一个优点中，可以看出这个人具备的其他优点，缺点也是一样。我想这也是古人说“3岁看大，7岁看老”的原因，虽然年纪越大，可塑性越差，但养成一个对自己有益的习惯总是没差的。潜移默化中，写博客这个习惯为我带来了很多收获，我想这就是花心思做这件事的价值。</p><ul><li><p><strong>写博客让我把问题想的更明白</strong>：在看到《孟子》中写道“尽信书，不如无书”前，我一直以为书上写的东西是完全正确。总觉得能把自己的思想和见识以书本为载体，复制出无数多份发行出去，是一件很酷的事情，一本书的作者提出的观点一定是经过深思熟虑，遣词造句都尽量贴合他的想法。但在自媒体如此发达的今天，也看到了不少漏洞百出，毫无逻辑的书或文章，会有这样的文章出现无非是作者水平不够，或者作者只求强行牵强的证明他提出的观点有些许正确而有意而为之。《如何阅读一本书》中说读文章就好像与作者交流，当作者对一件事情比你有更深刻见解时，读者才会觉得有所收获。因此在我自己写博文时，为了尽量能把一个想法弄的更明白，常常会一遍遍的查找资料，以免闹出笑话，或让自己和别人看了感觉毫无营养。这无疑对自己是一种鞭策，当脑海中的一个想法变成了字符呈现在自己眼前时，也像是与自己交流，写出的东西首先要过自己这关，没有什么错误才能继续往下写。</p></li><li><p><strong>让我更会表达自己的想法</strong>：读研期间和导师聊天时导师常和我说出了学校怎么让领导和周边人发现你的价值，实际上程序员这个职业，经过一个不错的正规学校培养，大家的编程能力都差不多，这个时候需要比别人更会表达自己的想法，表达能力有时比编码能力更重要，往往比别人优秀一点，出众的就会是你。我深以为然，但培养自己的表达能力不是一朝一夕的事情。翻看自己的这十多篇博文，很高兴从这些文章中我发现自己对于一个事情的表达，能说的更明白。工作的这一个月我也发现，在工作问题上，自己比想象中更能表达自己的想法，我想这多少也与写博文的积累有关。</p></li><li><p><strong>好记性不如烂笔头</strong>：在关键时刻能想起一个知识点极其重要，对于做技术而言，能随时随地使用的技能构成了技术人的基本价值，看了就忘像是在做无用功。学习的过程更重要的是构建知识体系，面对问题，脑海中需要有着几种可行的解决方案，这就需要大量的积累与记忆。常看常新，有时极力回想某篇论文中某个新颖的观点时，却怎么也想不出，这时翻看对应的博文又能重新的回顾当时看这篇论文时的思路。在一遍遍的翻看中，总能发现当时思考的不足，也更能加深对某个点的记忆</p></li><li><p><strong>为了收获志同道合的朋友</strong>：回想起本科参加电赛和飞思卡尔智能车比赛时，常常混迹于各个电子论坛，为了请教别人或者偶尔被人请教，加了很多好友。印象很深刻的是当时和一个南京农业大学的学长经常会交流到很晚，没有他的帮助，也许也很难在比赛中取得之后的成绩。比赛结束后，两人一起建了个群，为之后两个学校参加比赛的学弟学妹提供帮助，这是我大学中一件很有意义的事情。其实每个做技术的大牛都是从小白一路走过来，都明白在成长路上遇到困惑时求助他人会得到更快的成长，只要不是伸手党，在提问前对一个问题有切实的思考并尝试解决过，“大佬”都会乐于相助。写博客时书写的东西被人点赞关注，是一件值得开心的事情，每一次的点赞与关注都是一种激励。也希望通过这种方式结识各种牛人。</p></li></ul><h2 id="北漂"><a href="#北漂" class="headerlink" title="北漂"></a>北漂</h2><p>&emsp;&emsp;万万没想到自己会加入“北漂一族”,毕业时选择了相对难走的路,当然是希望自己能成长的更快。来北京的这一个月，磨练最多的就是工作能力，基本每天的工作时间都在12小时以上，有冒着大雨回家，也有晚上12点半还在等快车，深夜1，2点昏昏沉沉的写一天的工作总结。晚上10点后的北京是属于滴滴司机和程序员的，滴滴拼车时能碰到很多其他公司的程序员。每天下楼看见周边的其他楼都灯火通明时，有被震撼到。看见整个环境中的人都在为梦想拼搏，也多少更能激起自己的斗志。在这段时间的工作中也算是参与到了几个项目中，python脚本，C++，opencv，caffe，tensorflow在这段时间都有了更深的理解。无奈分身乏术，总觉得这些技能自己多少都会点，但也只是略懂皮毛，在接下来的时间里要多看论文，精进各种技术。<br>&emsp;&emsp;除了工作外，最重要的当然是生活了。这段时间学会了很多生活技能，修过空调，热水器，也开始自己做饭。内心也比想象中更加强大了，还记得来时的第二个周末，周五的晚上发烧，10点才到家，第二天还要加班，猛灌了两杯热水，第二天醒时也能跟没事一样的去上班。希望自己能如来时想的一样，在各方面都能有更快的成长。<br>&emsp;&emsp;北京有45%的非北京户籍人口，作为一个离梦想最近的城市，北京似乎为每个外来者都提供了希望的种子。这个种子无论怎样成长，在北京待上一段时间都会有属于自己的故事.。在北京待的这一段时间，见到了形形色色的人，有地痞无赖般的“中介”，扬言不给15元的辛苦费就让手机号报废，也有十分热切的提醒我东西掉了的陌生人，还有拿着微薄的工资依然每天斗志昂扬为了梦想奋斗的热血青年。也遇到了很多看起来一样的人，早上都蜂拥而至各个写字楼，晚上都拖着疲惫的身体挤着公交地铁，回到暂时属于自己的那一间屋子。我相信每一个能坚持下来的“北漂”人心中都有着深深的执念，有着对现状的不甘和对美好生活的向往。向那些为梦想为生活长期在这个城市奋斗的人致敬。<br>&emsp;&emsp;而对于我而言，希望以后回想起来北京这个决定，不说后不后悔，只讲在北京有多少收获。</p><p>&emsp;&emsp;这篇博客从两周前开始动笔，但无奈最近实在太忙，直到今天才算写完。七夕的晚上，把音乐开到最大声，自嗨的整理完这篇博客也是很无奈。唯一值得慰藉的是工作上提前一周完美完成任务，在周会上被导师和主管说很牛。愿接下来的日子，自己能为自己和别人带来更多惊喜吧。<br>&emsp;&emsp;晚安！2018年8月17日。</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p class=&quot;description&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="https://oysz2016.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="https://oysz2016.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>二维图像分割之分式分割</title>
    <link href="https://oysz2016.github.io/post/7f9c7afb.html"/>
    <id>https://oysz2016.github.io/post/7f9c7afb.html</id>
    <published>2018-07-27T16:42:46.318Z</published>
    <updated>2019-03-17T05:48:57.690Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p class="description"></p><a id="more"></a><p>&emsp;&emsp;从上周开始研究各种数学式子的切割，包括分式，竖式和脱式。本着由易到难的原则，开始做分式切割的调研。除了做PPT，写文档外，也将部分调研的结果整理成博文。</p><h2 id="图像数学公式定位的关键问题"><a href="#图像数学公式定位的关键问题" class="headerlink" title="图像数学公式定位的关键问题"></a>图像数学公式定位的关键问题</h2><p>&emsp;&emsp;一般地，公式定位流程如下：</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215085.png" alt="数学公式定位流程"></p><ul><li>预处理：包括灰度化、二值化、去除噪声和倾斜矫正等步骤</li><li>统计版面参数：包括字符的位置、字符的尺寸和相邻字符间的间隔等</li><li>版面分析：标注出字符域、图像域、表格域和图形域等</li><li>行提取：对字符域以行为单位进行划分</li><li>定位孤立数学公式：从提取的行中区分孤立数学公式行</li><li>定位内嵌公式：从非孤立数学公式行中区分出含数学公式的文本行</li></ul><h2 id="公式定位基本算法"><a href="#公式定位基本算法" class="headerlink" title="公式定位基本算法"></a>公式定位基本算法</h2><ol><li>预处理：对图像灰度化、二值化、倾斜校正、去噪等处理，使其更有利于数学公式定位</li><li><p>数学公式字符块提取<br> 1.行提取</p><ul><li>进行联通区域搜索，得到图片中所有连通区域</li><li>合并具有相交或包含关系的连通区域</li><li>根据连通区域宽高度统计直方图得到版面的字符宽、高度阈值<code>threshold_w</code>和<code>threshold_h</code>，据此去除图片中的无关信息，得到公式的候选区域</li><li>根据候选字符的连通区域的位置关系，合并候选字符连通区域，提取本文行及相应参数：行间距<code>Line_d</code>，行内相邻连通区域水平间隔平均值<code>threshold_d</code></li></ul><p>2.行内字符块提取:行内字符块提取是根据<code>threshold_d</code>将文本中的字符连通区域合并成字符块<br>3.后处理：数学公式字符块合并。对于分式，若将分数线与分子分母分别切割，可以根据其上下相邻行为单字符或行内字符均处于具有二维运算结构的运算符作用范围内，且两者之间的垂直距离小于行距<code>Line_d</code>的特点将其合并。</p></li></ol><p>&emsp;&emsp;图像倾斜的影响：当倾斜角增加时，数学公式定位准确率急剧下降。这是由于在数学公式定位算法采用连通区域空间位置关系特征提取文本行，很容易受到倾斜影响，进而影响公式定位准确性，因此<strong>首先需要对拍照图片进行测斜和校正处理</strong>，以确保公式定位算法的鲁棒性。</p><h2 id="公式定位错误的校正方法"><a href="#公式定位错误的校正方法" class="headerlink" title="公式定位错误的校正方法"></a>公式定位错误的校正方法</h2><ul><li>考虑根据图片中印刷体字符的大小，近似判断拍照的远近，从而确定各个阈值的大小</li><li>影响最大的参数有行内相邻连通区域的水平间隔<code>Character_dist</code>,可以考虑将行内连通区域按从左到右排列，然后计算版面中相邻连通区域的间隔，取数量最多的连通区域间隔座位阈值<code>Character_dist</code>。对与行间距阈值<code>Line_d</code>也可以用类似方法</li></ul><h2 id="分式分割的难点"><a href="#分式分割的难点" class="headerlink" title="分式分割的难点"></a>分式分割的难点</h2><h3 id="印刷体"><a href="#印刷体" class="headerlink" title="印刷体"></a>印刷体</h3><p>&emsp;&emsp;对于印刷体分式，如下图，如同印刷体文本的字符分割算法一样。采取水平投影的方式即可找到分割线。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1532749752156.jpg" alt="enter description here"></p><h3 id="手写体"><a href="#手写体" class="headerlink" title="手写体"></a>手写体</h3><p>&emsp;&emsp;分式分割的难点主要集中再手写体分式上。手写体的格式没有印刷体那么规范，分子与分母经常会出现粘合，带分式中也会出现整数与分数线、分数粘合在一起的情况，因此无法直接通过投影拆分分式。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215086.png" alt="enter description here"></p><h2 id="分式分割的处理思路"><a href="#分式分割的处理思路" class="headerlink" title="分式分割的处理思路"></a>分式分割的处理思路</h2><p>&emsp;&emsp;分式分割的关键点是找到分数线，分数线能作为带分数中整数与分数分割的参考，且能作为分数部分的分割线。因此分数线具有重要的参考价值。</p><p>&emsp;&emsp;考虑到手写体中分数线扭曲，各部分粘合的情况调研了许多直线检测的算法。有时间会做系统的整理。在这里线整理几种常用的直线检测算法。</p><h3 id="霍夫变换"><a href="#霍夫变换" class="headerlink" title="霍夫变换"></a>霍夫变换</h3><p>&emsp;&emsp;Hough是最经典也是应用最广泛的直线检测算法，hough使用极坐标的方式表示直线。极坐标下，直线的表达式可定义为：$\left(-\frac{cos\theta}{sin\theta}\right)x+\left(\frac{r}{sin\theta}\right)$，化简可得到$r=xcos\theta+ysin\theta$。对于每一点$(x_0,y_0)$，可以将通过该点的直线定义为$r_\theta=x_0cos\theta+y_0sin\theta$。</p><p>&emsp;&emsp;通过以上的推导，意味着每一对极坐标的参数$(r_\theta, \theta)$代表着一条通过$(x_0,y_0)$的直线。对于每个定点$(x_0,y_0)$，画出通过该点的所有直线并以极坐标表示，会得到一条正弦曲线。因此越多的点具有所描绘的正弦曲线相交，意为着这些点能组成平面内的一条直线。<br>&emsp;&emsp;如下图所示，点$x_1=9$,$y_1=4$，点$x_1=12$，$y_1=$以及$x_1=8$,$y_1=6$所描绘的通过它们的所有直线的正弦曲线。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215087.png" alt="enter description here"><br>hough的做法是追踪图像中每个点对应曲线间的交点，如果这些交点的数量超过了一点的阈值，即认为交点的参数$(r_\theta, \theta)$在原图像中为一条直线。</p><p>&emsp;&emsp;Hough变换的基本思想是利用图像的全局特征将特定形状的边缘连接起来，Hough通过点线的对偶性，将原图像中的点映射到用于累加的参数空间，将在原图像中寻找特定曲线的检测转化为寻找参数空间中的峰值问题。Hough的优点和缺点都来源于全局特征，因为全局特征，Hough提取的曲线受噪声和边界的影响较小，具有较好的鲁棒性，但也会带来效率低的缺点</p><h3 id="LSD算法"><a href="#LSD算法" class="headerlink" title="LSD算法"></a>LSD算法</h3><p>&emsp;&emsp;LSD发表于2012年，算是较新的直线检测算法。与Hough利用全局特征不同，LSD是一种局部提取直线的算法，在线性时间(liner-time)内能得到亚像素级准确度的直线。<br>&emsp;&emsp;LSD算法的流程如下图，LSD算法的核心思想是合并像素生成直线，合并的规则是根据每个像素点的梯度值建立状态列表，并将所有点设置为NOT USED。然后去除列表中梯度最大的点作为直线的第一个点，并将对应的状态设置为USED。再基于区域生长算法，得到line support region。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215203.png" alt="enter description here"></p><p>&emsp;&emsp;上面说了Hough由于全局的属性带来的优缺点，下面也说说LSD这种算法因为局部提取直线带来的缺点。<br>LSD号称是一种无需设置任何参数的算法，但在实际使用中，需要设置采样率，并且区域生长算法中，需要设置梯度角度变化的容忍（tolerance ）值。</p><ul><li>由于LSD算法的每个点都有状态值“NOT USED”和“USED”。因此每个点都智能属于一条直线，遇到相交直线时会出现至少一条直线被分割成多条直线的情况。</li><li>LSD算法在找寻line support region时，用了区域生长算法的思想。会由于线段间的遮挡和局部模糊导致一条直线被割裂成多条。</li></ul><p>&emsp;&emsp;由于是做手写分式分割中的分数线检测，直线往往弯曲而且易被其他线“切断”。因此做实验时，尝试将采样率与区域生长的容忍值调大了不少，让我困惑的是，在我将采样率和容忍值调大后，LSD算法的耗时呈大幅增长，失去了LSD算法的效率优势，这里暂时还不知道是我自己写算法的问题，还是LSD在增大局部搜索范围后效率会显著下降的原因。日后有了确切的结论会来更新，也欢迎大家指正。</p><p>&emsp;&emsp;下面贴几张LSD算法在分数检测的效果图，不同颜色的线段代表检测出的不同直线。可以看到对于扭曲小且没相交直线的情况下，LSD有较好的效果，而一旦干扰多了，一条直线会被分割成多条直线。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215201.png" alt="enter description here"></p><p>&emsp;&emsp;关于直线的论文有很多，还有使用图像分割或提取边缘算法得到图像边缘后，使用动态聚类算法聚合线段，再用直线拟合同一聚类中图像。</p><h2 id="实验与结论"><a href="#实验与结论" class="headerlink" title="实验与结论"></a>实验与结论</h2><p>&emsp;&emsp;不得不说，手写分式可能出现各种各样的情况。例如无法有效判断检测到的那条直线为分数线，对于带分式中整数与分子分母贴合紧密的情况，也暂时没有有效的解决思路。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215088.png" alt="enter description here"></p><p>&emsp;&emsp;最近看了很多图像处理的论文以及思路，很多棘手的任务，论文作者都巧妙的用图像处理的算法解决了。相信关于分式分割的处理，也会有相应的解决办法。事情总是要一步一个脚印的处理。先贴一个图，下图是这周做的真分式和假分式的切割算法，在525张手写分式图片中，正确分割了512张，也算是达到了预期的效果。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/小书匠/数学公式识别之分式分割1532748215385.png" alt="enter description here"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 毛星云. OpenCV3编程入门[M]. 电子工业出版社, 2015.</p><p>[2] Gioi R G V, Jakubowicz J, Morel J M, et al. LSD: A line segment detector[J]. Image Processing on Line, 2012, 2(4):35-55.</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p class=&quot;description&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="图像处理" scheme="https://oysz2016.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="图像处理" scheme="https://oysz2016.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="算法" scheme="https://oysz2016.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>《我不是药神》:今后会越来越好的</title>
    <link href="https://oysz2016.github.io/post/172fcfc5.html"/>
    <id>https://oysz2016.github.io/post/172fcfc5.html</id>
    <published>2018-07-10T15:34:07.333Z</published>
    <updated>2019-01-05T06:57:14.467Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p class="description"></p><a id="more"></a><p>&emsp;&emsp;最近刷朋友圈，以及和同事聊天总是听到谈论《我不是药神》。本来近期没看电影的打算，趁着周末的闲暇和好奇心驱使，去影院看了这部现象级电影。<br>&emsp;&emsp;电影的剧照是几个主演坐在一家“王子印度神油”的店前开怀大笑，一度以为是部恶搞印度神油的喜剧电影。但事实上，这部电影很多场景都很煽情，映像中好像没哪部电影看的让我感觉眼眶有点湿。说几个我记得的点</p><ul><li>吕受益随身带着橘子，这是因为橘子富含维C，据说有抗癌的作用，而且便宜不用削皮，但橘子除了少数成熟的月份以外都很酸，可想活命也只能如此。</li><li>从散伙饭，到吕受益去世才短短一年的时间，乐观向上的一个人就这样被疾病迅速摧毁，让人心疼。</li><li>吕受益死后的房门外，黄毛吃的干巴巴的橘子，应该是吕受益生前给他的。</li><li>一帮病友被带到警局，要求供出药贩子信息时，一位老奶奶苦苦哀求警察曹斌不要追究药贩子的对话。</li><li>最感人的是程勇整个人的升华，从最初唯利是图的商人模样，到最后不赚钱卖药，送走儿子后，一心打算卖药到被抓为止的心态转变。</li></ul><p>&emsp;&emsp;电影中的很多细节做的也无比用心，片中的人民币有新版的红色百元，也有符合那个年代的老版蓝白色百元。程勇平时都抽纸烟，唯独曹斌去的时候抽起了雪茄，口口声声说自己每月赚几十万，但应该赚不到这么多。黄毛不会开车，为了让程勇不被抓，起步时摇摇晃晃开着车去冲警察的追捕线。吕受益去世前后，房子都不一样了，在说明治病花去了很多钱。<br>&emsp;&emsp;不得不说这是一部很棒的电影，整个电影很沉重，但也适时的加入了一些包袱。电影中的瑞士公司高层无疑是一个负面形象，面对百姓抗议药价太高时，理直气壮的说定价合理合法，而印度公司生产的仿制药在药效几乎一样的情况下，其费用仅为瑞士药的1/80。看电影时也不由的觉得印度真是良心公司，而瑞士药的高昂价格无异于是将白血病人当成了勒索对象。但电影的冲突与矛盾是非常强烈的。有中国那时进口药价格高昂引起的社会矛盾，正版药与仿制药的矛盾，还有人情与法理的矛盾。<br>&emsp;&emsp;有一个学制药的朋友和我讲过研制一款药有多难，看了电影后也去查了下。</p><blockquote><p>根据 2016 年德勤会计事务所发布的一份研究报告，从其对 12 家大型药企的持续 6 年的追踪结果可看出，研发巨头的投资回报率从 2010 年的 10.1% 下降至 2016 年的 3.7%。与此同时，研发一个新药的平均成本已经从低于 12 亿美元增长至 15.4 亿美元，而且需要耗时 14 年才能推出一个新药。</p></blockquote><p>&emsp;&emsp;这个费用还没有算上审批通过后，药企研究剂量强度、配方和新的适应症的后续资金投入。制药企业的目的之一当然是为了盈利，在高昂的药价研制费用加上十多年的人力物力投入。而且药物的专利期一般是20年，药企只能在20年内把巨额的药物研发成本赚回来。而且对于罕见的病，由于患者少，企业为了追求利润，高额的定价似乎也是没办法的事。<br>除此之外，中国对于进口药必须进行临床试验，一款药临床试验的成本在300-700万美元。累加的成本当然由患者买单。<br>都知道香港的很多进口商品会便宜很多，一些药品也类似，2017年，在北大卫生经济论坛上，国家发改委价格司副巡视员郭剑英解释了为什么大陆80%的进口原研药价格会高于香港</p><blockquote><p>香港没有5%的关税和17%的增值税，没有15%的医院加价，流转费用也不会达到20%多</p></blockquote><p>&emsp;&emsp;电影的最后字幕，向观众介绍了今年4月25日，中国取消了28种进口药的关税。但这部分只占5%。<br>除了以上的层层加价外，在中国，过了20年专利保护期的原研药仍然拥有自主定价权。不需要遵循政府的指导定价。</p><blockquote><p>在药品的使用上，大部分二级以上医疗机构执行了《处方管理规定》中的“一品两规”，也就是说，同一种药物，只使用两个厂家的产品。在实际操作中，往往就会变成一个国产品种和一个进口品种。国内的药厂有6000多个，竞争激烈，而进口药物缺乏竞争，等于保证了自己的销路。</p></blockquote><p>&emsp;&emsp;这部电影的总体口碑很好，评价负面的人多是说妖魔化药企，看这部电影时，有让我想起韩国的一些反映社会问题的电影，但毕竟不像韩国电影什么内容都能拍，在国内的审查制度下，这样一部电影能即叫好又叫座而且能唤起民众的反思已实属不易。最后关于医疗，愿如电影中所说“今后会越来越好的”。</p>]]></content>
    
    <summary type="html">
    
      &lt;p class=&quot;description&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="影评" scheme="https://oysz2016.github.io/categories/%E5%BD%B1%E8%AF%84/"/>
    
    
      <category term="随笔" scheme="https://oysz2016.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
      <category term="影评" scheme="https://oysz2016.github.io/tags/%E5%BD%B1%E8%AF%84/"/>
    
  </entry>
  
  <entry>
    <title>毕业</title>
    <link href="https://oysz2016.github.io/post/9b1b667b.html"/>
    <id>https://oysz2016.github.io/post/9b1b667b.html</id>
    <published>2018-06-25T03:55:45.624Z</published>
    <updated>2019-03-17T05:48:45.797Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p class="description" align="center">从最初面红，到现在双眼通红</p><a id="more"></a><p>&emsp;&emsp;从研究生入学到毕业前，我一直觉得读研期间，没有本科那样像“学生”，在我的潜意识里，“学生”时代有上不完的课，考不完的试，关于学习的讨论也是在课堂和题目上。而读研期间，半学期紧张的课程和考试结束后，就再也没有了必须要去上的课和必须要完成的考试。即使我早就知道研究生要培养自己的自主学习能力，我还是会去怀念那时候的“学生”时代。可直到今天，往常的收发快递点门前待寄包裹堆积如山；互相询问离校时间，筹划实验室和寝室的散伙饭；导师对我常说的话由“抓紧时间”变成了祝福的话语。拿到学位证的欣喜与即将离开学校的失落交杂其中。我才知道，这两年我所经历的才是最值得缅怀的。<br>&emsp;&emsp;研究生的两年出现了许多足以影响我一生的人和事。</p><h2 id="我的导师"><a href="#我的导师" class="headerlink" title="我的导师"></a>我的导师</h2><p>&emsp;&emsp;读研期间最该感谢的人就是我的导师钟珞教授。入学前就不断有耳闻各个学校研究生导师的负面新闻，读过研的哥哥也告诉我，他所知道的导师压榨学生的事情。本想选个年轻导师，跟着一起多做项目的我却偏偏选了学院最有资历的老院长作为导师。<br>从入学起就能感觉到钟老师对我的关照，研一时虽然每次都只是周一开例会才会见面，但开会时，聚餐时都能感觉到钟老师对我的关照。从研一到研二有个转变，开始变得喜欢开会，也更多的找导师讨论大论文和小论文的事。整理寝室时才发现，上一篇小论文打印出来的修改稿有6份。研二开始更觉得导师像是朋友，讨论论文之余，也会聊聊家事和导师读研读博时期的一些趣事。相比较于学业上的帮助，更多的收获是关于做人做事的一些道理，钟老师用自己的亲身经历对我的指导让我受益匪浅。</p><h2 id="关于读博"><a href="#关于读博" class="headerlink" title="关于读博"></a>关于读博</h2><p>&emsp;&emsp;虽然没有读博，但这也算是一个会改变一生的决定。导师在研二下时和我说过几次读博士的事情，与企事业工作的优缺点做对比。心里一方面很感激导师对我的认同。可能很早给自己定的目标就是最多读到硕士，便没有读博士的打算。而且自认为自己不能静下心来4，5年坐在实验室里写论文 。自作主张的我没有和家里商量读博士的事，快毕业说起这事才知道父母很赞同我继续读下去。想着商量了或者早点去考虑这事会变得不一样，不得不说心里会有点失落。以后如果心态不一样了，会去考虑这个事。</p><h2 id="实习与工作"><a href="#实习与工作" class="headerlink" title="实习与工作"></a>实习与工作</h2><p>&emsp;&emsp;研一下学期找实习之前，只是抱着试一试的心态投了华为一家公司，后续也没有做其他公司的笔试面试。很幸运的通过了华为的笔面试，怀着对大公司的向往和导师商量了去实习的事情。去了后感到遗憾的是岗位不是我喜欢的，也错过了不少公司的提前批招聘，尽管华为在最后录用阶段做的很让人糟心。但去华为那段时间认识了不少好朋友，在深圳也度过了整个研究生期间最开心的时光，现在想起来，是我研究生期间最大的遗憾了，后来做选择时，竟有点没勇气再去面对这座城市，我还是很感激17年的那个夏天。<br>&emsp;&emsp;实习结束回来找工作，我心里想的是深圳或者广州的大公司，心里面是抵触去北京的。投的一些北京的公司，是想着多涨些笔面试经验。可最后一连串的事情，我也说不清楚怎么会最后签约北京的公司。我始终觉得做每一个决定都是复杂的，只言片语或者我刻意去想都多少会觉得有点不合适。但正是这一个个因素，最后促使了我这个决定。想起某个多少会有遗憾的决定时，我会经常问自己“后悔吗？”，我的回答一直是“多少有点”。每一个会让我感到遗憾的决定，我都会再三犹豫与挣扎，可我真的尽力了。我只能安慰自己，再来一次，我的“决定”还会是这样。</p><h2 id="关于多肉"><a href="#关于多肉" class="headerlink" title="关于多肉"></a>关于多肉</h2><p>&emsp;&emsp;我想我对多肉有一种特殊的感情，在学校折腾了两天。回到家的第一件事情就是为“观音莲”安置新家。我很抱歉，因为我怕麻烦，说了两年要为你安置大一点的花盆，直到我快要走了，才做到。希望你茁壮成长，无忧无虑。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1530451274313.jpg" alt="enter description here"></p><hr><p>&emsp;&emsp;毕业前总是感慨万千，再过两天就要上班了，祝自己一切顺利。感恩这两年所有的人与事。</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p class=&quot;description&quot; align=&quot;center&quot;&gt;从最初面红，到现在双眼通红&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="https://oysz2016.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="https://oysz2016.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>极简人类史</title>
    <link href="https://oysz2016.github.io/post/ad14e7f5.html"/>
    <id>https://oysz2016.github.io/post/ad14e7f5.html</id>
    <published>2018-06-05T03:12:09.146Z</published>
    <updated>2019-03-17T05:49:07.832Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p class="description"></p><a id="more"></a><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>最近一段时间开始对历史感兴趣起来，看历史的时候能获得一些力量和安慰，一段历史的厚重感和其中的伟人能让人肃然起敬，现在面临的一些不顺与一段历史中被命运捉弄的人来说也变得不值一提。趁着即将工作的间隙，追完了一部电视剧《大明王朝1566》，看完了《极简人类史》和另一本还在看的《人类简史》。<br>《大明王朝1566》这部剧给我的感受是厚重且华丽，是我看过的最棒的国产剧了。剧中大量运用了黑白 闪回，与前一刻的剧情形成反差，或重复加强的效果，有不少闪回表现的人物内心活动。有一篇关于该剧的评论<a href="https://movie.douban.com/review/1296072/" target="_blank" rel="noopener">《大明王朝1566》：太极·利剑·雪</a>，太极那部分分析的很好。<br>《极简人类史》以时间为轴线，主要介绍了人类发展历史中的三个阶段，脉络清晰，书中没有细讲一个国家或一位伟人，但描述了人类历史一步步的发展轨迹，讲述的是一部简洁易懂的人类历史。</p><h2 id="前传"><a href="#前传" class="headerlink" title="前传"></a>前传</h2><p>宇宙史是比人类历史更大的一个范畴。这一部分书中也只是简短的介绍，关于这部分看书中的描述只能有个大致的时间概念和各阶段发生的事情。有许多制作精良的纪录片绝对能带来比看书更好的体验。<br>这里放一个简短的科普视频。</p><iframe id="video" height="415" width="544" align="MIDDLE" src="//player.bilibili.com/player.html?aid=9340850&cid=15435812&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="false" type="application/x-shockwave-flash" pluginspage="//www.adobe.com/shockwave/download/download.cgi?P1_Prod_Version=ShockwaveFlash"> </iframe><h2 id="采集狩猎时代"><a href="#采集狩猎时代" class="headerlink" title="采集狩猎时代"></a>采集狩猎时代</h2><p>首先看看采集狩猎的定义：</p><blockquote><p>采集狩猎时代是人类历史中这样一个时代：整个人类社会依靠采集或狩猎，而不是通过种植或制造，来获取食物和其他必需品。此时的人类被称为“采集狩猎者”。这个时代也被称作“旧石器时代”。采集狩猎时代是人类历史上的第一个时代，也是迄今为止最长的时代，这是为人类历史奠定基础的时代。<br>采集狩猎者始于25万年前，独特的文化和技术创新，将他们的生活方式与其他非人类物种区分开来</p></blockquote><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1525925266504.jpg" alt="enter description here"></p><p>采集狩猎时代距今很遥远，研究那个时代的学者采用三种截然不同的证据：</p><ol><li>远古社会留下的物质遗迹。如石器、制作品或者事物残渣。如对牙齿的细致研究，可以获知早期人类日常的饮食信息；男女之间骨骼的大小，可以反映两性关系；研究海床和数万年前形成的冰盖中提取的划分和果核样本，考古学家能重构当时的气候和环境变化模型。</li><li>研究现代采集狩猎部落。</li><li>基于现代基因差异进行对比研究。基因研究可以测定现代族群之间的基因差异程度，帮助预估自己族群的历史。</li></ol><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1525925761140.jpg" alt="enter description here"></p><h3 id="采集狩猎的生活方式"><a href="#采集狩猎的生活方式" class="headerlink" title="采集狩猎的生活方式"></a>采集狩猎的生活方式</h3><ul><li>生产力水平低下的，那时人类每天从自然环境中获取的热量很难超过3000卡路里，而这是一个成年人类维持基本生存所必须的能量。</li><li>人口密度低，平均每平方公里不足1人。我查阅了资料，中国人口平均密度目前是每平方公里130人，但人口分布及不平衡。东部沿海地区，每平方公里超过400人，中部地区每平方公里200多人，而西部高原地区，每平方公里不足10人。香港旺角是世界人口最密集的地方，每平方公里有13万多人！！！（真正意义上的寸土寸金）。<h4 id="亲缘关系"><a href="#亲缘关系" class="headerlink" title="亲缘关系"></a>亲缘关系</h4>狩猎时代几乎所有的人类部族都鼓励与外族通婚，能确保邻近族群之间的团结意识和语言之间的相互重叠。<h4 id="生活水平"><a href="#生活水平" class="headerlink" title="生活水平"></a>生活水平</h4>与如今的人们没有私有财产就是贫穷的标志不同，采集狩猎时代随时从周围环境获取生存所需的的物质，不积累财富。因此生活在温带地区的采集狩猎者生活水平相对较高，因为他们饮食多种多样，免受饥荒的困扰。<h4 id="生活闲适，但生命短暂"><a href="#生活闲适，但生命短暂" class="headerlink" title="生活闲适，但生命短暂"></a>生活闲适，但生命短暂</h4>采集狩猎者居住的小型社会使他们和流行疾病隔离开，频繁的迁移活动也避免了招致病害虫的垃圾堆积。但生活艰苦，平均寿命可能低于30岁左右（由于婴儿死亡率高，意外事故和人为暴力）。<br>最终，采集狩猎时代技术发展的足够高潮，使某些地区的一些部落能更加深入，集中的利用当地资源。标志着迈向农耕社会。<h3 id="采集狩猎时代的重大变革"><a href="#采集狩猎时代的重大变革" class="headerlink" title="采集狩猎时代的重大变革"></a>采集狩猎时代的重大变革</h3></li><li>技术变革：出现了新的石器<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1527953031918.jpg" alt="enter description here"></li><li>向非洲以外地区迁徙：向东向西迁徙，来到亚欧大陆更偏南、更温暖的地区。这里面有几个大事件，一个是距今5.5万年至4万年，人类出现在冰河时代的澳大利亚，这被视为技术创新的明显标志，因为抵达澳大利亚大陆需要高超复杂的航海技术。还有一个是距今3万年前出现在西伯利亚，在这里生存需要捕获大型哺乳动物（鹿、马和猛犸），表明掌握了高超的狩猎技术。</li><li>人类对环境的影响：许多大型动物灭绝，刀耕火种</li></ul><h2 id="人类历史的开端"><a href="#人类历史的开端" class="headerlink" title="人类历史的开端"></a>人类历史的开端</h2><p>关于人类历史开端的问题，一直有着争议。主要存在两种假说：</p><ul><li>多地起源模式，这种模式的证据来自对骨骼遗迹的对比研究。</li><li>走出非洲假说，主要依赖于现代人类的基因对比。</li></ul><p>作为一个没有查阅过人类起源的门外汉，如今听到的比较多的关于人类起源的说法是“走出非洲假说”，得益于基因检测技术的发展，这种假说的可信度越来越高。</p><h2 id="农耕时代"><a href="#农耕时代" class="headerlink" title="农耕时代"></a>农耕时代</h2><p>距今1.1万年至1万年前，农耕社会诞生了。直到近250年，工业革命的开始，农耕社会才走向消亡。虽然和长达25万年的狩猎时代相比，农耕时代才延续了1万年。但迄今为止，70%的人类成员都生活在农耕社会。<br>从生态学讲，农业能比采集狩猎更有效率地获取自然界通过光合作用存储的能量与资源。农业通过砍伐森林、使河流改道、开垦山坡和耕种土地，农业耕种者极大地改变了地球的面貌，使其变得更受人类活动控制。</p><h3 id="农耕时代的最早证据"><a href="#农耕时代的最早证据" class="headerlink" title="农耕时代的最早证据"></a>农耕时代的最早证据</h3><p>迄今为止，对农业诞生的源头仍缺乏令人信服的解释。有趣的是在直至公元前1500年，几个“世界区域”——非洲、亚欧大陆、美洲和澳大利亚记忆太平洋各岛屿完全没有联系，但却相继进入了农耕时代。正是大家在没有交流的情况下，相继进入了农耕时代，这推翻了农业是一项绝妙发明的观点。而且现代的狩猎者反对进入农耕时代，有种猜想是早期的农耕者并非心甘情愿的接受这种生活方式，而是被迫接受！</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528164538191.jpg" alt="enter description here"></p><h3 id="总体特点和长期趋势"><a href="#总体特点和长期趋势" class="headerlink" title="总体特点和长期趋势"></a>总体特点和长期趋势</h3><p>农耕时代具有超乎寻常的文化多样性，农耕部落之间共享着一些重要的特征，这些特征确保了农耕时代的延续。</p><ul><li>以村庄为基础的社会构成：都需要家庭内部和家庭之间的协同合作，都需要处理与外部族群之间的关系。</li><li>人口活力增强：世界人口由1万年前的600万增长到1750年现代社会初期的7.7亿</li><li>技术创新加速：本地人口压力、新环境的扩张和不断增长的思想和贸易交流促使农耕技术不断进步。</li><li>农副产品革命：纤维、奶和肥料</li><li>水利技术</li><li>流行性疾病：相比于采集狩猎时代没有流行疾病的优势，农业时代的定居且人口多、与牲畜的密切接触导致流行性疾病的产生。</li><li><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528165676516.jpg" alt="enter description here"></li><li>权力等级：为了控制不断增加的宝贵粮食库存，冲突时有发生，导致了新形势的社会不公，形成了新的权力体系。</li></ul><h3 id="城市出现之前的农业社会"><a href="#城市出现之前的农业社会" class="headerlink" title="城市出现之前的农业社会"></a>城市出现之前的农业社会</h3><p>这个时代已经有了农耕部落，但尚未出现大型城市和国家。在非洲和亚欧大陆地区，这一时期从约公元前8000年延续到公元前3000年。在美洲，这一时期开始的晚，持续时间也更长。太平洋和太平洋岛屿，这一时期延续至现代。</p><ul><li>村庄组成的世界</li><li>等级制度出现：由于部落的扩大，人们需要定义自己与邻里关系。司法、战事、贸易和宗教等都需要人管理。有着政治和经济制度。</li><li>早期妇女地位限制：女性通常没有机会承担专业化的角色，随着部落间竞争加剧，男性开始垄断暴力组织。</li></ul><h3 id="最早的城市和国家"><a href="#最早的城市和国家" class="headerlink" title="最早的城市和国家"></a>最早的城市和国家</h3><p>公元前3000年到公元前500年才是人类历史真正的开始时期。在非洲和亚欧低地区第一批城市和国家出现在公元前3000年左右，美洲出现在公元前1000年，而大洋洲在距今1000年左右，国家才出现在一些海岛（夏威夷和汤加）。国家出现的首要原因是不断增加的人口密度。</p><ul><li>农耕文明：随着国家模式的不断扩张，与其相关的制度和实践也固定下来，称为“农耕文明”</li><li>帝制国家：随着国家规模的扩大，独裁者掌控的众多城镇区域内形成了帝国体制。通过地方统治者直接或间接扩大了征税和管辖的区域。</li></ul><h3 id="农业、城市与帝国"><a href="#农业、城市与帝国" class="headerlink" title="农业、城市与帝国"></a>农业、城市与帝国</h3><p>在公元前500到1000年，随着世界各国人口增多、国家势力和数量的不断增长，交换网络的范围扩大。这期间诞生了很多王朝。</p><ul><li>非洲，亚欧大陆，最早的帝国是创建于公元前6世纪的波斯（现伊朗）王朝。该王朝掌控的区域达到其过往朝代最大疆域面积的5倍。在此后的 1500年里，类似规模的国家被称为帝国。</li><li>美洲，公元后第一个千年里，复杂的城邦体制与初创的帝国出现在中美洲。处于鼎盛时期的墨西哥特奥蒂瓦坎城，拥有超过10万人口，控制着跨越中美洲大部分地区的贸易网络。</li><li>农耕文明以外的地带，人口增长促使了新的阶层结构产生。在亚欧大陆人烟稀少的地区，匈奴人于公元前2世纪创立了帝国。</li></ul><h3 id="现代革命前夕的农业社会"><a href="#现代革命前夕的农业社会" class="headerlink" title="现代革命前夕的农业社会"></a>现代革命前夕的农业社会</h3><p>农耕时代的最后一个阶段，是1000年至1750年。该时期农耕文明传播到以往边缘化的区域，例如北美洲、非洲南部、中国西部地区。这一时期最为重要的变化就是世界主要地区在16世纪实现了统一。在此基础上，第一个全球交换网络诞生了。将数千年来从未来往的区域联系在一起，形成商业和知识协同，为现代社会的兴起发挥了至关重要的作用。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1529392312275.jpg" alt="enter description here"></p><h2 id="我们的世界——近现代"><a href="#我们的世界——近现代" class="headerlink" title="我们的世界——近现代"></a>我们的世界——近现代</h2><p>在人类史的三个时代中，近现代才维持了250年，确实最动荡不安的。近现代的主要特征如下:</p><ul><li>人口增长和生产力提高：1750年至2000年间，世界人口从7.7亿左右增长到近60亿，人均生产量也提高了9倍。</li><li>城镇扩展：在1500年，全球只有约50个城市剧名人口超过10万，到2000年，数千个城市的居民人口超过10万。<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1529392663798.jpg" alt="enter description here"></li><li>日益复杂和强大的政府：人口的增长及人们间的相互关系，需要更复杂的管理方式运作。</li><li>日益增大的贫富差距</li><li>女性享有更多机遇</li><li>前现代生活方式的消亡：采集狩猎和农业耕作的生活方式都走向没落。<h3 id="工业革命"><a href="#工业革命" class="headerlink" title="工业革命"></a>工业革命</h3>工业革命从 1750年到1914年。起源于苏格兰发明家詹姆斯·瓦特改良的蒸汽机以及第一列机车。工业革命的技术创新呈现波浪式发展态势，每一波都带来了新的生产力提升技术，并将工业化进程扩展到新的区域。工业革命带来了以下改变：</li><li>经济发展：从1820年至1913年，英国国内生产总值增长了6倍，德国增长了9倍，美国增长了41倍。与此同时，中国和印度等传统农耕社会受到了冲击，中国国内生产总值在世界的占比从33%下降至9%，印度从16%降至8%。</li><li>民主革命：<strong>经济基础决定上层建筑</strong>，产生了如法国大革命等变革。</li><li>文化变革：北美及欧洲大部分地区，大众教育将读写能力传授给大多数民众。所有的宗教传统此时都必须直面现代科学提出的挑战，例如达尔文提出的进化论对宗教的冲击。</li></ul><h3 id="20世纪危机"><a href="#20世纪危机" class="headerlink" title="20世纪危机"></a>20世纪危机</h3><p>从1913到1950年间，世界经济增长缓慢，曾经促进工业革命发展的国际金融业和贸易体系的崩溃是增速减缓的部分原因。各国将经济增长视为零和博弈，排挤市场中其他竞争对手。随后爆发了第一次世界大战，第一次世界大战将工业化战争的惊人规模和破坏力展现得淋漓尽致。在第一次世界大战后，德国出现了以西特勒为首的法西斯政权，俄国出现了由马克思主义指导，决心推翻资本主义的社会主义国家。20世纪30年代期间，第二次世界大战起源于日本和德国妄图创建各自的陆上帝国。第二次世界大战后，欧洲不再主导全球经济体系，美国和苏联成为新的超级大国。</p><h3 id="现代历史"><a href="#现代历史" class="headerlink" title="现代历史"></a>现代历史</h3><p>现代历史从1945年至今，第二次世界大战后，资本主义引擎再次轰鸣，早就了世界历史上最快的经济增速。美国的“马歇尔计划”提供了大规模 的重建援助资金，推动了全球 监管机构，如联合国（1945年）和国际货币基金组织（1947年）的成立，国际经济秩序回复 了稳定。在1945年后的40年间，大约有100个国家从欧洲领主手中取得了 独立，另一批新兴国家涌现于1991年苏联解体之后。<br>最后做一个总结：</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1529402706167.jpg" alt="enter description here"></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p class=&quot;description&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://oysz2016.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="读书笔记" scheme="https://oysz2016.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="历史" scheme="https://oysz2016.github.io/tags/%E5%8E%86%E5%8F%B2/"/>
    
  </entry>
  
  <entry>
    <title>《Deep Facial Expression Recognition:A Survey》论文笔记</title>
    <link href="https://oysz2016.github.io/post/5d962f61.html"/>
    <id>https://oysz2016.github.io/post/5d962f61.html</id>
    <published>2018-06-01T09:22:25.558Z</published>
    <updated>2019-03-17T05:48:03.867Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p class="description"></p><a id="more"></a><p>&emsp;&emsp;论文链接：<a href="https://arxiv.org/abs/1804.08348" target="_blank" rel="noopener">https://arxiv.org/abs/1804.08348</a><br>&emsp;&emsp;这篇文章<sup><a href="#fn_1" id="reffn_1">1</a></sup>是北邮的邓伟洪教授关于<strong>深度人脸表情识别(Deep Facial Expression Recognition,DFER)</strong> (<strong>情感识别</strong>)的一篇综述性文章，该文章被<strong>计算机视觉顶会CVPR</strong>收录。对于像我这样对情感识别感兴趣，但又没做过具体应用的小白来说研读这篇文章再合适不过了。</p><hr><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>&emsp;&emsp;表情可以说是一门世界语，不分国界、种族以及性别，可以说所有人都有着通用的表情。FEP在机器人、医疗、驾驶员驾驶疲劳检测和人机交互系统中都有广泛应用，最早在20世纪，Ekman和Friesen通过跨文化研究，定义了6种<strong>基础表情</strong>：生气、害怕、厌恶、开心、悲伤和吃惊，随后又加入了“蔑视” 这一表情。开创性的工作和直观的定义，使该模型在自动人脸表情识别(automatic facial expression analysis, AFEA)中依然很流行。<br>&emsp;&emsp;根据特征表示，FER系统可以划分为图片FER和视频FER两类。图片FER只提取当前图片的特征，而视频需要考虑相邻帧之间的关系。实际上所有计算机视觉的任务的处理对象都可以划分为图片和视频两类。<br>&emsp;&emsp;FER传统的方式使用手工提取的特征和浅层学习，这种方式的弊端就不多赘述了。得益于深度学习的发展和更具有挑战性的数据集FER2013的出现，越来越多的研究者将深度学习技术运用到FER中。</p><h2 id="深度人脸表情识别"><a href="#深度人脸表情识别" class="headerlink" title="深度人脸表情识别"></a>深度人脸表情识别</h2><p>&emsp;&emsp;这一节讨论了深度学习在人脸表情识别应用上的三个步骤。分别是预处理、特征提取和特征分类，简述了每一步的具体做法，并引用了相关论文。</p><h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><h4 id="人脸对齐"><a href="#人脸对齐" class="headerlink" title="人脸对齐"></a>人脸对齐</h4><p>&emsp;&emsp;给定一个数据集，第一步是移除与人脸不相关的背景和非人脸区域。ViolaJones(V&amp;J)人脸检测器<sup><a href="#fn_2" id="reffn_2">2</a></sup> (在OpenCV和Matlab中都有实现)，该检测器能将原始图片裁剪以获得人脸区域，<br>&emsp;&emsp;第二步是面对齐，这一步至关重要，因为可以减少人脸尺度改变和旋转产生的影响。最常用的面部对齐的实现是IntraFace<sup><a href="#fn_3" id="reffn_3">3</a></sup>,IntraFace采用SDM算法，定位出49个面部特征点（双眼、两个眉毛、鼻子和嘴巴）</p><h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><p>&emsp;&emsp;数据增强包括在线和离线两种方式：</p><ul><li>离线方式：随机扰动，图像变换（旋转、评议、翻转、缩放和对齐），添加噪声（椒盐噪声和斑点噪声），以及调整亮度和饱和度，以及在眼睛之间添加2维高斯分布的噪声。此外，还有用对抗神经网络GAN<sup><a href="#fn_4" id="reffn_4">4</a></sup>生成脸，3DCNN辅助AUs生成表情。使用GAN生成脸对网络性能是否有提升还没有验证过。</li><li>在线方式：包含在训练时，裁剪图片，水平翻转。主要是通过随机扰动训练模型。<h4 id="人脸归一化"><a href="#人脸归一化" class="headerlink" title="人脸归一化"></a>人脸归一化</h4>&emsp;&emsp;人脸的光照和头部姿势变化会削弱训练模型的性能，有两种脸部归一化的策略削弱影响，分别是亮度归一化和姿态归一化。</li><li>亮度归一化:Inface 工具箱<sup><a href="#fn_5" id="reffn_5">5</a></sup>是最常用的光照不变人脸检测箱。除了直观的调整亮度以外，还有对比度调整。常见的对比度调整方法有直方图归一化、DCT归一化、Dog归一化。</li><li>姿态归一化：这是一个棘手的问题，目前的方法都不太理想。有2D的landmark对齐,3Dlandmark对齐，有通过图像和相机参数估计，也有通过深度传感器测量然后计算出来。比较新的模型都是基于GAN的，有FF-GAN、TP-GAN和DR-GAN。</li></ul><h3 id="深度特征学习"><a href="#深度特征学习" class="headerlink" title="深度特征学习"></a>深度特征学习</h3><p>&emsp;&emsp;这一部分主要讲的是使用深度学习模型提取特征，包括卷积神经网络(Convolutional neural network，CNN)、深度置信网络（Deep belief network ，DBN）、深度自动编码器(Deep autoencoder，DAN)和递归神经网络(Recurrent neural network，RNN)。深度人脸表情识别的流程如下，下图可以看出，深度网络模型部分有四种常用的模型。作者只是简单的介绍了几种网络模型，在这里我也不过多的赘述。CNN模型在我前几篇博文<a href="https://oysz2016.github.io/post/4ec18e58.html">卷积神经网络的结构与相关算法</a>和<a href="https://oysz2016.github.io/post/d86a012b.html">卷积神经网络模型解读汇总——LeNet5，AlexNet、ZFNet、VGG16、GoogLeNet和ResNet</a>有详细介绍。其余的网络模型以后有时间会逐一整理。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1527855909779.jpg" alt="enter description here"></p><h3 id="人脸表情分类"><a href="#人脸表情分类" class="headerlink" title="人脸表情分类"></a>人脸表情分类</h3><p>&emsp;&emsp;完成了特征提取后，最后一步对其进行分类。在传统FER系统中，特征提取和特征分类是独立的。而深度学习的FER是端到端的模型，可以在网络的末端添加损失层调节反向传播的误差，预测概率可以由网络直接输出。也可以将两者结合，即用深度学习提取特征，再用SVM等分类器分类。</p><h2 id="面部表情数据库"><a href="#面部表情数据库" class="headerlink" title="面部表情数据库"></a>面部表情数据库</h2><p>&emsp;&emsp;该部分总结了FER可用的公开数据集。</p><ul><li><strong>CK+</strong>:包括123个subjects, 593 个 image sequence。该数据库由118名受试者录制，在这593个image sequence中，有327个sequence 有 emotion的 label。除了中性外包含7种表情：愤怒、蔑视、厌恶、恐惧、高兴、悲伤和惊讶。</li><li><strong>MMI</strong>：包括32个subjects，326个image sequence。213个sequence 有 emotion的 label。包含6中表情（相比较于CK+没有蔑视），MMI更具挑战性，因为很多人都戴有配饰。</li><li><strong>JAFFE</strong>：包含213副（每幅图像分辨率为256*256）日本女性的脸部图像，包含7种表情。该数据库均为正面脸相，且对原始图像进行了调整和修剪，光照均为正面光源，但光照强度有差异。</li><li><strong>TFD</strong>：改数据库是几个面部表情数据集的集合，TFD包含112234张图片(每张图片被调整到48*48大小)，所有实验对象的眼睛都是相同的距离。其中4189张有标注，包含7种表情。</li><li><strong>FER2013</strong>：改数据库通过谷歌图片API自动收集，数据库中所有图片都修正了标签，将图片调整到48*48大小。包含28709张训练图像，3589张测试图像，包含7种表情。</li><li><strong>AFEW</strong>：AFEW数据集为Emotion Recognition In The Wild Challenge (EmotiW)系列情感识别挑战赛使用的数据集，该比赛从2013开始每年举办一次。 该数据集的内容是从电影中剪辑的包含表情的视频片段，包含7类表情。训练集、验证集和测试集分别包含773、383和653sample。</li><li><strong>SFEW</strong>：该数据集是从AFEW数据集中抽取的有表情的静态帧，包含7类表情。训练集、验证集和测试集分别包含958、436和372sample。</li><li><strong>Multi-PIE</strong>：包含4个场景9种光照条件15个视角下337个subject，总计有755370张图片。包含6种表情（没有蔑视）</li><li><strong>BU-3DFE</strong>：从100个人获取的606个面部表情sequence，包含6种表情（没有蔑视），多用于三维面部表情分析。</li><li><strong>Oulu-CASIA</strong>：80个没被标记的subject收集了2880个image sequence。包含6种表情（没有蔑视）。有红外（NIR）和可见光（VIS）两种摄像头在3种不同光照条件下拍摄。</li><li><strong>RaFD</strong>：包含67个subject的1608张图片，眼睛有不同的三种注视方向，包括前、左和右。包含7种表情。</li><li>KDEF:最初用于医学和心理学研究。数据集来自70个演员从5个角度的6种表情。</li><li><strong>EmotioNet</strong>：包含从网上收集到的接近100万张面部表情图片。</li><li><strong>RAF-DB</strong>：包含从网上收集的29672张面部图像，包含7中基本表情和11种复合表情。</li><li><strong>AffectNet</strong>：包含从网上收集的100多万张面部图像，其中45万张图片手工标注为7种表情。</li></ul><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1527912456772.jpg" alt="enter description here"></p><h2 id="FER目前发展水平"><a href="#FER目前发展水平" class="headerlink" title="FER目前发展水平"></a>FER目前发展水平</h2><p>&emsp;&emsp;总结了基于静态图像和动态图像序列(视频)的FER进展。</p><h3 id="静态图像FER进展"><a href="#静态图像FER进展" class="headerlink" title="静态图像FER进展"></a>静态图像FER进展</h3><p>&emsp;&emsp;对于每一个数据集，下表显示了目前最优异的方法，在该数据集上取得的效果。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1527930265914.jpg" alt="enter description here"></p><h4 id="预训练和微调"><a href="#预训练和微调" class="headerlink" title="预训练和微调"></a>预训练和微调</h4><p>&emsp;&emsp;在相对较小的数据集上直接训练深度网络很容易导致过拟合。为了缓解这个问题，许多研究会在大数据集上先预训练网络，或者对已经训练好的网络进行微调。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1527930845031.jpg" alt="enter description here"></p><p>&emsp;&emsp;如上图所示，先在ImageNet数据集上训练，然后再在具体的人脸表情数据集上微调。微调有较好的效果，人脸表情识别有各种微调方式，比如分级、固定某些曾，不同网络层用不同数据集微调，具体可以看看原文中所引用的论文。<br>&emsp;&emsp;此外，文献<sup><a href="#fn_6" id="reffn_6">6</a></sup>指出FR和FER数据集存在巨大差异，人脸似乎别模型弱化了人脸表情的差异，提出了FaceNet2ExpNet网络消除这种影响。该模型分为两个阶段，首先用人脸识别模型提取特征，然后用表情识别网络消除人脸识别模型带来的情绪差异弱化。如下图所示。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1527932192340.jpg" alt="enter description here"></p><h4 id="多样化网络输入"><a href="#多样化网络输入" class="headerlink" title="多样化网络输入"></a>多样化网络输入</h4><p>&emsp;&emsp;传统的做法是使用原始的RGB图像作为网络的输入，然而原始数据缺乏重要的信息，如纹理信息，以及图像缩放、旋转、遮挡和光照等方面的不变性。因此可以借助一些手工设计的特征。如SIFT、LBP、MBP、AGEhe NCDV等。PCA可以裁剪出五官进行特征学习而不是整个脸部等。</p><h4 id="辅助块与层改进"><a href="#辅助块与层改进" class="headerlink" title="辅助块与层改进"></a>辅助块与层改进</h4><p>&emsp;&emsp;基于经典的CNN架构，有些研究设计了良好的辅助模块或者改进了网络层，这部分文中有列举几个例子，感兴趣可以找出相关论文翻看。<br>&emsp;&emsp;值得注意的是，Softmax在表情识别领域的表现不太理想。这是由于<strong>表情的类间区分度较低</strong>。作者整理了几种针对表情分类层的改进。</p><ul><li>受到center loss的启发，对特征与相应的类距离加了惩罚项，这分为两种<ul><li>一种是增加类间距离的island loss<sup><a href="#fn_7" id="reffn_7">7</a></sup>，如下图所示<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528008215481.jpg" alt="enter description here"></li><li>另一种是减下类内距离的LP<sup><a href="#fn_8" id="reffn_8">8</a></sup> loss,使同一类的局部相邻特征结合在一起。</li></ul></li><li>基于triplet-loss，关于triplet-loss的想法可以参考原文和<a href="https://blog.csdn.net/tangwei2014/article/details/46788025" target="_blank" rel="noopener">这篇博文</a>。<ul><li>exponential triplet-based loss(增加困难样本的权重)</li><li>(N+M)-tupes cluster loss(降低anchor的选择难度，以及阈值化triplet不等式),如下图所示。<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528008239408.jpg" alt="enter description here"><h4 id="网络集成"><a href="#网络集成" class="headerlink" title="网络集成"></a>网络集成</h4>&emsp;&emsp;之前的研究表明，多个网络的集合可以比单个网络表现的更好。在网络集成时，要考虑两点：</li></ul></li><li>网络模型要有充分的多样性，以确保网络之间具有互补性</li><li>要有可靠的集成算法</li></ul><p>&emsp;&emsp;关于第一点，网络的多样性产生有很多方法，不同的训练数据、不同的预处理方式、不同的网络模型、不同的参数都能产生不同的网络。<br>&emsp;&emsp;关于第二点集成算法。这其中也主要有两点，一个是特征集成，另一个是输出的决策集成。特征集成最常见的做法是将不同网络模型的特征直接链接，还有如下图的方式</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528008895990.jpg" alt="enter description here"></p><p>&emsp;&emsp;关于决策集成采用投票的机制，不同网络有不同的权重。关于决策集成的几种策略如下表所示。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528009037931.jpg" alt="enter description here"></p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528008969737.jpg" alt="enter description here"></p><h4 id="多任务网络"><a href="#多任务网络" class="headerlink" title="多任务网络"></a>多任务网络</h4><p>目前许多网络都是单一任务的输出，但在现实中，往往需要考虑其他多种因素的作用。多任务模型能从其他任务中学习到额外的信息有助于提高网络的泛化能力。关于多任务模型的好处，可以参考<a href="https://blog.csdn.net/laolu1573/article/details/78205180#3" target="_blank" rel="noopener">这篇博文</a>。如下如所示，在MSCNN<sup><a href="#fn_9" id="reffn_9">9</a></sup>模型中将脸部验证与表情识别两个任务集成在一个网络中。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528009483598.jpg" alt="enter description here"></p><h4 id="网络级联"><a href="#网络级联" class="headerlink" title="网络级联"></a>网络级联</h4><p>&emsp;&emsp;在级联网络中，将不同模块处理不同的任务组合在一起设计一个更深层的网络，前一个模块的输出被后一个模块使用。如下图所示，在AUDN网络中，该网络由三部分组成。</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528009785945.jpg" alt="enter description here"></p><h3 id="动态图像序列FER进展"><a href="#动态图像序列FER进展" class="headerlink" title="动态图像序列FER进展"></a>动态图像序列FER进展</h3><p>&emsp;&emsp;基于动态的表情识别相比静态图片能更全面，这里指的动态图像序列，即在视频中。</p><h4 id="帧聚合"><a href="#帧聚合" class="headerlink" title="帧聚合"></a>帧聚合</h4><p>&emsp;&emsp;考虑到表情在不同时刻有不同的变化，但又不可能单独的统计每帧的结果作为输出，因此需要对一段帧序列给出一个识别结果，这就需要用到帧聚合。即用一个特征向量表示这一段时间序列。与集成算法类似，帧聚合有有两类，分别是决策级帧聚合和特征级帧聚合。这两部分感兴趣的可以参看论文。</p><h4 id="强度表达网络"><a href="#强度表达网络" class="headerlink" title="强度表达网络"></a>强度表达网络</h4><p>&emsp;&emsp;在视频中表情会有微妙的变化，而强度是指在视频中，所有帧表现某个表情的程度。一般在中间位置最能表达某个表情，即为强度峰值。大多数方法，都关注峰值附近而忽略了开始和结束时的低谷帧。这部分，主要介绍几个深度网络，输入是具有一定强度信息的样本序列，输出是某一个类表情中不同强度帧之间的相关性结果。如PPDN（peak-piloted），用以内在表情序列里帧之间相关性识别，还有基于PPDN的级联PPDN网络DCPN，具有更深更强的识别能力。虽然，这些网络，都考虑了一段序列里的表情变换，甚至为了计算表情的变化趋势，设计了不同的损失函数，但是，真心觉得，这种代价，对于工程来说，其实是没有意义的。有兴趣的，可以看看论文里对应的方法，这里不再赘述了。</p><h4 id="深度时空FER网络"><a href="#深度时空FER网络" class="headerlink" title="深度时空FER网络"></a>深度时空FER网络</h4><p>&emsp;&emsp;前面介绍的帧聚合和强度表达网络都属于传统的结构化流程，而在视频中将一些列帧作为单独的图像序列输入，输出某一类表情的分类结果。而RNN网络能利用”序列信息”，所以视频FER模型用RNN网络，还有C#D:</p><ul><li>RNN: 从理论上讲，它可以利用任意长序列的信息,RNN呢能对时间序列上的变化建模。</li><li>C3D: 在通常图像上的2D空间卷积的基础上，沿着时间轴加了一个时间维度，就形成了3D时空卷积。例如3DCNN-DAP<sup><a href="#fn_10" id="reffn_10">10</a></sup>，网络模型如下图所示。<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528019915713.jpg" alt="enter description here"></li></ul><p>&emsp;&emsp;还有种“暴力”做法，不考虑时间维度，将帧序列拼接成大向量，再进行CNN分类，如DTAN<sup><a href="#fn_11" id="reffn_11">11</a></sup>。</p><ul><li>面部landmark运动轨迹：通过研究五官的变化轨迹，进而分析表情的变化，如深度几何空间网络(deep temporal geometry network，DTGN)。该方法联合每帧landmark的x,y坐标值，归一化处理后，将landmark作为一个运动轨迹维度，或者或者计算landmark特征点的成对L2距离特征，以及基于PHRNN用于获取帧内的空间变化信息。还有根据五官将landmark点分成4块，输入到BRNNs，定位局部特征，如下图：<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528020429700.jpg" alt="enter description here"></li><li>级联网络：跟之前静态图像的级联网络思路一样，主要是CNN提取特征，级联RNN做序列特征分类。如LRCN，级联CNN与LSTM，类似的，还有级联DAE作为特征提取，LSTM进行分类，还有ResNet-LSTM,即在低级CNN层，直接用LSTM连接序列之间的低级CNN特征，3DIR用LSTM作为一个单元构建了一个3D Inception-ResNet特征层，其他还有很多类似的级联网络，包括，用CRFs代替了LSTM等等。</li><li>网络集成：如两路CNN网络模型用于行为识别，一路用多帧数据的稠密光流训练获取时间信息，一路用于单帧图像特征学习，最后融合两路CNN的输出。还有多通道训练，如一通道用于自然脸和表情脸之间的光流信息训练，一路用于脸部表情特征训练，然后用三种融合策略，平均融合，基于SVM融合，基于DNN融合。也有基于PHRNN时间网络和MSCNN空间网络相结合来提取局部整体关系，几何变化以及静动态信息。除了融合，也有联合训练的，如DTAN和DTGN联合fineturn训练。<br><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528020602436.jpg" alt="enter description here"></li></ul><p>&emsp;&emsp;目前各个数据集上，动态序列的表情识别的最佳效果如下表所示：</p><p><img src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/1528020691450.jpg" alt="enter description here"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote id="fn_1"><sup>1</sup>. Li S, Deng W. Deep Facial Expression Recognition: A Survey[J]. 2018.<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. Viola P, Jones M. Rapid object detection using a boosted cascade of simple features[J]. Proc Cvpr, 2001, 1:511.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. Torre F D L, Chu W S, Xiong X, et al. IntraFace[C]// IEEE International Conference and Workshops on Automatic Face and Gesture Recognition. IEEE, 2015:1-8.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. Goodfellow I J, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets[C]// International Conference on Neural Information Processing Systems. MIT Press, 2014:2672-2680.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. <a href="http://luks.fe.uni-lj.si/sl/osebje/vitomir/face tools/INFace/" target="_blank" rel="noopener">http://luks.fe.uni-lj.si/sl/osebje/vitomir/face tools/INFace/</a><a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. Ding H, Zhou S K, Chellappa R. FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition[J]. 2016:118-126.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><blockquote id="fn_7"><sup>7</sup>. Cai J, Meng Z, Khan A S, et al. Island Loss for Learning Discriminative Features in Facial Expression Recognition[J]. 2017.<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></blockquote><blockquote id="fn_8"><sup>8</sup>. Li S, Deng W, Du J P. Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2017:2584-2593.<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></blockquote><blockquote id="fn_9"><sup>9</sup>. Zhang K, Huang Y, Du Y, et al. Facial Expression Recognition Based on Deep Evolutional Spatial-Temporal Networks[J]. IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 2017, PP(99):1-1.<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></blockquote><blockquote id="fn_10"><sup>10</sup>. Liu M, Li S, Shan S, et al. Deeply Learning Deformable Facial Action Parts Model for Dynamic Expression Analysis[M]// Computer Vision — ACCV 2014. Springer International Publishing, 2014:143-157.<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a></blockquote><blockquote id="fn_11"><sup>11</sup>. Jung H, Lee S, Yim J, et al. Joint Fine-Tuning in Deep Neural Networks for Facial Expression Recognition[C]// IEEE International Conference on Computer Vision. IEEE, 2016:2983-2991. <a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a></blockquote><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p class=&quot;description&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CVPR" scheme="https://oysz2016.github.io/tags/CVPR/"/>
    
      <category term="FER" scheme="https://oysz2016.github.io/tags/FER/"/>
    
  </entry>
  
  <entry>
    <title>数学公式语法——Mathjax教程</title>
    <link href="https://oysz2016.github.io/post/8611e6fb.html"/>
    <id>https://oysz2016.github.io/post/8611e6fb.html</id>
    <published>2018-05-31T11:02:33.380Z</published>
    <updated>2019-03-17T05:49:41.870Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p class="description"></p><a id="more"></a><p>&emsp;&emsp;在着手写博客前，喜欢在“印象笔记”上记录学习笔记，当时觉得“印象笔记”的富文本编辑器用着还挺顺手。在搭建博客开始学着用Markdown写作后，再看原来在“印象笔记”中的笔记，格式排版真是惨不忍睹，Markdown的使用很大程度上提升了写作效率，也统一了排版。这里顺便推荐一款Markdown的编辑器——<a href="http://soft.xiaoshujiang.com/" target="_blank" rel="noopener">小书匠</a>，小书匠支持标准的Markdown语法，也具有强大的语法扩展功能，支持大多数图床和“印象笔记”等第三方存储。<br>&emsp;&emsp;在上一篇博客<a href="https://oysz2016.github.io/post/1b649e52.html">《Relation Networks for Object Detection》论文笔记</a>中由于论文中有不少公式需要介绍，又不想用图片代替影响阅读体验，好在Markdown支持Mathjax语法。但不得不说刚开始使用Mathjax编辑公式，还是很不习惯，几千字的博文，公式编辑花了很长时间。因此用这篇博文总结一下Mathjax的语法，搬砖的过程也让自己熟悉Mathjax。</p><h2 id="Mathjax简介"><a href="#Mathjax简介" class="headerlink" title="Mathjax简介"></a>Mathjax简介</h2><p>&emsp;&emsp;Mathjax是一款运行在浏览器中的开源数学符号渲染引擎，使用MathJax可以方便的在浏览器中显示数学公式，不需要使用图片。</p><h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><ul><li>在正文中同一行插入LaTeX公式用<code>$...$</code>定义<ul><li>例如语句为<code>$\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$</code></li><li>显示为$\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$</li></ul></li><li>另起一行显示LaTeX公式用<code>$$...$$</code><ul><li>例如语句为<code>$$W_G^{mn}=max\{0,W_G.\xi_G(f_G^m,f_G^n)\}$$</code></li><li>显示为<script type="math/tex">W_G^{mn}=max\{0,W_G.\xi_G(f_G^m,f_G^n)\}</script><h2 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h2></li></ul></li></ul><div class="table-container"><table><thead><tr><th>显示</th><th>命令</th><th>显示</th><th>命令</th></tr></thead><tbody><tr><td>$\alpha$</td><td>\alpha</td><td>$\beta$</td><td>\beta</td></tr><tr><td>$\gamma$</td><td>\gamma</td><td>$\delta$</td><td>\delta</td></tr><tr><td>$\epsilon$</td><td>\epsilon</td><td>$\zeta$</td><td>\zeta</td></tr><tr><td>$\eta$</td><td>\eta</td><td>$\theta$</td><td>\theta</td></tr><tr><td>$\iota$</td><td>\iota</td><td>$\kappa$</td><td>\kappa</td></tr><tr><td>$\lambda$</td><td>\lambda</td><td>$\mu$</td><td>\mu</td></tr><tr><td>$\nu$</td><td>\nu</td><td>$\xi$</td><td>\xi</td></tr><tr><td>$\pi$</td><td>\pi</td><td>$\rho$</td><td>\rho</td></tr><tr><td>$\sigma$</td><td>\sigma</td><td>$\tau$</td><td>\tau</td></tr><tr><td>$\upsilon$</td><td>\upsilon</td><td>$\phi$</td><td>\phi</td></tr><tr><td>$\chi$</td><td>\chi</td><td>$\psi$</td><td>\psi</td></tr><tr><td>$\omega$</td><td>\omega</td></tr></tbody></table></div><ul><li>若需要大写希腊字母，将命令首字母大写即可。<code>$\gamma$</code>呈现为$\Gamma$ <ul><li>若需要斜体希腊字母，将命令前加上var前缀即可。<code>$\varGamma$</code>呈现为$\varGamma$</li></ul></li></ul><h2 id="关系运算符"><a href="#关系运算符" class="headerlink" title="关系运算符"></a>关系运算符</h2><div class="table-container"><table><thead><tr><th>显示</th><th>命令</th><th>显示</th><th>命令</th></tr></thead><tbody><tr><td>$\mid$</td><td>\mid</td><td>$\nmid$</td><td>\nmid</td></tr><tr><td>$\cdot$</td><td>\cdot</td><td>$\leq$</td><td>\leq</td></tr><tr><td>$\geq$</td><td>\geq</td><td>$\neq$</td><td>\neq</td></tr><tr><td>$\approx$</td><td>\approx</td><td>$\equiv$</td><td>\equiv</td></tr><tr><td>$\prec$</td><td>\prec</td><td>$\preceq$</td><td>\preceq</td></tr><tr><td>$\ll$</td><td>\ll</td><td>$\succ$</td><td>\succ</td></tr><tr><td>$\succeq$</td><td>\succeq</td><td>$\gg$</td><td>\gg</td></tr><tr><td>$\sim$</td><td>\sim</td><td>$\simeq$</td><td>\simeq</td></tr><tr><td>$\asymp$</td><td>\asymp</td><td>$\cong$</td><td>\cong</td></tr><tr><td>$\doteq$</td><td>\doteq</td><td>$\propto$</td><td>\propto</td></tr><tr><td>$\models$</td><td>\models</td><td>$\parallel$</td><td>\parallel</td></tr><tr><td>$\bowtie$</td><td>\bowtie</td><td>$\perp$</td><td>\perp</td></tr><tr><td>$\circ$</td><td>\circ</td><td>$\ast$</td><td>\ast</td></tr><tr><td>$\bigodot$</td><td>\bigodot</td><td>$\bigotimes$</td><td>\bigotimes</td></tr><tr><td>$\bigoplus$</td><td>\bigoplus</td><td></td></tr></tbody></table></div><h2 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h2><div class="table-container"><table><thead><tr><th>显示</th><th>命令</th><th>显示</th><th>命令</th></tr></thead><tbody><tr><td>$\pm$</td><td>\pm</td><td>$\mp$</td><td>\mp</td></tr><tr><td>$\times$</td><td>\times</td><td>$\ast$</td><td>\ast</td></tr><tr><td>$\star$</td><td>\star</td><td>$\circ$</td><td>\circ</td></tr><tr><td>$\bullet$</td><td>\bullet</td><td>$\cdot$</td><td>\cdot</td></tr><tr><td>$\div$</td><td>\div</td><td>$\sum$</td><td>\sum</td></tr><tr><td>$\prod$</td><td>\prod</td><td>$\coprod$</td><td>\coprod</td></tr><tr><td>$\oplus$</td><td>\oplus</td><td>$\bigoplus$</td><td>\bigoplus</td></tr><tr><td>$\ominus$</td><td>\ominus</td><td>$\otimes$</td><td>\otimes</td></tr><tr><td>$\bigotimes$</td><td>\bigotimes</td><td>$\oslash$</td><td>\oslash</td></tr><tr><td>$\odot$</td><td>\odot</td><td>$\bigodot$</td><td>\bigodot</td></tr><tr><td>$\diamond$</td><td>\diamond</td><td>$\bigtriangleup$</td><td>\bigtriangleup</td></tr><tr><td>$\bigtriangledown$</td><td>\bigtriangledown</td><td>$\triangleleft$</td><td>\triangleleft$</td></tr><tr><td>$\triangleright$</td><td>\triangleright</td><td>$\triangleright$</td><td>\triangleright</td></tr><tr><td>$\bigcirc$</td><td>\bigcirc</td></tr></tbody></table></div><h2 id="字母修饰"><a href="#字母修饰" class="headerlink" title="字母修饰"></a>字母修饰</h2><h3 id="上下标"><a href="#上下标" class="headerlink" title="上下标"></a>上下标</h3><ul><li>上标:<code>^</code></li><li>下标:<code>_</code></li><li>例如:<code>C_n^2</code>，显示为$C_n^2$<h2 id="矢量"><a href="#矢量" class="headerlink" title="矢量"></a>矢量</h2></li><li><code>\vec</code> a，显示为$\vec a$</li><li><code>\overrightarrow{xy}</code>，显示为:$\overrightarrow{xy}$<h2 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h2></li><li>打印机字体Typewriter：<code>\mathtt{A}</code>显示为$\mathtt{A}$</li><li>黑板粗体字Blackboard Bold：<code>\mathbb{A}</code>呈现为$\mathbb{A}$</li><li>无衬线字体Sans Serif：<code>\mathsf{A}</code>呈现为$\mathsf{A}$</li><li>手写体:<code>\mathscr{A}</code>呈现为$\mathscr{A}$</li><li>罗马字体:<code>\mathrm{A}</code>呈现为$\mathrm{A}$<h2 id="括号"><a href="#括号" class="headerlink" title="括号"></a>括号</h2></li><li>小括号:<code>()</code>，显示为()</li><li>中括号：<code>[]</code>，显示为[]</li><li>尖括号：<code>\langle</code>,<code>\rangle</code>呈现为⟨⟩ </li><li>自适应括号：<code>\left(...\right)</code>能使符号大小与邻近公式相适应<ul><li><code>(\frac{x}{y})</code>，显示为$(\frac{x}{y})$</li><li><code>\left(\frac{x}{y}\right)</code>，显示为$\left(\frac{x}{y}\right)$<h2 id="求和、极限与积分"><a href="#求和、极限与积分" class="headerlink" title="求和、极限与积分"></a>求和、极限与积分</h2></li></ul></li><li>求和：<code>\sum</code><ul><li>举例：<code>\sum_{i=1}^n{a_i}</code>呈现为$\sum_{i=1}^n{a_i}$</li></ul></li><li>极限：<code>\lim</code><ul><li>举例:<code>\lim_{x\to 0}</code>呈现为$\lim_{x \to 0}$</li></ul></li><li>积分:<code>\int</code><ul><li>举例:<code>\int_0^xf(x)dx</code>呈现为$\int_0^xf(x)dx$</li></ul></li></ul><h2 id="分式与根式"><a href="#分式与根式" class="headerlink" title="分式与根式"></a>分式与根式</h2><ul><li>分式:<code>\frac</code><ul><li>举例:<code>\frac{分子}{分母}</code>呈现为$\frac{分子}{分母}$</li></ul></li><li>根式:<code>\sqrt</code><ul><li>举例：<code>\sqrt[x]{y}</code>呈现为$\sqrt[x]{y}$<h2 id="特殊函数"><a href="#特殊函数" class="headerlink" title="特殊函数"></a>特殊函数</h2></li></ul></li><li><code>\函数名</code><ul><li>举例:<code>\sin x</code>，<code>\ln x</code>，<code>\max(A,B,C)</code>呈现为$sin x$,$ln x$,$max(A,B,C)$<h2 id="空格"><a href="#空格" class="headerlink" title="空格"></a>空格</h2></li></ul></li><li>LaTex语法会忽略空格，需要用转义字符\<ul><li>小空格:<code>a\ b</code>呈现为$a\ b$</li><li>四个空格:<code>a\quad b</code>呈现为$a\quad b$<h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h2><h3 id="基本语法-1"><a href="#基本语法-1" class="headerlink" title="基本语法"></a>基本语法</h3></li></ul></li><li>起始标记<code>\begin{matrix}``，结束标记``\end{matrix}</code></li><li>每一行末尾标记\\，行间元素以$分割</li><li>举例<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$$\begin&#123;matrix&#125;</span><br><span class="line"><span class="number">1</span>&amp;<span class="number">0</span>&amp;<span class="number">0</span>\\</span><br><span class="line"><span class="number">0</span>&amp;<span class="number">1</span>&amp;<span class="number">0</span>\\</span><br><span class="line"><span class="number">0</span>&amp;<span class="number">0</span>&amp;<span class="number">1</span>\\</span><br><span class="line">\end&#123;matrix&#125;$$</span><br></pre></td></tr></table></figure></li></ul><p>呈现为:<script type="math/tex">\begin{matrix}1&0&0\\0&1&0\\0&0&1\\\end{matrix}</script></p><h3 id="矩阵边框"><a href="#矩阵边框" class="headerlink" title="矩阵边框"></a>矩阵边框</h3><ul><li>在起始、结束标记处用下列词替换matrix<ul><li>pmatrix：小括号边框</li><li>bmatrix：中括号边框</li><li>Bmatrix：大括号边框</li><li>vmatrix：单竖线边框</li><li>Vmatrix：双竖线边框<h3 id="省略元素"><a href="#省略元素" class="headerlink" title="省略元素"></a>省略元素</h3><ul><li>横省略号：\cdots</li><li>竖省略号：\vdots</li><li>斜省略号：\ddots</li><li>举例<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$$\begin&#123;bmatrix&#125;</span><br><span class="line">&#123;a<span class="number">_</span>&#123;<span class="number">11</span>&#125;&#125;&amp;&#123;a<span class="number">_</span>&#123;<span class="number">12</span>&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a<span class="number">_</span>&#123;<span class="number">1</span>n&#125;&#125;\\</span><br><span class="line">&#123;a<span class="number">_</span>&#123;<span class="number">21</span>&#125;&#125;&amp;&#123;a<span class="number">_</span>&#123;<span class="number">22</span>&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a<span class="number">_</span>&#123;<span class="number">2</span>n&#125;&#125;\\</span><br><span class="line">&#123;\vdots&#125;&amp;&#123;\vdots&#125;&amp;&#123;\ddots&#125;&amp;&#123;\vdots&#125;\\</span><br><span class="line">&#123;a<span class="number">_</span>&#123;m1&#125;&#125;&amp;&#123;a<span class="number">_</span>&#123;m2&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a<span class="number">_</span>&#123;mn&#125;&#125;\\</span><br><span class="line">\<span class="keyword">end</span>&#123;bmatrix&#125;$$</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><p>呈现为:<script type="math/tex">\begin{bmatrix}{a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\{a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{a_{m1}}&{a_{m2}}&{\cdots}&{a_{mn}}\\\end{bmatrix}</script></p><h2 id="方程组"><a href="#方程组" class="headerlink" title="方程组"></a>方程组</h2><ul><li>需要cases环境：起始、结束处以{cases}声明</li><li>举例<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="formula">$$<span class="tag">\<span class="name">begin</span><span class="string">&#123;cases&#125;</span></span></span></span><br><span class="line"><span class="formula">a_1x+b_1y+c_1z=d_1<span class="tag">\<span class="name">\</span></span></span></span><br><span class="line"><span class="formula">a_2x+b_2y+c_2z=d_2<span class="tag">\<span class="name">\</span></span></span></span><br><span class="line"><span class="formula">a_3x+b_3y+c_3z=d_3<span class="tag">\<span class="name">\</span></span></span></span><br><span class="line"><span class="formula"><span class="tag">\<span class="name">end</span><span class="string">&#123;cases&#125;</span></span></span></span><br><span class="line"><span class="formula">$$</span></span><br></pre></td></tr></table></figure></li></ul><script type="math/tex; mode=display">\begin{cases}a_1x+b_1y+c_1z=d_1\\a_2x+b_2y+c_2z=d_2\\a_3x+b_3y+c_3z=d_3\\\end{cases}</script><h2 id="公式编号"><a href="#公式编号" class="headerlink" title="公式编号"></a>公式编号</h2><ul><li>用<code>\tag{n}</code>标签</li><li>举例<code>f(x)=x\tag{1}</code>显示为$f(x)=x\tag{1}$</li></ul><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>&emsp;&emsp;以上列举的都是常用的Mathjax语法，以后有用到新的会继续补充。</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p class=&quot;description&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo博客搭建教程" scheme="https://oysz2016.github.io/categories/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="Hexo" scheme="https://oysz2016.github.io/tags/Hexo/"/>
    
      <category term="Mathjax" scheme="https://oysz2016.github.io/tags/Mathjax/"/>
    
  </entry>
  
</feed>
