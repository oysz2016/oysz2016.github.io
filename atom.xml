<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>冲弱&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/ab8296a41c8b88ea9f8a771cd548cb5e</icon>
  <subtitle>多阅读 多积累</subtitle>
  <link href="https://oysz2016.github.io/atom.xml" rel="self"/>
  
  <link href="https://oysz2016.github.io/"/>
  <updated>2023-02-04T12:11:32.505Z</updated>
  <id>https://oysz2016.github.io/</id>
  
  <author>
    <name>冲弱</name>
    <email>ouyang-sz@foxmail.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>半监督学习——分类</title>
    <link href="https://oysz2016.github.io/post/18365ad2.html"/>
    <id>https://oysz2016.github.io/post/18365ad2.html</id>
    <published>2023-02-04T10:56:45.104Z</published>
    <updated>2023-02-04T12:11:32.505Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>半监督学习是机器学习中很常用的一种算法，之前有了解些大概，但没系统性的看过该领域的文章。趁着有空总结了下该领域的文章，由于文章比较多，整理完会分成两部分:</p><ul><li><strong>半监督学习——分类</strong></li><li><strong>半监督学习——检测</strong><br>这篇文章是分类，对半监督学习感兴趣的朋友可以期待下。</li></ul><h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><p><strong>监督算法:</strong> 监督算法的训练数据是有标签的，目的是让模型能正确预测数据的标签</p><p><strong>无监督算法:</strong> 训练数据没有标签，根据数据自身的特性设计模型进行分类</p><p><strong>半监督算法:</strong> 半监督算法是介于无监督和监督算法之间的一种算法类型。其特点是有少量有标签的数据，以及大量的无标签数据，可以得到只用有标签数据训练更好的结果</p><h1 id="π-model"><a href="#π-model" class="headerlink" title="π model"></a>π model</h1><p>π model网络结构如下图所示。因为是半监督学习，既有有标签数据，也有无标签数据，π model将网络分为了两部分:</p><ul><li>第一部分为图中使用交叉熵损失。该部分为有label的数据$x_i$，$z_i$为模型预测结果，用于和$y_i$比对，使用corss entropy</li><li>第二部分使用MSE损失。该部分为无label的数据$x_i$，$x_i$会经过数据增广和模型中的drop out，这两部分都是为了引入一定的数据差异。同一个输入经过两次增广和dropout之后得到特征$z_i$和$\widetilde{z_i}$，使用MSE loss，目的是使增广前后的特征尽可能的相似。<br>两个损失会经过weighted sum配置合适的权重，得到最终的loss</li></ul><p>下面是π model的伪代码</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/2.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/3.png" alt=""></p><h1 id="Temporal-ensembling"><a href="#Temporal-ensembling" class="headerlink" title="Temporal ensembling"></a>Temporal ensembling</h1><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/1610.02242">https://arxiv.org/abs/1610.02242</a><br><strong>代码链接:</strong> <a href="https://github.com/ferretj/temporal-ensembling">https://github.com/ferretj/temporal-ensembling</a><br>Temporal ensembling的方法和π model非常相似，核心思想都是想利用好有监督的数据和无监督的数据，改进的原因是在π model中使用了两次模型增广和推理，耗时比较长，在Temporal ensembling中使用了<strong>Temporal</strong>的模型，具体而言是使用了之前epoch得到的特征结果去做对比，模型结构如下图:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/4.png" alt=""></p><p>$\widetilde{z_i}$为之前epoch模型保存的特征，$z_i$为当前模型的推理结果，并且会保存给下一个epoch使用</p><p>伪代码如下:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/5.png" alt=""><br>注意, 在更新特征z时，不光用到了当前的特征，也会考虑到之前所有的特征，具体可以看伪代码最后几行</p><p>在大量半监督数据上做了消融实验，验证了Temporal ensembling的有效性</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/6.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/7.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/8.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/9.png" alt=""></p><h1 id="Mean-teacher"><a href="#Mean-teacher" class="headerlink" title="Mean teacher"></a>Mean teacher</h1><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/1703.01780">https://arxiv.org/abs/1703.01780</a><br><strong>代码链接:</strong> <a href="https://github.com/CuriousAI/mean-teacher">https://github.com/CuriousAI/mean-teacher</a></p><p>mean teacher的思想和Temporal ensembling也非常相似。改进的原因是在Temporal ensembling中每个epoch才会更新一次对比的特征，在大型的数据集上训练时，用于对比的特征更新的非常慢，时间成本也很高，因此在mean teacher中使用更新模型的方式取代原来更新特征的方式。思想如下图所示</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/10.png" alt=""></p><p>student model即当前模型，teacher model为更新的模型，会参考当前模型和之前模型的权重，得到一个新的模型。更新的方式用公式</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/11.png" alt=""></p><p>目的也是使student mode和teacher model预测出的特征一致</p><p>mean teache和之前的工作π model以及Temporal ensembling进行了对比</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/12.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/13.png" alt=""></p><h1 id="MixMatch"><a href="#MixMatch" class="headerlink" title="MixMatch"></a>MixMatch</h1><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/1905.02249">https://arxiv.org/abs/1905.02249</a><br><strong>代码链接:</strong> <a href="https://github.com/YU1ut/MixMatch-pytorch">https://github.com/YU1ut/MixMatch-pytorch</a></p><p>mixmatch是最小化熵的方法，和前面介绍的三篇一致性正则化法有些差异。最小化熵方法的思想基于机器学习中的一个共识。即分类器的边界边际分布的高密度区域(说人话就是不能从类中心去划分边界)。因此强迫分类器对未标记数据做出低熵预测。实现方法是在损失函数中增加一项，最小化$p_{model}(y|x)$对应的熵。</p><p>mixmatch的思想在下图中伪代码展示的很清楚</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/14.png" alt=""></p><p>一个batch中有B份有标签的数据X和B份无标签的数据U</p><ul><li>将有标签的数据增广一次</li><li>将无标签的数据增广K次</li><li>对无标签的数据增广后的结果模型推理后分类，对K个结果取平均，得到模型预测的标签</li><li>使用temperature sharpening算法，对上一步得到的标签后处理。sharpening算法公式如下:</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/15.png" alt=""></p><ul><li>增强后带标签的数据组成一个batch，即$\widetilde{X}$</li><li>增强后无标签的数据，和预测出的标签组成K个batch，即$\widetilde{U}$</li><li>将$\widetilde{X}$和$\widetilde{U}$混合，得到新的数据集W</li><li>应用mixup将增广后有标签的数据$\widetilde{X}$和新的数据W混合得到$X^{‘}$</li><li>应用mixup将增广后无标签的数据$\widetilde{U}$和新的数据W混合得到$U^{‘}$</li></ul><p>得到两个新的数据集后就可以按mixup混合的思路进行训练</p><p>sharpening的大致流程如下图所示</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/16.png" alt=""></p><p>接下来是实验结果部分:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/17.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/18.png" alt=""></p><h1 id="ReMixMatch"><a href="#ReMixMatch" class="headerlink" title="ReMixMatch"></a>ReMixMatch</h1><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/1911.09785">https://arxiv.org/abs/1911.09785</a><br><strong>代码链接:</strong><a href="https://github.com/google-research/remixmatch">https://github.com/google-research/remixmatch</a><br>从名字上可以看出ReMixMatch是MixMatch的改进版本。MixMatch中比较重要的思想是猜测无标签数据的标签，使用最小化熵的方法做训练。<br>ReMixMatch的改进主要包含两部分: Distribution Alignment和Augmentation Anchor。伪代码如下:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/19.png" alt=""></p><ul><li><strong>Distribution Alignment</strong>。考虑到猜测无标签数据的label有可能存在噪声，因此考虑到使用有标签数据的标签分布，对无标签的猜测进行对齐。对应代码中的第7行。如下图所示，$q_b$对应Label guess。$\widetilde{p}(y)$是一个运行平均版本的无标签猜测，$p(y)$是有标签数据的标签分布。对齐之后的标签猜测如下公式<script type="math/tex; mode=display">q_b=Normalize(q_b*\frac{p(y)}{\widetilde{p}(y)})</script></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/20.png" alt=""></p><ul><li><strong>Augmentation Anchor</strong>。考虑到简单数据增广得到的预测结果会比复杂增广得到的预测结果要更加可信。因此对于一张图片，首先会进行弱增广，再进行多次强增广。弱增广和强增广共同使用一个标签进行mixup和模型训练</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/21.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/22.png" alt=""></p><p>接下来是实验结果部分:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/23.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/24.png" alt=""></p><h1 id="FixMatch"><a href="#FixMatch" class="headerlink" title="FixMatch"></a>FixMatch</h1><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2001.07685">https://arxiv.org/abs/2001.07685</a><br><strong>代码链接:</strong><a href="https://github.com/google-research/fixmatch">https://github.com/google-research/fixmatch</a></p><p>在FixMatch中没有采用MixMatch系列中有标签数据和无标签数据猜测标签后互相mixup作为训练样本的方法，本身思想和mean teacher及之前方法更相似。<strong>FixMatch中的Fix主要强调的是混合两种数据增广方式，分别是弱增广和强增广</strong><br>弱数据增广的方式:</p><ul><li>平移</li><li>反转</li><li>平移&amp;反转<br>强数据增广:</li><li>cutout</li><li>random augment</li><li>control theory augment<br>其中若数据增广和强数据增广的前两种方式都比较常见，这里详细说下control theory augment数据增广方法:</li><li>该数据增广方法中有18个候选集，例如旋转，色彩变换等</li><li>初始化transform的权重$W=[w_1,w_2,…,w_18]$</li><li>随机选择其中两个增广方法i和j，增广的权重分别为$w_i/(w_i+w_j)$, $w_j/(w_i+w_j)$</li><li>将两个增广图像混合，更新transform的权重$W$, 第一个公式可以看作MAE损失，第二个公式是用动量的方式更新transform的权重。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/30.png" alt=""></li></ul><p>因此可以将control theory augment看作是可以学习的rand augment, 能学习出好的增广方式，使得分类更准确。为什么不用auto augment呢？因为auto augment需要标签作为学习的条件，而半监督中大部分数据都没有标签</p><p>对于无标签的数据，会先经过弱数据增广获取伪标签，只有某一类的置信度大于一定的阈值才会执行为标签的生成（下图中红色部分），生成的伪标签用于监督强增广的输出值。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/25.png" alt=""></p><p>接下来是实验结果部分:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/26.png" alt=""></p><h1 id="Noisy-Student"><a href="#Noisy-Student" class="headerlink" title="Noisy Student"></a>Noisy Student</h1><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/1911.04252">https://arxiv.org/abs/1911.04252</a><br><strong>代码链接:</strong><a href="https://github.com/google-research/noisystudent">https://github.com/google-research/noisystudent</a></p><p>Noisy Student和之前的一致性正则化法也非常相似。区别在于强调了在student模型中加入噪声。在这篇文章中有teacher model和student model的概念。teacher是使用有标签数据训练的模型，student是使用teacher模型预测了无标签数据的标签后，用无标签数据训练的模型。</p><p>而Noisy Student强调的是在student模型中加入噪声，teacher模型和student模型可以用不同的模型训练，也可以使用相同的模型。要思考加入噪声的原因，可以假设teacher和student相同结构联合训练的场景，则student模型不加入噪声，则预测结果和通过teacher模型生成的伪标签会完全一致，student模型就失去了更新的动力。所以加入噪声是很有必要的。</p><p>Noisy Student的思想如下图所示:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/27.png" alt=""></p><p>关于student模型使用数据增广和dropout的消融实验如下：</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/28.png" alt=""></p><p>该文章列举了一些消融实验得到的经验，基本都很符合直觉:</p><ul><li>使用性能好的teacher模型能得到更好的结果</li><li>大量的无标签数据是必要的</li><li>在某些场景下，soft伪标签比hard伪标签效果要好</li><li>大的学生模型很重要</li><li>数据均衡对小模型很重要</li><li>有标签数据和无标签数据联合训练效果更好</li><li>无标签数据:有标签数据的比值越大，该方法越有效</li><li>从头开始训练student有时比用teacher初始化student效果要好</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过阅读上面的7篇文章，半监督学习大概可以分成3种训练方式，分别是<strong>一致性正则(Consistency Regularization Model)</strong>, <strong>伪标签模型(Proxy Label Model)</strong>, <strong>一致性正则&amp;伪标签模型</strong></p><ul><li><strong>一致性正则:</strong> 图片增广前后，模型的预测结果应该相同。在该种方法中一般会加入数据增广和dropout，引入随机性，损失函数会使用MSE Loss。例如π model，Temporal ensembling，Mean teacher都是该类型的方法</li><li><strong>伪标签模型:</strong> 先在有标签的数据上做训练，然后预测无标签数据的伪标签；模型的训练包括有标签的训练和伪标签的训练。例如MixMatch、ReMixmatch都是该类型的方法</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="半监督" scheme="https://oysz2016.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3/"/>
    
    <category term="分类" scheme="https://oysz2016.github.io/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>决策树汇总——ID3、C4.5、CART</title>
    <link href="https://oysz2016.github.io/post/5f5a1d32.html"/>
    <id>https://oysz2016.github.io/post/5f5a1d32.html</id>
    <published>2023-01-15T06:58:46.653Z</published>
    <updated>2023-01-15T07:48:24.536Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span><br>决策树是机器学习中的经典算法之一，十大机器学习算法中就包含决策树算法(Decision Tree). 顾名思义，决策树是基于树结构进行决策的，根据算法不同会使用多叉树或者二叉树。其实人类在日常生活中，遇到问题时也会使用决策树进行判断。例如西瓜书里有列举挑选西瓜的例子。</p><p>一颗决策树一般包含一个根结点，若干个内部节点和若干叶节点，根结点是判断的开始，内部节点是一些子判断流程，而叶节点对应于决策的结果。决策树作为机器学习算法的一种，其目的是产生一颗泛化能力强的树，能处理训练集中没有见到的情形，并能正确决策。</p><h2 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h2><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>信息熵是度量样本集合纯度最常用的一种指标，令D中第k类样本的比例为$p_k$, 则D的信息熵为<br>$Ent(D)=-\sum_{k=1}^{|y|} p_klog_2{p_k}$</p><p>信息增益=信息熵-条件熵<br>$Gain(D,a)=Ent(D)-\sum_{v=1}^{V} \frac{D^v}{D}Ent(D^v)$</p><p>以西瓜书中的列子<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%86%B3%E7%AD%96%E6%A0%91%E6%B1%87%E6%80%BB/2.png" alt=""></p><p>17个样本中正例有8个，负例有9个，则根结点的信息熵为:<br>$Ent(D)=-\sum_{k=1}^{2} p_klog_2{p_k}=-(\frac{8}{17}log_2 \frac{8}{17}+\frac{9}{17}log_2 \frac{9}{17})=0.998$</p><p>然后计算相应属性的条件熵，以色泽属性为例，可以分为 $D^1$(色泽=青绿)有6个样本，正反各3个，$D^2$(色泽=乌黑)有6个样本，正4，饭2，$D^3$(色泽=浅白)，有5个样本，正1反4.则3个分支节点的信息熵为：<br>$Ent(D^1)=-(\frac{3}{6}log_2 \frac{3}{6}+\frac{3}{6}log_2 \frac{3}{6})=1$<br>$Ent(D^2)=-(\frac{4}{6}log_2 \frac{4}{6}+\frac{2}{6}log_2 \frac{2}{6})=0.918$<br>$Ent(D^2)=-(\frac{1}{5}log_2 \frac{1}{5}+\frac{4}{5}log_2 \frac{4}{5})=0.722$</p><p>则色泽的信息增益为:</p><p>$Gain(D,a)=Ent(D)-\sum_{v=1}^{V} \frac{D^v}{D}Ent(D^v)=0.998-(\frac{6}{17}<em>1+\frac{6}{17}</em>0.918+\frac{5}{17}*0.722)=0.109$</p><p>也可以得到其他属性的信息增益，在根节点的位置信息增益最大的属性为纹理Gain=0.381.<br>确定第一个分支后，后面的内部节点和叶节点叶通过类似方式递归判断，则生成的决策树为:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%86%B3%E7%AD%96%E6%A0%91%E6%B1%87%E6%80%BB/3.png" alt=""></p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>ID3没有剪纸的策略，容易过拟合</li><li>没有考虑缺失值</li><li>只能处理离散分布的特征</li></ul><h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><p>C4.5在ID3的基础上做了一些改进:</p><ul><li>引入后剪枝缓解过拟合</li><li>可以处理属性中有连续值的情形，具体做法是对于某属性在区间$[a^i, a^{i+1})$，若取任意值产生的划分结果相同，则取该区间的中位点作为划分点</li><li>对于缺失值，C4.5的做法是用没有缺失的样本子集计算信息增益</li></ul><h3 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h3><p>剪枝的目的是防止决策树过拟合，在构建决策树时，为了能正确分类训练样本。决策树的结点划分过程会不断重复，可能导致分的过细，在训练集拟合的很好，但测试集的效果变得很差。所以需要剪枝帮助模型获得泛化性。<br>剪枝有<strong>预剪枝</strong>和<strong>后剪枝</strong>两种，引用下西瓜书里关于两种剪枝方式的差异</p><blockquote><p><strong>预剪枝</strong>是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;<strong>后剪枝</strong>则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点.</p></blockquote><p>判断上述两种剪枝方式是否能带来性能提升的方式是采用验证集，验证剪枝前后的效果。</p><p>预剪枝和后剪枝的例子见下面两图:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%86%B3%E7%AD%96%E6%A0%91%E6%B1%87%E6%80%BB/4.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%86%B3%E7%AD%96%E6%A0%91%E6%B1%87%E6%80%BB/5.png" alt=""></p><h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ul><li>C4.5使用的是多叉树，二叉树效率更高</li><li>只能用于分类</li><li>使用的熵模型有大量耗时的对数运算，连续值还有排序运算</li><li>在构造树的过程中，对数值属性值需要按照其大小排序，从中选择一个分割点。当训练集打到内存无法容纳时，会无法运行</li></ul><h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><h3 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h3><p>CART使用基尼指数作为划分属性的度量<br>$Gini(D)=\sum_{k=1}^{|y|}\sum_{k^{‘} \neq k} p_k p_{k^{‘}}=1- \sum_{k=1}^{|y|}p_k^2$</p><p>$Gini(D)$反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此$Gini(D)$越小，则数据集D的纯度越高</p><p>将其写成和信息增益等价的形式，则基尼指数为</p><p>$ Gini_index(D,a)=\sum_{v=1}^{V} \frac{D^v}{D} Gini(D^v) $</p><p>在是否要划分决策树的判断时，CART会选择使得划分后基尼指数最小的属性作为最优划分属性。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://oysz2016.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="决策树" scheme="https://oysz2016.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>因果图的思想优化长尾问题</title>
    <link href="https://oysz2016.github.io/post/3a13345e.html"/>
    <id>https://oysz2016.github.io/post/3a13345e.html</id>
    <published>2023-01-01T04:00:43.755Z</published>
    <updated>2023-01-01T04:21:49.320Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2009.12991">https://arxiv.org/abs/2009.12991</a><br><strong>代码链接:</strong> <a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch">https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch</a><br>这篇文章和之前有介绍过的一篇场景图生成的文章<a href="https://arxiv.org/abs/2002.11949">Unbiased Scene Graph Generation from Biased Training</a>思想比较类似, 作者也是同一个团队。贴一个<a href="https://oysz2016.github.io/post/b4822109.html">传送门</a>，方便感兴趣的同学浏览。<br>之所以说思想类似，是因为这两篇文章不仅都用来解决长尾问题，而且都用到了<strong>因果图</strong>的的思想。在看这篇论文的时候，有些内容读起来还是很难理解的，查阅了一些<strong>统计学</strong>和<strong>因果关系</strong>的相关概念才觉得清晰了些。这些理论也非常有意思，看完后我觉得在阅读这篇文章前还是很有必要学习的。在写这篇blog的时候，我也尽量把需要用到的背景知识整理出来，方便理解。</p><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><h3 id="因果关系"><a href="#因果关系" class="headerlink" title="因果关系"></a>因果关系</h3><p>关于因果关系，在<a href="https://zhuanlan.zhihu.com/p/111306353">https://zhuanlan.zhihu.com/p/111306353</a>中有详细的背景知识。为了方便阅读和加深自己的理解。我在这里精简的引用下，更详细的内容可以去阅读大佬的知乎文章。</p><blockquote><p>《The Book of Why》中将因果理论的探索类比成一个向上的阶梯，包含三个层级：Seeing，Doing和Imaging。<br><strong>第一层级: Association 关联</strong>，对应的是大多数机器学习算法和动物。该层级强调基于被动的观察（passive observation）来预测，通过观察来寻找规律，并非真正的因果。<strong>其本质就是条件概率P(Y|X)，在观察到X的条件下Y发生的概率，也是传统机器学习里被广泛应用的。</strong><br><strong>第二层级：Intervention干预</strong>, 干预指的是消除因果关系中的混杂影响，推导出真正的因果关系，如随机对照试验。<br><strong>第三层级：Counterfactual 反事实</strong>，counterfactual和干预intervention区分的关键在于“hindsight”(事后来看)，即反事实强调在对结果已知观测的基础上再对反事实的问题进行解答。</p><h3 id="因果推断中的变量"><a href="#因果推断中的变量" class="headerlink" title="因果推断中的变量"></a>因果推断中的变量</h3><p>因果推断中有一些专业术语用来表示在因果中不同变量的角色，主要分为<a href="https://zh.wikipedia.org/wiki/%E5%B9%B2%E6%93%BE%E5%9B%A0%E7%B4%A0">混淆变量(confounder)</a>，<a href="https://zh.wikipedia.org/wiki/%E4%B8%AD%E4%BB%8B%E8%AE%8A%E9%A0%85">中介变项(mediator)</a>，<a href="https://zh.wikipedia.org/wiki/%E5%B0%8D%E6%92%9E%E5%9B%A0%E5%AD%90">对撞因子(collider)</a><br><strong>混淆变量</strong>比较通俗的例子是老年人因为退休了会更有时间晨炼，但老年人却比年轻人更容易得癌症，如果不控制年龄的分布，就会得到晨炼的人容易得癌症。这里的年龄就是混淆变量，需要被控制<br><strong>中介效应</strong>指的是从一个变量到另一个变量，中间会有些其他的变量带来影响。比如吃药能带来疾病的好转，可能是药本身起了作用，也可能是心理安慰。比较极端的例子是是不是就会出现用面粉做假药的新闻，对于患者而言，药本身可能没起到作用，但可能由于吃了“药”，觉得自己马上会好，比较积极的心态带来了身体的好转。<br><strong>对撞因子</strong>指的是同时被两个以上变量影响的因素，而这些影响对撞因子的变量之间不见得有因果关系。例如在NBA球员中，会发现身高比较高的人得分率并没有很高，这是因为身高矮的人能进NBA必然是用其他优势弥补了劣势。<strong>身高</strong>和<strong>得分率</strong>之间并没有明显的因果关系，而他们都决定能不能<strong>进NBA</strong>。仔细思考就会发现对撞因子的例子很容易造成幸存者偏差。</p><h3 id="Propensity-Score"><a href="#Propensity-Score" class="headerlink" title="Propensity Score"></a>Propensity Score</h3><p><strong>倾向评分匹配(Propensity Score Matching)</strong> 是一种统计学的方法，指的是在<strong>观察研究中</strong>，由于种种原因，<strong>数据偏差（bias）</strong> 和 <strong>混杂变量（confounding variable）</strong> 较多，<strong>倾向评分匹配的方法正是为了减少这些偏差和混杂变量的影响，以便对实验组和对照组进行更合理的比较。</strong><br>为了能较好说明这个方法的价值，可以用理科里学的随机对照实验(Randomized Controlled Trial data)做对比。随机对照试验在样本量足够的情况下是很科学的评判变量对结果影响的实验方法，但很多时候是不符合科研伦理的，比如要研究吸烟是否有害健康，如果招收大量人员，然后随机分配到吸烟组和不吸烟组，这种实验设计不太容易实现，而且也存在危害测试人员健康的可能。而这个研究课题其实很容易通过<strong>观察的研究数据</strong>进行实验，面对观察的研究数据，如果不加调整，很容易获得错误的结论，比如拿吸烟组健康状况最好的一些人和不吸烟组健康状况最不好的一些人作对比，得出吸烟对于健康并无负面影响的结论。从统计学角度分析原因，这是因为观察研究并未采用随机分组的方法，无法基于大数定理的作用，在实验组和对照组之间削弱混杂变量的影响，很容易产生系统性的偏差。<strong>倾向评分匹配</strong>就是用来解决这个问题，消除组别之间的干扰因素。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>Re-Balanced Training:</strong> re-blance的方法主要有两种，分别是re-sampling和re-weighting。<br><strong>Hard Example Mining:</strong> 不关注于每个类别样本数量的先验分布，而是关注于难样本用于缓解长尾问题，代表方法是focal loss<br><strong>Transfer Learning/Two-Stage Approach:</strong> 作者总结的这类工作的特点是将头部类的knowledge转移到尾部类，用以改善长尾问题。其中比较有代表性的是<a href="https://arxiv.org/abs/1910.09217">Decoupling</a>算法，和受Decoupling启发的<a href="https://arxiv.org/abs/1912.02413">BBN</a>算法。BBN在之前的<a href="https://oysz2016.github.io/post/b4822109.html">关于长尾问题的文章</a>中也有过总结。<br><strong>Causal Inference:</strong> 因果图推理作者主要列举了一些这方面的著作，当然也提到了自己在场景图生成中的文章</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>作者将视角聚焦于梯度优化器中常用的track——<strong>Momentum</strong>。在讲解作者是怎么做的之前，让我们回顾下Momentum。Momentum的思想是累积一个历史的梯度信息用来加速优化器，好处主要有以下两点:</p><ul><li>每次梯度更新的时候，不仅考虑了当前梯度的方向，同时也考虑了之前更新的方向，在梯度优化时，不会抖动的那么随意</li><li>Momentum相当于给梯度优化的方向施加了一个惯性，参数优化时容易突破局部最优解，更可能找到全局最优解.</li></ul></blockquote><p><strong>上面是一些比较定性的分析，关于Momentum为什么能work?</strong><a href="https://distill.pub/2017/momentum/">https://distill.pub/2017/momentum/</a>做了非常好的可视化，可以直观的感受到Momentum为梯度优化带来的改变<br>下面的两张图分别是没有Momentum和使用合理参数设置Momentum对模型优化带来的差异，可以看到Momentum能提高网络训练的稳定性，并且同样的迭代次数更容易收敛到全局最优。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/1.png" alt=""><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/2.png" alt=""><br>回归正题，对于长尾分布的数据集，正是由于Momentum会受之前梯度信息的影响，Momentum所产生的惯性，会带来马太效应，即模型的优化方向会倾向于让模型对头部类的效果更好。<br>作者将Momentum对网络的作用用因果图抽象了出来，如figure 1(a)所示<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/3.png" alt=""><br>上图中，<strong>X代表backbone提取到的特征</strong>，<strong>M代表优化器中的动量</strong>，<strong>Y代表预测结果</strong>，<strong>D代表动量所产生的惯性，由于是长尾数据集，这里的D特指对头部类优化的惯性，而在balanced的数据集中，D对每个类别的贡献是一样的</strong>。图中的箭头表示彼此的影响，例如，X-&gt;Y以为着，Y的得出收到X的影响。从因果图中的关系可以看出<strong>节点M和节点D分别代表混淆变量(confounder)和中介效应(mediator)</strong>。<br><strong>M-&gt;X</strong>代表的是特征图X的是在动量M的影响下训练的,figure 1(b)中可视化了动量M对不同类别的影响，可以发现头部类在动量中占比较大。<br>知道了混淆变量和中介效应之后，需要做的就是<strong>消除这些变量对模型带来的偏见</strong>。和<a href="https://arxiv.org/abs/2002.11949">Unbiased Scene Graph Generation from Biased Training</a>中类似，接下来需要构建TDE(Total Direct Effect)用于消除偏见，公式如下:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/10_4.png" alt=""><br>和公式对应的因果图如Figure 3<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/4.png" alt=""><br>在计算TDE时需要抹除掉混淆变量M对X的影响，但没有办法得到M的分布。在<a href="http://bayes.cs.ucla.edu/jsm-july2012-pdf.pdf">Causal inference in statistics: A primer</a>的书中有提到<strong>Inverse Probability Weighting</strong>的公式，这个公式给出了一种思路，即没有办法得到M的分布时，可以看M和X有没有一一对应关系。在这个方法里，M和X确实是有对应关系的。所以可以将对X的采样看成是对M的近似<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/5.png" alt=""><br>这里将weights和features的通道/维度划分成k组，可以认为是做了k倍的细粒度采样。这样的好处是通过multi-head多重采样能更好的近似。<br>M能够做近似之后，还需要考虑 <strong>倾向评分(Propensity Score)</strong> 的影响，在这个问题中需要对所有类别做归一化的统一分布，也就是考虑每个类别的模长。下面就是得到的Propensity Score的公式, 其中第一项是类别感知的, 其中第一项是class-specific，第二项是class-agnostic。需要第二项的原因是因为从Figure 1(b)中可以看出x也具有bias。</p><script type="math/tex; mode=display">g(i, x^k; w^k_i)=||x^k|| \cdot ||w_i^k|| + \gamma||x^k||</script><p>则公式2中的第一项$P (Y = i|do(X = x))$可以表示为<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/6.png" alt=""><br>公式2中的第二项和第一项的区别在于使用了空数据$x_0$替代x。而其他项保持不变，这一部分是构建反事实的因果图，相当于让网络仅通过M和D得到Y，而x没起到作用。可以把这部分看作是偏差。<br>最终的TDE如下式：<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/7.png" alt=""></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>和其他方法的对比如下:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/8.png" alt=""><br>关于本文方法和其他方法的差异，可以用下图表示。</p><ul><li>在baseline的数据集上有问题是由于训练数据是长尾的，而测试数据是balanced的，存在<strong>分布不匹配</strong>。</li><li><strong>One-stage Re-balancing</strong>的方法本质是<strong>改变了训练数据的分布，这种方式会带来错误的模型建模</strong>；</li><li><strong>Two-stage Re-balancing</strong>的方法是第一阶段先通过原始的数据对模型建模，第二阶段再优化分类器，<strong>对分类器边界做调整</strong>，所以能work</li><li>而本文的方法是在测试的时候将分布做了移动<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/9.png" alt=""><br><strong>优势:</strong></li><li><strong>不需要复杂的stage训练方式</strong></li><li><strong>可适用于多个任务，如图片分类，检测之类</strong></li><li><strong>不需要依赖数据的分布, 感觉这个优势对于online的训练比较有意义, 因为其他的训练方式其实都可以获取到数据的数据分布。</strong><h2 id="笔者总结"><a href="#笔者总结" class="headerlink" title="笔者总结"></a>笔者总结</h2>要使用作者提供的 <a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch">https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch</a> 代码中的<strong>CausalNormClassifier</strong>总结起来有几个要点, 为方便理解，下面要点中的超链接会索引到具体的代码:</li><li>训练的时候需要用<a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch/blob/9726439a702614e99a02e2ba321ec4e56491239e/classification/models/CausalNormClassifier.py#L54">multi-head normalized classifier</a></li><li>训练时需要记录<a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch/blob/9726439a702614e99a02e2ba321ec4e56491239e/classification/run_networks.py#L215">移动的平均特征</a></li><li>测试的时候需要用<strong>counterfactual TDE inference</strong>，即<a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch/blob/9726439a702614e99a02e2ba321ec4e56491239e/classification/models/CausalNormClassifier.py#L43">去除具有头部类倾向的部分</a><br>让我们回顾上面的图7，在作者<a href="https://zhuanlan.zhihu.com/p/259569655">博客</a>的评论部分，有非常简洁的总结, 在这里引用下：<blockquote><ol><li>decouple两阶段都是在train过程中，一阶段长尾分布下训练representation + classifier；二阶段直接通过暴力resample来调整classifier。</li><li>de-confound也可以看做两阶段，一阶段在train过程中，通过重采样和normalized的措施来训练representation + classifier；二阶段放在了test过程中，用一阶段中统计的bias来缓解测试中的class bias，得到TDE。<br>这两篇文章都很巧妙的使用了因果图。虽然很多trick可解释性确实不太强，不过细细思考起来，一些算法流程对整体算法的影响还是比较make sence。私以为在如果算法框架中有些trick能带来收益的同时，也会让带来一些问题负面的影响，就很适合用因果图的思想消除负面影响，不过这确实很考验对于问题的抽象能力和对因果图的理解<blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></li></ol></blockquote></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="长尾优化" scheme="https://oysz2016.github.io/tags/%E9%95%BF%E5%B0%BE%E4%BC%98%E5%8C%96/"/>
    
    <category term="因果图" scheme="https://oysz2016.github.io/tags/%E5%9B%A0%E6%9E%9C%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>扩散模型汇总——从DDPM到DALLE2</title>
    <link href="https://oysz2016.github.io/post/d10097db.html"/>
    <id>https://oysz2016.github.io/post/d10097db.html</id>
    <published>2022-12-17T01:25:30.740Z</published>
    <updated>2022-12-25T09:20:29.682Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><p>扩散模型最早是在2015年在<a href="https://arxiv.org/abs/1503.03585">Deep unsupervised learning using nonequilibrium thermodynamics</a>中提出，<strong>其目的是消除对训练图像连续应用的高斯噪声</strong>，可以将其视为一系列<strong>去噪自编码器</strong>。它使用了一种被称为“潜在扩散模型”（latent diffusion model; LDM）的变体。训练自动编码器将图像转换为低维潜在空间。随后在2020年提出的DDPM将扩散模型的思想用于图像生成。</p><p><strong>生成模型</strong>: 给定来自感兴趣分布的观察样本x，生成模型的目标是学习对其真实数据分布$p(x)$进行建模<br><strong>隐变量(latent Variable)</strong>: 对于许多模态，我们可以将我们观察到的数据视为由相关的看不见潜在变量生成的，我们可以用随机变量 z 表示。为什么能用看不见的潜在变量表示，感性的理解可以参考柏拉图洞穴的寓言。在这个寓言中，一群人一生都被锁在一个山洞里，只能看到投射在他们面前的墙上的二维阴影，这是由看不见的三维物体在火前经过而产生的。对于这样的人来说，他们所观察到的一切，实际上都是由他们永远看不到的更高维度的抽象概念决定的。</p><p>生成模型发展到如今有下图中几种流派，从下面的算法结构图可以看出Diffusion model相比其他方法<strong>有比较大的不同:</strong></p><ul><li><strong>Diffusion model的算法过程中的latent Variable维度都是相同的</strong></li><li><strong>存在一个前向和反向的过程</strong></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/1.png" alt=""></p><p>从以上的不同出发，可以推测出Diffusion model包含两个过程：分别是<strong>前向过程</strong>和<strong>反向过程</strong><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/2.png" alt=""></p><h3 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h3><p>前向过程在论文中也称为<strong>扩散过程(diffusion process)</strong>，<strong>是向数据随机添加噪声，直至原始图像整个变成随机噪声的过程</strong>，这个过程记为$x_o~q(x_o)$<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/3.png" alt=""></p><h3 id="反向过程"><a href="#反向过程" class="headerlink" title="反向过程"></a>反向过程</h3><p>反向过程是<strong>前向过程的反转</strong>，反向过程的<strong>目的是将随机噪声的分布，逐渐去噪生成真实的样本。</strong>反向过程实际上也是生成数据的过程。将该过程的表达式为:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/4.png" alt=""></p><h2 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a><br><strong>代码链接:</strong> <a href="https://github.com/hojonathanho/diffusion">https://github.com/hojonathanho/diffusion</a></p><p>在DPM中原始的扩散模型，在反向过程中是用$x_{t-1}$ 预测 $x_{t}$，而<strong>DDPM预测的是从t时刻到t-1时刻添加的噪声</strong>，只要减去添加的噪声，同样也能得到$x_{t}$时刻的特征。</p><p>由于在从噪声恢复到目标图像的过程中，特征维度是一致的，在DDPM中采用的是U-Net的结构，在T步的反向过程中，U-Net模型是参数共享的，为了能告知U-Net模型现在是反向传播的第几步，在每一步反向传播时会增加一个<strong>time embedding</strong>，其实现和transformer中的position embedding相似</p><p>在反向过程中预测的噪声都是符合正态分布的，也就是只用拟合噪声的均值和方差就可以预测出噪声，在DDPM中将方差固定为常数，只预测均值</p><p>DDPM的原理到这里基本就介绍完了。关于论文中扩散模型正向和反向过程都是符合高斯分布的，可以做比较详尽的推理和证明，对正向和反向推理过程感兴趣的同学可以看看论文或者其他blog中的推导</p><h2 id="improved-DDPM"><a href="#improved-DDPM" class="headerlink" title="improved DDPM"></a>improved DDPM</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2102.09672">https://arxiv.org/abs/2102.09672</a><br><strong>代码链接:</strong> <a href="https://github.com/openai/improved-diffusion">https://github.com/openai/improved-diffusion</a><br>从论文的名字可以看出主要是对DDPM做的改进，<strong>主要介绍下和DDPM的差异</strong></p><ul><li>将DDPM中用常数指代的方差，用模型学习了</li><li>将添加噪声的schedule改了，从线性的改成了余弦的<br>作者发现DDPM中线性的噪声schedule在高分辨率的图像生成中表现较好，但对于分辨率比较低的，例如64<em>64和32</em>32的图像任务中表现的不那么好。特别是在扩散过程的后期，最后的几步噪声过大，对样本质量的贡献不大。从Figure 3可以看出，cosine schedule的方法在每一步添加的噪声后相比之前图片都有一些差异，而linear schedule方法，在后期几步差异已经不大了。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/5.png" alt=""><br>我也用coco的数据集试了下，确实cosine比linear要合理些。<br>下图展示的是前向过程，迭代不同次数的结果</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/7.png" alt=""></p><p>下面以mnist数据集为例，展示反向过程，迭代不同次数的结果</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/7_2.png" alt=""></p><h2 id="Diffusion-models-beat-GAN-on-image-Synthesis"><a href="#Diffusion-models-beat-GAN-on-image-Synthesis" class="headerlink" title="Diffusion models beat GAN on image Synthesis"></a>Diffusion models beat GAN on image Synthesis</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2105.05233">https://arxiv.org/abs/2105.05233</a><br><strong>代码链接:</strong> <a href="https://github.com/openai/guided-diffusion">https://github.com/openai/guided-diffusion</a></p><p><strong>和之前方法的差异</strong></p><ul><li><strong>从GAN的实验中得到启发，对扩散模型进行了大量的消融实验，找到了更好的架构更深更宽的模型</strong></li><li><strong>用了classifier guider diffusion</strong></li></ul><h3 id="网络结构消融实验"><a href="#网络结构消融实验" class="headerlink" title="网络结构消融实验"></a>网络结构消融实验</h3><p>文中使用的基础模型是U-Net加一个单头全局注意力模块，以FID为评价指标，在ImageNet128<em>128上进行消融实验。<br>作者从模型的<strong>宽度(channels)</strong>、<strong>深度(depth)</strong>、<strong>注意力头的数量(heads)</strong>、<strong>注意力的分辨率(attention resolutions)</strong>、<em>*使用BigGAN的上/下采样激活(BigGAN-up/downsample)</em></em>、调整残差连接的权重(rescale-resblock)等方面进行了消融实验</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/9.png" alt=""></p><p>从上表中可以看到，加宽和加深网络都能带来明显的提升，增加注意力头的数量、使用多分辨率组合的注意力模块比只使用单头单一分辨率更有助于提升模型表现，BigGAN的上下采样也能提升模型表现。<strong>唯独修改残差连接的权重没有带来提升。</strong></p><p>虽然增加深度能带来模型性能的提升，但也会增加训练时间，并且需要更长时间才能拟合到一个一般结构模型能达到的效果。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/10.png" alt=""></p><p>另外通过Table 1的实验，作者使用了Channels为128，2个残差块，高分辨率的attention和使用BigGan中的上下采样激活。进一步探究注意力头的数量和每个注意力头通道数间的关系<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/11.png" alt=""></p><p>通过上表的结果进一步证明，<strong>注意力头的数量能提升性能，但每个注意力头的通道数并不是越多越好</strong></p><h3 id="Classifier-Guidance"><a href="#Classifier-Guidance" class="headerlink" title="Classifier Guidance"></a>Classifier Guidance</h3><p>作者受目前GAN方法里通常会使用的类别信息辅助图像生成的原理启发，开发了一个<strong>将类别信息引入扩散模型中的方法Classifier Guidance Diffusion</strong>，这个方法通俗的说是会训练一个图片分类器，在扩散模型的生成过程中的中间的latend code会<strong>通过分类器计算得到一个梯度，该梯度会指导扩散模型的迭代过程</strong>。其实这一操作也比较make sense，有一个分类器的存在能更好的告诉U-Net的模型在反向过程生成新图片的时候，当前图片有多像需要生成的物体。<strong>有点类似GAN中存在一个判别器的意思</strong>。</p><p>在论文中提到使用Classifier Guidance的技术<strong>能更好的生成逼真的图像</strong>，同时能加速图像生成的速度。论文中也提到，通过使用Classifier Guidance的track会牺牲掉一部分的多样性，换取图片的真实性</p><h2 id="Classifier-Free-Diffusion-Guidance"><a href="#Classifier-Free-Diffusion-Guidance" class="headerlink" title="Classifier-Free Diffusion Guidance"></a>Classifier-Free Diffusion Guidance</h2><ul><li>论文链接: <a href="https://arxiv.org/abs/2207.12598">https://arxiv.org/abs/2207.12598</a></li></ul><p>这篇论文的主要贡献是优化了Openai在《Diffusion models beat GAN on image Synthesis》中提出的Classifier Guidance，<strong>在Classifier Guidance中提出用另外一个模型做引导，需要用预训练的模型或者额外训练一个模型。不仅成本比较高而且训练的过程是不可控的。</strong></p><p>而这篇论文的方法研究的是没有分类器，也可以用生成模型自己做引导，所以起名叫“Classifier-Free Diffusion Guidance”。具体来说在该方法中联合训练了conditional和unconditional的扩散模型，并且结合了两个模型的score estimates，以实现样本质量和多样性之间的均衡。下图的Algorithms1和Algorithms2详细描述了Classifier-Free的做法。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/12.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/13.png" alt=""></p><p>最终模型的输出为下面的公式:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/14.png" alt=""><br>最终的输出为有条件生成的输出减去无条件生成的输出，看到这里个人感觉和之前介绍的两篇关于因果分析的论文<a href="https://oysz2016.github.io/post/b4822109.html">《Unbiased Scene Graph Generation from Biased Training》</a>和<a href="https://oysz2016.github.io/post/3a13345e.html">《Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect》</a>思路十分相似，<strong>可以将无条件生成的输出看作是偏差，用正常训练的网络减去有偏差的网络能得到想要的输出</strong>。回到这篇论文的思路，有条件生成的可以看作是用了和图片匹配的文本对c，而无条件生成将其中的文本对c置为了空集。其中w是超参数，用来条件有条件和无条件生成两者的比例，实验部分有该参数对性能的详细对比，以及和其他方法的对比</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/15.png" alt=""></p><p><strong>从下面的结果图可以看出Classifier-Free Guidance相比不用non-guided的方法多样性会有些损失，但图像的真实性和色彩饱和度是要更好的</strong></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/16.png" alt=""></p><p>值得一提的是，虽然Classifier-Free Guidance的方法没有引入新的模型，但方法本身仍然是”<strong>昂贵的</strong>“，因为训练的时候需要生成两个输出。在扩散模型本身就很慢的情况下，会进一步增加耗时</p><h2 id="GLIDE"><a href="#GLIDE" class="headerlink" title="GLIDE"></a>GLIDE</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2112.10741">https://arxiv.org/abs/2112.10741</a><br><strong>代码链接:</strong> <a href="https://github.com/openai/glide-text2im">https://github.com/openai/glide-text2im</a><br>在前面扩散模型的一系列进展之后，尤其是当guidance技术之后证明扩散模型也能生成高质量的图像后。Openai开始探索文本条件下的图像生成，并在这篇论文里对比了两种不同的guidance策略，分别是通过<strong>CLIP引导</strong>和<strong>classifier-free的引导</strong>。验证了classifier-free的方式生成的图片更真实，与提示的文本有更好的相关性。并且使用classifier-free的引导的GLIDE模型在35亿参数的情况下优于120亿参数的DALL-E模型</p><p>该方法沿袭了Openai一贯的做法，什么模块效果好就用什么，然后进一步增加模型的参数量和数据量。具体而言:</p><ul><li>使用了更大的模型，其中模型的结构和《Classifier-Free Diffusion Guidance》方法中的模型结构一样，不过增大了通道数，参数量达到了35亿</li><li>更多的数据，和Dalle相同的图像-文本对</li><li>更充分的训练，2048的batch size，迭代了250万次</li></ul><p><strong>GLIDE最大的贡献是开始用文本作为条件引导图像的生成</strong>，下图是其训练过程，和之前工作差异主要有以下几点：</p><ul><li><strong>分词后将文本送入transformer（bert），生成文本的embedding</strong></li><li><strong>文本embedding中最后一个token的特征作为扩散模型中classifier-free guidance中的条件c</strong></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/17.gif" alt=""></p><p>GLIDE的效果确实十分惊艳，图片非常真实而且有很多细节。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/18.png" alt=""></p><h2 id="DALLE2"><a href="#DALLE2" class="headerlink" title="DALLE2"></a>DALLE2</h2><p><strong>论文链接:</strong> <a href="https://cdn.openai.com/papers/dall-e-2.pdf">https://cdn.openai.com/papers/dall-e-2.pdf</a><br><strong>代码链接:</strong> <a href="https://github.com/lucidrains/DALLE2-pytorch">https://github.com/lucidrains/DALLE2-pytorch</a><br>如果说前面所提到的方法将扩散模型优化到比同期gan模型指标还要好，让研究人员看到了扩散模型在生成领域的前景，那么Dalle2则将扩散模型引入了公众视野。</p><p>在GLIDE取得成功之后，Openai又进一步在GLIDE上加了一些track，成为了Dalle2。dalle2的结构如下图所示：</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/19.png" alt=""></p><p>上面的图中将有一条分割线，分割线的上半部分代表CLIP模型，下半部分代表DALLE2。<br>为了更好的理解DALLE2，回顾一下CLIP模型，CLIP模型是有一个图像-文本对，文本通过一个text encoder得到文本特征，图像通过image encoder得到图像特征。他们两者就是一对正样本，而该文本跟其他的图像就构成负样本。<br>在Dalle2中CLIP模型没有经过进一步的训练，主要用处是用来根据文本生成文本特征，然后prior根据文本特征生成对应的图像特征，<strong>这一步很有意思，在论文中作者认为显式将图像特征建模出来，再用图像特征生成图像，会比直接通过文本特征生成图像效果要好。</strong></p><p>方法:<br>dalle2使用的数据和CLIP，Dalle，GLIDE一样，都是图像文本对(x，y)。x代表图像，y代表图像对应的文本，$z_i$代表CLIP模型输出的图像特征，$z_t$代表CLIP模型输出的文本特征。则Dalle2的网络结构由两部分组成:</p><ul><li>prior: 根据文本y生成图像特征$z_i$</li><li>decoder: 使用prior生成的$z_i$(对应的文本y，y可有可无)，生成图像x</li></ul><script type="math/tex; mode=display">P(x|y)=P(x，z_i|y)=P(x|z_i，y)P(z_i|y)</script><p>作者用公式证明了可以通过两阶段方式生成图像的原因。$P(X|Y)$代表要用文本生成图像，可以等价于$P(x，z_i|y)$，因为可以认为x和图像特征$z_i$是一一对应的，可以根据链式法则等价于$P(z_i|y)$。</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder部分的模型结构和GLIDE基本一致，使用了CLIP模型作为guidance，也使用了classifier-free guidance，并且classifier-free中的guidance有两种，一种是CLIP模型，另外一种是文本。<br>使用了级联式的生成，即生成的图片先从64<em>64到256</em>256，再到1024*1024</p><h3 id="Prior"><a href="#Prior" class="headerlink" title="Prior"></a>Prior</h3><p>prior模型的任务是从文本特征生成图像特征，这里有两种常用的方法</p><ul><li>Auto-regressive</li><li>扩散模型</li></ul><p>但是自回归的模型训练效率比较低，所以DALLE2的方法中使用的是扩散模型。在prior模型里也是用到了classifier-free guidance。在模型实现上使用的是transformer的decoder，模型的输入非常多，包含文本、CLIP的text embedding、扩散模型中常见的time step的embedding，还有加过噪声之后CLIP的image embedding；输出则预测没有噪声的CLIP的image embedding。和之前扩散模型不一样的地方在于没有使用DDPM中预测噪声的方式，而是直接还原每一步的图像</p><p>最后再贴一贴惊艳的效果<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/20.png" alt=""></p><h2 id="笔者总结"><a href="#笔者总结" class="headerlink" title="笔者总结"></a>笔者总结</h2><p>上面总结了各种各样的扩散模型，总体来说扩散模型的可解释性比GAN要好很多，也有很多数学公式可以证明。发展到如今，扩散模型慢慢的接替了GAN在生成领域头把交椅的地位。并且随着Dalle2的提出确实带来了无穷的想象力。在人工智能没有普及的年代，就有讨论随着人工智能的发展有哪些职业会被取代，但是基本上大家都觉得创造性的工作，是无法被取代的，因为创造性的工作没有固定的模式，大多数需要灵感。但目前来看扩散模型已经具有了一定的创造性，相信对大部分的认知都是有一定的冲击，也说明扩散模型确实是一个很有趣的研究方向。</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="扩散模型" scheme="https://oysz2016.github.io/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="生成模型" scheme="https://oysz2016.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>EDU。</title>
    <link href="https://oysz2016.github.io/post/2c662fdb.html"/>
    <id>https://oysz2016.github.io/post/2c662fdb.html</id>
    <published>2022-12-07T02:04:37.899Z</published>
    <updated>2022-12-04T02:22:04.833Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="053a8f692f6e720d6e9784b4a87b6ed7ebbc5a60433d3a26cc95a253d9d1ee8a">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">Here&#39;s something encrypted, password is required to continue reading.</summary>
    
    
    
    <category term="随笔" scheme="https://oysz2016.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="https://oysz2016.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>《VQ-VAE》论文笔记</title>
    <link href="https://oysz2016.github.io/post/d1dddfb8.html"/>
    <id>https://oysz2016.github.io/post/d1dddfb8.html</id>
    <published>2022-11-27T02:12:48.415Z</published>
    <updated>2022-12-05T01:10:19.851Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>论文链接: <a href="https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf</a></p><p>论文代码: <a href="https://github.com/ritheshkumar95/pytorch-vqvae">https://github.com/ritheshkumar95/pytorch-vqvae</a></p><p>VQ-VAE(Vector Quantised- Variational AutoEncoder)和VAE(TODO)一样也是生成模型。虽然文中作者给自己的模型取名为VQ-VAE，但实际上和VAE的关系不太太，其模型其实是基于自编码器AE</p><h1 id="Pixel-CNN"><a href="#Pixel-CNN" class="headerlink" title="Pixel CNN"></a>Pixel CNN</h1><p>既然思想是基于自编码器AE，那就得追溯到自编码生成模型的代表作Pixel CNN了，Pixcel CNN是Google Deep Mind在2016年提出的。其思想是通过前面的一些数值，得到当前数值的分布，其预测方式为:<br>$p(x)=\prod_{i=1}^{n^{2}}p(x_i|x_1,…,x_{i-1})$</p><p>每个像素是一个256分类的问题，且每个像素的分类，依赖于之前像素的信息。以cifa10数据集为例，图像大小为32<em>32</em>3，可以将图片看成序列，则长度为32<em>32</em>3=3072。生成cifa图片，需要对3072的序列按seq2seq的方式推理。<br>基于Pixel CNN的思想，由于模型用到的是CNN，为了实现在推理当前像素时能有效的遮住还未看到的信息。其提出的方式是<strong>Gate Convolutions Layers</strong>层，其思想如下图所示：<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VQ-VAE/WX20221012-200838%402x.png" alt=""><br>简言之就是对卷积做mask处理，生成一个n*n的卷积，按照从上到下，从左到右的顺序，将卷积中心及之后的特征值置为0，而其他位置置为1，保证卷积操作只能看到该像素之前的像素</p><p>Pixel CNN等自回归模型的缺点:</p><ul><li>模型耗时较长，对于分辨率稍微高点的图像，自回归模型需要逐像素推理才能还原出图像</li><li>图像的像素时很冗余的，这一思想在最近的很多论文中都有论证，如<a href="https://arxiv.org/abs/2111.06377">MAE</a>。虽然图像中每个像素时离散的，但事实上连续的像素时相近的，有时RGB值差个别数值，并不影响图像的生成，而转变成像素分类问题，只有非对即错的结果</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>VQ-VAE的论文确实写的比较难懂，而苏神的blog则要清晰非常多，关于算法原理这块推荐大家可以读读苏神的blog</p><p>针对自回归模型存在的缺点，VQ-VAE的思想是先对图像降维，再对编码向量用Pixel CNN的方式建模。这种i想具有如下的坑:</p><ul><li>因为Pixel CNN建模使用的离散的序列，就意味着VQ-VAE降维度的时候需要转换为离散的序列。其实自编码器就是很常用的降维方法之一，然而其生成的编码向量都是连续的</li><li>降维后的特征和原始特征存在差异，求梯度的时候不能用原始特征和gt比对，因为优化目标已经变成了降维后的特征。也不能直接用降维后的特征，因为VQ-VAE中的降维实际上是映射到编码表，这个过程是不能求梯度的</li></ul><h2 id="离散化"><a href="#离散化" class="headerlink" title="离散化"></a>离散化</h2><p>在VQ-VAE中，一张图片x会先经过encoder，得到连续的变量z</p><script type="math/tex; mode=display">z=encoder(x)</script><p>这里的z是一个大小为d的向量，VQ-VAE还维护一个Embedding层，我们也可以称为编码表，记为</p><script type="math/tex; mode=display">E=(e_1, e_2,..., e_k)</script><p>其中每一个$e_i$都是大小为d的向量，接着，VQ-VAE通过最邻近搜索，将z映射为这K个向量之一：</p><script type="math/tex; mode=display">z=e_k, k=argmin||e_j||_2</script><p>将z映射到编码表后的特征记为$z_q$, 则$z_q$才是编码后的结果，会将$z_q$传给decoder做生成，这样以来就将连续的特征z转变为了降维后的离散特征$z_q$<br><strong>上面的流程实际上是简化的</strong>，如果只编码一个向量，重构时容易出现失真，而且泛化性一般，因此实际编码时直接用多层卷积将x编码为m×m个大小为d的向量，也就是说，z的总大小为m×m×d，它依然保留着位置结构，然后每个向量都用前述方法映射为编码表中的一个，就得到一个同样大小的$z_q$，然后再用它来重构。这样一来，$z_q$也等价于一个m×m的整数矩阵，这就实现了离散型编码。</p><h2 id="前向和反向传播"><a href="#前向和反向传播" class="headerlink" title="前向和反向传播"></a>前向和反向传播</h2><p>如果是普通的自编码器，直接用下述loss训练即可:</p><script type="math/tex; mode=display">||x-decoder(z)||^2_2</script><p>但是$z_q$并不是原来的z, 可就算换成$z_q$也不能计算梯度，换言之，我们的目标其实是$‖x−decoder(z_q)‖_2^2$最小，但是却不好优化，而$||x-decoder(z)||^2_2$容易优化，但却不是我们的优化目标。那怎么办呢？当然，一个很粗暴的方法是两个都用：</p><script type="math/tex; mode=display">||x-decoder(z)||^2_2+‖x−decoder(z_q)‖_2^2</script><p>但这样并不好，因为decoder(z)并不是优化目标，会带来额外的约束</p><p>VQ-VAE中用了一个巧妙且直接的方法，称为Straight-Through Estimator，你也可以称之为“直通估计”。最早源于论文<a href="https://arxiv.org/abs/1308.3432">Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation</a></p><p>Straight-Through Estimator的思想很简单，就是前向传播的时候可以用想要的变量(哪怕不可导)，而反向传播的时候用你设计的梯度去替代。根据这个思想，我们设计的目标函数是：</p><script type="math/tex; mode=display">||x-decoder(z+sg[z_q-z])||^2_2</script><p>其中sg是stop gradient的意思，就是不要它的梯度。这样一来，前向传播计算（求loss）的时候，就直接等价于$decoder(z+z_q−z)=decoder(z_q)$.然后反向传播（求梯度）的时候，由于$z_q−z$不提供梯度，所以它也等价于decoder(z)，这个就允许我们对encoder进行优化了。</p><h2 id="维护编码表"><a href="#维护编码表" class="headerlink" title="维护编码表"></a>维护编码表</h2><p>上面我们提到离散化是通过编码表映射完成，期望是映射后的$z_q$和$z$相近，不然仍然会导致生成的图像失真严重，因为离散化其实是在做量化，而量化的目的是减少计算量的同时，尽量不损失精度。由于编码表E是相对自由的，而z要尽力保证重构效果，所以我们应当尽量“让$z_q$去靠近$z$”而不是“让z去靠近$z_q$”。而因为$‖z_q−z‖^2_2$的梯度等于对zq的梯度加上对z的梯度，所以我们将它等价地分解为:</p><script type="math/tex; mode=display">||sg[z]-z_q||^2_2+||z-sg[z_q]||^2_2</script><p>第一项相等于固定$z$，让$z_q$靠近$z$，第二项则反过来固定$z_q$，让$z$靠近$z_q$。注意这个“等价”是对于反向传播（求梯度）来说的，对于前向传播（求loss）它是原来的两倍。根据我们刚才的讨论，我们希望“让$z_q$去靠近$z$”多于“让$z$去靠近$z_q$”，所以可以调一下最终的loss比例：</p><script type="math/tex; mode=display">||x-decoder(z+sg[z_q-z])||^2_2+\beta||sg[z]-z_q||^2_2+\gamma||z-sg[z_q]||^2_2</script><p>其中$\gamma&lt;\beta$，在原论文中使用的是$\gamma=0.25\beta$</p><h2 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h2><p>经过上面的离散化之后，将图片编码为$m*m$的整数矩阵。该矩阵也一定程度保留了原始图片的信息，可以使用自回归模型如Pixel CNN，对编码矩阵拟合。</p><p>通过Pixel CNN得到编码分布后，可以随机生成一个新的编码矩阵，然后通过编码表E映射为浮点数$z_q$,最后经过decoder得到一张图片.</p><p>一般来说，得到的<script type="math/tex">m*m</script> 比原来的<script type="math/tex">n*n*3</script>要小的多，因此计算也更快速</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生成网络" scheme="https://oysz2016.github.io/tags/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>《VAE》论文笔记</title>
    <link href="https://oysz2016.github.io/post/63606c55.html"/>
    <id>https://oysz2016.github.io/post/63606c55.html</id>
    <published>2022-11-20T02:12:26.984Z</published>
    <updated>2022-12-05T01:10:15.363Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>论文链接: <a href="https://arxiv.org/pdf/1312.6114.pdf">https://arxiv.org/pdf/1312.6114.pdf</a></p><p>论文代码: <a href="https://github.com/devnag/pytorch-generative-adversarial-networks">https://github.com/devnag/pytorch-generative-adversarial-networks</a></p><h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><h2 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h2><p>auto-encoder是一种无监督的算法，自编码器是一个输入和学习目标相同的神经网络，其结构分为编码器和解码器两部分。其思想是输入x经过encoder生成hidden layer特征z，再将z经过decoder重新预测生成$x^{‘}$<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VAE/v2-ec10f5dbd7197120cb87eaee5dc04e9a_b.png" alt=""></p><h2 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h2><p>VAE和GAN一样都属于生成模型，即从训练数据建模真实数据的分布，再反过来用学到的模型生成新的数据。这一类模型是假设有一个数据集$X=\{x^{(i)}\}_{i=1}^N$, 理想情况下是用$x_i$拟合函数p(x), 通过p(x)能得到$X$之外的数据，但这是理想情况，GAN和VAE采用了不同的方式达到生成模型的效果，之前GAN的论文笔记中有梳理过，GAN是一个对抗网络，通过判别器来判断生成器产生数据的效果。而VAE的方式在下面会详细介绍</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>VAE的思想是对于一个真实样本$x_k$, 假设存在后验分布$P(Z|x_k)$和$x_k$是一一对应的，$P(Z|x_k)$是一个正态分布图分布，能知道该正态分布的均值和方差，就能将其还原回X，在VAE中$P(Z|x_k)$的均值和方差是通过模型计算得到。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VAE/WX20221010-203445@2x.png" alt=""><br>为了使模型具有生成能力，VAE希望$p(Z_x)$趋向于正态分布<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VAE/WX20221010-204114@2x.png" alt=""></p><p>在VAE中实际上也有两个encoder，分别用来计算均值和方差，在encoder计算的损失中会加入KL Loss，实际上是给encoder部分增加正则项，希望encoder的均值为0.VAE的损失函数会倾向于在训练初期，会降低噪声(KL loss增加)，使得拟合更容易，而在decoder训练的不错时，会增加噪声(KL loss减少)，使得拟合更困难</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>实际上VAE中也有对抗的过程，因为decoder的部分希望没有噪声，而KL loss希望有高斯噪声，两者在训练的时候实际上是对立的。VAE和GAN两种方式实际上各有优缺点，GAN在训练时不稳定，而VAE生成的图像相对GAN会模糊些</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生成网络" scheme="https://oysz2016.github.io/tags/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>NER&amp;RE联合抽取汇总</title>
    <link href="https://oysz2016.github.io/post/f45dd794.html"/>
    <id>https://oysz2016.github.io/post/f45dd794.html</id>
    <published>2022-11-13T02:12:09.430Z</published>
    <updated>2022-12-05T01:10:03.898Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><h1 id="背景"><a href="#背景" class="headerlink" title="背景:"></a>背景:</h1><ul><li>NER任务: 预测文本中具有特定意义的实体，如人名，地名等</li><li>RE任务: 多分类任务，通过NER得到了实体之后，预测任意两个实体存在怎样的关系 </li></ul><p><strong>实体关系抽取可以分为两类方法:</strong></p><ul><li>pipeline models:可以任意组合不同的模型和数据，但关系模型使用实体模型的的预测结果作为其输入，会导致实体预测的错误传播到关系预测模型中</li><li>joint models:在一个模型中同时完成实体和关系抽取的任务,增强实体和关系的信息交互</li></ul><p><strong>关系抽取需要考虑的问题:</strong><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/1.PNG" alt="enter description here"></p><ul><li>SEO: (Single Entity Overlap)，一个实体与多个其他实体有关系</li><li>EPO(Entity Pair Overlap):  两个实体之间有多个关系</li><li>SOO(Subject Object Overlap)/HTO)(Head Tail Overlap): subject和object存在嵌套的情形</li></ul><h1 id="CsRel-A-Novel-Cascade-Binary-Tagging-Framework-for-Relational-Triple-Extraction"><a href="#CsRel-A-Novel-Cascade-Binary-Tagging-Framework-for-Relational-Triple-Extraction" class="headerlink" title="CsRel:A Novel Cascade Binary Tagging Framework for Relational Triple Extraction"></a>CsRel:A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</h1><p><strong>论文:</strong> <a href="https://arxiv.org/pdf/1909.03227.pdf">https://arxiv.org/pdf/1909.03227.pdf</a><br><strong>代码:</strong> <a href="https://github.com/weizhepei/CasRel">https://github.com/weizhepei/CasRel</a><br>CasRel:</p><ul><li>对关系三元组联合建模</li><li>通过下面的公式，可以将句子中含有(s,r,o)三元组的最大似然估计转换为先提取s后，在关系r的前提下，提取对应的o<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/2.PNG" alt="enter description here"></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/3.PNG" alt="enter description here"></p><p>Cascade: 先抽取subject, 再抽取对应的关系和object。整个网络分成两步:</p><ul><li>先抽取subject，再抽取对应的关系和object</li><li>对于每一个关系，都要做对应关系的object抽取，如果有N个关系，则有2N个序列</li></ul><p><strong>subject tagger</strong></p><p>采用两个单独的二分类器分别检测subject实体的开始和结束，具体做法是经过bert后输出两个序列，start序列将实体的头token标记为1，end序列将实体的尾token标记为1。如果一个句子中检测出多个实体，则采用nearest的原则解码</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/4.PNG" alt="enter description here"></p><p><strong>relation-specific object taggers</strong><br>将subject的信息作为先验信息，带入到object和关系的抽取中。由于每个span的宽度不同，为了保证x和v的维度一致，需要将subject做max pooling</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/5.PNG" alt="enter description here"></p><h1 id="TPLinker-Single-stage-Joint-Extraction-of-Entities-and-Relations-Through-Token-Pair-Linking"><a href="#TPLinker-Single-stage-Joint-Extraction-of-Entities-and-Relations-Through-Token-Pair-Linking" class="headerlink" title="TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking"></a>TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</h1><p><strong>论文:</strong> <a href="https://arxiv.org/pdf/2010.13415.pdf">https://arxiv.org/pdf/2010.13415.pdf</a><br><strong>代码:</strong> <a href="https://github.com/131250208/TPlinker-joint-extraction">https://github.com/131250208/TPlinker-joint-extraction</a></p><p>之前工作存在的问题</p><ul><li>曝光偏差(exposure bias): 在训练时，grouth truth token作为上下文，训练object和关系。而在预测时，用预测的subject作为输入，导致了训练和测试时的偏差</li><li>误差传播: 错误不可逆有的传播，如果subject未抽取到，则object和关系也将预测不出来</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/6.PNG" alt="enter description here"></p><p>TPLinker通过三种类型的span矩阵抽取实体关系三元组，N是序列长度，R是关系的总数</p><ul><li>EH-to-ET: 表示实体的头尾关系，1个N*N的矩阵。如两个实体：New York City:M(New, City) =1; De Blasio:M(De, Blasio) =1，在上图中紫色标记。</li><li>SH-to-OH: 表示subject和object的头部token间的关系，是R个N*N矩阵；如三元组(New York City, mayor,De Blasio):M(New, De)=1，在上图中位红色标记。</li><li>ST-to-OT: 表示subject和object的尾部token间的关系，是R个N*N的矩阵；如三元组(New York City, mayor,De Blasio):M(City, Blasio)=1，在上图中为蓝色标记。</li></ul><p>共有2R+1个矩阵，为了防止稀疏计算，下三角矩阵不参与计算。实体标注不会产生下三角矩阵，但关系可能会存在，若关系矩阵存在于下三角，则将其转置到上三角，并由标记1转换为标记2</p><p><strong>解码过程</strong></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/7.PNG" alt="enter description here"></p><ol><li>解码EH-to-ET可以得到句子中所有的实体，用实体头token idx作为key，实体作为value，存入字典D中；得到三个实体{New York,New York City,De Blasio};则D={New:(New York,New York City),De:(De Blasio)}</li><li>对每种关系r，解码SH-to-OH得到token对并在D中关联其token idx的实体value；以关系mayor为例，解码SH-to-OH得到(De,New)，关联到D可以知道subject实体为(De Blasio), object实体为(New York, New York City)</li><li>解码ST-to-OT得到E=(City,Blasio), 关联上面的到的subject和object集合可以确认subject为(De Blasio),object为New York City</li></ol><h1 id="PRGC-Potential-Relation-and-Global-Correspondence-Based-Joint-Relational-Triple-Extraction"><a href="#PRGC-Potential-Relation-and-Global-Correspondence-Based-Joint-Relational-Triple-Extraction" class="headerlink" title="PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction"></a>PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction</h1><p><strong>论文:</strong> <a href="https://arxiv.org/pdf/2106.09895.pdf">https://arxiv.org/pdf/2106.09895.pdf</a><br><strong>代码:</strong> <a href="https://github.com/hy-struggle/PRGC">https://github.com/hy-struggle/PRGC</a><br><strong>CsRel缺点:</strong> </p><ul><li>每个subject需要判断大量冗余的关系</li><li>每次只能处理一个subject，工程效率不太友好</li></ul><p><strong>TPLinker缺点:</strong></p><ul><li>构建了大量的关系矩阵，导致标签稀疏和收敛速度慢</li><li>同样存在关系冗余</li></ul><p><strong>将实体和关系抽取建模成三个子任务:</strong></p><ul><li>关系判断(Relation Judgement): 输入为句子的特征向量h,  输出是长度为r的向量。判断句子中可能存在的关系。</li><li>实体提取(Entity Extraction): 输入是句子的特征向量h和可能存在的关系$R_{pot}$。 对于每个候选关系，采用softmax进行两次三分类，第一次确定头实体的BIO标签，第二次确定尾实体的BIO标签。在抽取subject和object时，用到了关系的特征。将关系向量加到对应token向量上，经过全连接层、softmax得到分类概率。</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/8.png" alt="enter description here"></p><ul><li>主宾语对齐(Subject-object Alignment):输入是句子的特征向量。构建二维矩阵$M\in R^{n*n}<script type="math/tex">,</script>M_{i,j}$存储的是subject实体首词为第i个token，object实体首词为第j个token的概率。由于通过实体提取获取了句子中在每个关系$r_{i}$相应的实体，则只要subject和object实体的首词能匹配上，则对应的实体也能匹配上<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/9.png" alt="enter description here"></li></ul><h1 id="OneRel-Joint-Entity-and-Relation-Extraction-with-One-Module-in-One-Step"><a href="#OneRel-Joint-Entity-and-Relation-Extraction-with-One-Module-in-One-Step" class="headerlink" title="OneRel: Joint Entity and Relation Extraction with One Module in One Step"></a>OneRel: Joint Entity and Relation Extraction with One Module in One Step</h1><p><strong>论文:</strong> <a href="https://arxiv.org/pdf/2203.05412">https://arxiv.org/pdf/2203.05412</a><br><strong>代码:</strong> <a href="https://github.com/ssnvxia/OneRel">https://github.com/ssnvxia/OneRel</a><br>TPLinker存在的问题:</p><ul><li>在构建实体和关系时，引入了1+2R个矩阵，即一个矩阵用来抽取关系，2R个矩阵抽取subject和object实体对头之间的关系，R个矩阵抽取是梯队尾之间的关系。存在较多冗余的信息，而且忽略了三元组中关系实体、头实体和尾实体相互的关系<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/10.png" alt="enter description here"></li></ul><p>提出单模块，单步解码的实体关系联合抽取方法</p><ul><li>Multi-Module Multi-Step: 实体和关系分别建模，串行多步解码，会存在误差传递</li><li>Multi‐Module One‐Step: 实体和关系分别建模，单步解码，最后组装成三元组。存在冗余计算</li><li>One‐Module One‐Step: 用单个模块直接建模头实体，关系，尾实体<br>使用token-pair的方式，用4个标记类型建模三元组</li><li>HB-TB：头实体的开始token与尾实体的开始token 进行连接。</li><li>HB-TE：头实体的开始token与尾实体的结束token 进行连接。</li><li>HE-TE：头实体的结束token与尾实体的结束token 进行连接。</li><li>-：不存在连接关系。</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/11.png" alt="enter description here"></p><p>相比TPLinker构建的矩阵数量由1+2R个，减少到R个<br>在(New York State, Contains, New York City)的三元组中, 关系<script type="math/tex">M_r=contains</script>的矩阵中。解码时通过通过HB-TE,HE-TE可以得到头实体New York State，HB-TB, HB-TE可以得到尾实体New York City</p><h1 id="UniRE-A-Unified-Label-Space-for-Entity-Relation-Extraction"><a href="#UniRE-A-Unified-Label-Space-for-Entity-Relation-Extraction" class="headerlink" title="UniRE: A Unified Label Space for Entity Relation Extraction"></a>UniRE: A Unified Label Space for Entity Relation Extraction</h1><p><strong>论文:</strong> <a href="https://arxiv.org/abs/2107.04292">https://arxiv.org/abs/2107.04292</a><br><strong>代码:</strong> <a href="https://github.com/Receiling/UniRE">https://github.com/Receiling/UniRE</a></p><ul><li>之前联合学习的方法仍然使用各自的标签空间，并没有将标签空间联合起来</li><li>将sequence labeling调整为关系抽取任务，将NER和RE两个任务放在同一个标签空间进行处理</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/12.png" alt="enter description here"></p><ul><li>引入词对关系表，将实体和关系通过该表完整的表示出来（PER：人名实体，GPE：地理位置实体，PER-SOC: 社会关系，ORG-AFF：机构附属关系， PHYS：位置临近关系</li><li>正向关系：表的上三角部分。人名实体 David Perkins 对地理位置实体 California 存在位置临近关系 PHYS ，那 David 对California，Perkins 对 California 都具有 PHYS 关系；</li><li>逆向关系：表的下三角部分。人名实体 doctors 对地理位置实体 village 存在隶属关系 ORG-AFF ，那 doctors 对 village具有 ORG-AFF 关系；</li><li>无向关系：和表对角线对称的。两个人名实体 David Perkins 和 wife 之间存在社会关系 PER-SOC ，这被分解成两个对称关系，David Perkins 对 wife 的正向关系和 wife 对 David Perkins 的逆向关系。实体也可以看做无向的关系，例如，David Perkins 是一个人名实体，那 David 对 David ，David 对Perkins，Perkins 对 David ，Perkins 对 Perkins 都具有标签为 PER 的关系。</li></ul><p>模型的训练转换为预测任意两个单词之间的关系，采用双仿射变换</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/13.png" alt="enter description here"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/14.png" alt="enter description here"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/15.png" alt="enter description here"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/16.png" alt="enter description here"></p><p>损失函数为交叉墒损失</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/17.png" alt="enter description here"></p><p>对该建模的一些约束</p><ul><li><p>对称性: 对称关系具有如下性质，<script type="math/tex">(e_1,e_2,l)</script>和<script type="math/tex">(e_2,e_1,l)</script>是等价的。因此满足该条件的两个关系关于对象线是对称的。实体和无向关系关于对角线是对称的。因此，这些标签对应的概率分数应该关于对角线对称。将标签分为y分为对称标签<script type="math/tex">y_{sym}</script>(实体和无向关系)和非对称标签<script type="math/tex">y_{asym}</script>。对于对称标签，具有如下约束:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/18.png" alt="enter description here"></p></li><li><p>蕴含性: 给定一个关系，则参与关系的必定是两个实体。相反，给定两个实体，他们两个之间不一定存在关系。则关系的概率分布应该不大于参与该关系的两个实体的概率分布</p></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/19.png" alt="enter description here"></p><p>Max为hinge loss。最终的loss</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/NER%26RE%E8%81%94%E5%90%88%E6%8A%BD%E5%8F%96/20.png" alt="enter description here"></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://oysz2016.github.io/tags/NLP/"/>
    
    <category term="NER" scheme="https://oysz2016.github.io/tags/NER/"/>
    
    <category term="RE" scheme="https://oysz2016.github.io/tags/RE/"/>
    
  </entry>
  
  <entry>
    <title>长尾优化汇总</title>
    <link href="https://oysz2016.github.io/post/8592ee55.html"/>
    <id>https://oysz2016.github.io/post/8592ee55.html</id>
    <published>2022-10-25T02:11:37.871Z</published>
    <updated>2022-12-05T01:09:45.458Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="经典的深度学习数据集"><a href="#经典的深度学习数据集" class="headerlink" title="经典的深度学习数据集"></a>经典的深度学习数据集</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295054598.png" alt="enter description here"><br>Mnist: 数据规模较小，10个类别<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295071148.png" alt="enter description here"><br>ImageNet: 百万数据量，1000个类别<br>两者的共同点：类别是均匀分布的</p><h2 id="真实场景中的深度学习任务"><a href="#真实场景中的深度学习任务" class="headerlink" title="真实场景中的深度学习任务"></a>真实场景中的深度学习任务</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295017948.png" alt="enter description here"></p><ul><li>类别不平衡是常态<h2 id="长尾问题"><a href="#长尾问题" class="headerlink" title="长尾问题"></a>长尾问题</h2><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295083966.png" alt="enter description here"><br>长尾问题常见的表现及原因 ：</li><li>头部类的效果好，尾部类的效果差</li><li>模型是数据驱动的，头部类的数据多，尾部类的数据少</li><li>尾部类数量少的同时可能造成类别中样本差异较大，网络学习不充分<h1 id="Resampling"><a href="#Resampling" class="headerlink" title="Resampling"></a>Resampling</h1><h2 id="BBN-Bilateral-Branch-Network-with-Cumulative-Learning-for-Long-Tailed-Visual-Recognition"><a href="#BBN-Bilateral-Branch-Network-with-Cumulative-Learning-for-Long-Tailed-Visual-Recognition" class="headerlink" title="BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition"></a>BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition</h2></li></ul><p><strong>论文链接：</strong><a href="https://arxiv.org/pdf/1912.02413.pdf">https://arxiv.org/pdf/1912.02413.pdf</a></p><p>two stage finetuning:</p><ul><li>第一阶段在原始不平衡的数据集上训练</li><li>第二阶段以一个很小的学习率使用resampling/reweighting的方法fintune</li></ul><p>根据two stage fintuning方法比只使用resampling/reweighting好的原因做了假设</p><ul><li>reblance的方法有效的原因在于提升了分类器的性能</li><li>会损害网络学习到的特征<br>为了验证上述假设，将网络拆解成了分类器和特征提取器两部分，分别实验:</li><li>在第一阶段使用交叉熵和resampling/reweighting训练整个网络</li><li>将第一阶段特征提取器的参数固定，使用交叉熵和resampling/reweighting训练分类器</li></ul><p>为了验证上述假设，将网络拆解成了分类器和特征提取器两部分，分别实验:</p><ul><li>在第一阶段使用交叉熵和resampling/reweighting训练整个网络</li><li>将第一阶段特征提取器的参数固定，使用交叉熵和resampling/reweighting训练分类器</li></ul><p>设计了一个<a href="https://arxiv.org/pdf/1912.02413.pdf">双分支的网络</a></p><p>主要思想是设计了一个<strong>two stage finetuning</strong>的训练方式</p><ul><li>第一阶段在原始不平衡的数据集上训练</li><li>第二阶段以一个很小的学习率使用resampling/reweighting的方法fintune</li></ul><p>根据two stage fintuning方法比只使用resampling/reweighting好的原因做了假设</p><ul><li>reblance的方法有效的原因在于提升了分类器的性能</li><li>会损害网络学习到的特征</li></ul><p>为了验证上述假设，将网络拆解成了分类器和特征提取器两部分，分别实验:</p><ul><li>在第一阶段使用交叉熵和resampling/reweighting训练整个网络</li><li>将第一阶段特征提取器的参数固定，使用交叉熵和resampling/reweighting训练分类器</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295186969.png" alt="enter description here"><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295194164.png" alt="enter description here"><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295201430.png" alt="enter description here"></p><p>总结下BBN思想中的<strong>关键点:</strong></p><ul><li>第一个采样器是一个公平的采样器</li><li>第二个采样器是一个resample的采样器</li><li>两个分支共享权重，减少参数量的同时，让第二个分支受益于第一个分支中更好的特征</li><li>使用一个adaptor的策略，调节两个分支在网络训练中的权重</li></ul><p>下面是添加BBN的结果和其他方法的对比:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295216148.png" alt="enter description here"><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295235292.png" alt="enter description here"></p><h1 id="Cost-sensitive-learning"><a href="#Cost-sensitive-learning" class="headerlink" title="Cost-sensitive learning"></a>Cost-sensitive learning</h1><h2 id="Class-Balanced-Loss-Based-on-Effective-Number-of-Samples"><a href="#Class-Balanced-Loss-Based-on-Effective-Number-of-Samples" class="headerlink" title="Class-Balanced Loss Based on Effective Number of Samples"></a>Class-Balanced Loss Based on Effective Number of Samples</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/pdf/1901.05555.pdf">https://arxiv.org/pdf/1901.05555.pdf</a><br>之前方法存在的问题：在reweighting等方法中，一般将样本数量的倒数作为该类别的权重，但是样本之间能提供的信息可能存在重合，简单通过样本数量判断权重会存在问题</p><ul><li>提出了一种计算有效样本的方法</li><li>用有效样本数代替原始的样本频率，再用其倒数对损失进行加权</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295283511.png" alt="enter description here"></p><p>有效样本的定义:</p><ul><li>样本的有效数据量是样本的期望体积，一个新的采样数据只能存在两种情况</li><li>新样本存在于之前样本中的概率为p</li><li>新样本不存在于之前样本中的概率为1-p</li></ul><p>提出有效样本的计算公式:</p><script type="math/tex; mode=display">E_{n}=\frac{1-\beta^{n}}{1-\beta}</script><p>当n=1时，<script type="math/tex">E_1=\frac{1-\beta^1}{1-\beta}=1</script><br>假设当n=k-1时成立,即<script type="math/tex">E_{k-1}=\frac{1-\beta^{k-1}}{1-\beta}</script>，<br>设样本的体积为K，已经采样的样本体积为<script type="math/tex">E_{k-1}</script>。则<script type="math/tex">p=\frac{E_{k-1}}{K}</script>,经过k次采样后，第k次采样时有以下情况:</p><ul><li>第k次采样和之前样本存在重叠的情况,则样本体积为<script type="math/tex">E_{k-1}</script></li><li>第k次采样是新的有效样本，与之前不存在重叠的情况,则样本体积为<script type="math/tex">E_{k-1}+1</script></li></ul><p>期望体积为:</p><script type="math/tex; mode=display">E_k=pE_{k-1}+(1-p)(E_{k-1}+1)=1+\frac{K-1}{K}E_{k-1}</script><p>其中</p><script type="math/tex; mode=display">\beta=\frac{K-1}{K}$$, 则$$E_k=1+\beta E_{k-1}=1+\beta \frac{1-\beta^{k-1}}{1-\beta}=\frac{1-\beta^{k}}{1-\beta}</script><p>则有效数据量是数据总量n的指数函数，超参数<script type="math/tex">\beta\in[0,1)</script><br>有效数据量<script type="math/tex">E_n</script>具有如下性质:</p><ul><li>当n很大时，有效数据量等于n</li><li>当n为1时，有效数据量为1</li></ul><p>CB-Loss:</p><script type="math/tex; mode=display">CB(p,y)=\frac{1}{E_{n_y}}L(p,y)=\frac{1-\beta}{1-\beta_{n_y}}L(p,y)</script><p>贴一下实验结果：<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295322950.png" alt="enter description here"></p><h1 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h1><h2 id="Feature-Space-Augmentation-for-Long-Tailed-Data"><a href="#Feature-Space-Augmentation-for-Long-Tailed-Data" class="headerlink" title="Feature Space Augmentation for Long-Tailed Data"></a>Feature Space Augmentation for Long-Tailed Data</h2><p><strong>论文链接：</strong><a href="https://arxiv.org/abs/2008.03673">https://arxiv.org/abs/2008.03673</a></p><p>常见的解决方法，如data manipulation和Balanced loss function design在提升长尾数据集模型性能的同时会损害特征表示能力<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295376341.png" alt="enter description here"></p><p>两个假设:</p><ul><li>头部类中类别无关的特征可以让尾部类特征更加丰富</li><li>高级特征空间具有更“线性”的表示，可以提取类通用和类特定的特征，并重新混合生成新的样本</li></ul><p>方法:<br>CAM(Class Activation Map)</p><script type="math/tex; mode=display">M_c(x,y)=\sum_{k}w_k^cf_k(x,y)</script><p>c: class; x,y: pixel position; k: channel; w: weight; f: feature，将M归一化到[0,1]，设定两个阈值。<script type="math/tex">0<\tau_s,\tau_t<1</script>。则类特定特征和类通用特征分别为:</p><ul><li><script type="math/tex; mode=display">M_c^s=sgn(M_c-\tau_s) \bigodot M_c</script></li><li><script type="math/tex; mode=display">M_c^g=sgn(\tau_g-M_c) \bigodot M_c</script></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295358501.png" alt="enter description here"><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295382285.png" alt="enter description here"></p><ul><li>正常训练得到特征提取网络和分类器，使用注意力机制CAM图做特征分解，将特征分为类别无关的特征和类别特定的特征。</li><li>两阶段训练，第一阶段正常训练，负责提取特征和cam，第二阶段是做尾部类的增广训练，分为两步<ul><li>网络输入一张头部类图片和一张尾部类图片，通过分类置信度排序，选择和当前尾部类距离最近的头部类特征，融合头部类中类通用特征和尾部类中类特征特征</li></ul></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295403120.png" alt="enter description here"></p><ul><li>使用增强的特征图微调FC分类器层</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/1664295415204.png" alt="enter description here"></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="长尾优化" scheme="https://oysz2016.github.io/tags/%E9%95%BF%E5%B0%BE%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>《Unbiased Scene Graph Generation from Biased Training》论文笔记</title>
    <link href="https://oysz2016.github.io/post/b4822109.html"/>
    <id>https://oysz2016.github.io/post/b4822109.html</id>
    <published>2022-10-15T08:26:18.047Z</published>
    <updated>2023-01-01T04:01:59.574Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><h1 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h1><p>场景图生成: 描述目标检测的物体之间，具有怎样的关系</p><h1 id="之前算法存在的问题"><a href="#之前算法存在的问题" class="headerlink" title="之前算法存在的问题"></a>之前算法存在的问题</h1><ul><li>数据集中关系词存在严重的偏见，原因有以下几点:<ol><li>标注时,倾向于简单的关系</li><li>日常生活中确实有些事物的关联性比较多</li><li>语法习惯的问题</li></ol></li></ul><ul><li>往往通过union box和两个物体的类别就预测了两个物体的关系，几乎没有使用visual feature，也就预测不出有意义的关系</li></ul><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><ol><li>为了让模型预测更有意义的关系，用了一个causal inference中的概念，即Total Direct Effect（TDE）来取代单纯的网络log-likelihood。在预测关系时更关注visual feature.</li></ol><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-144828@2x.png" alt="enter description here"></p><ol><li>提出了新的评测方法mR@K:把所有谓语类别的Recall单独计算，然后求均值，这样所有类别就一样重要了</li></ol><h2 id="Biased-Training-Models-in-Causal-Graph"><a href="#Biased-Training-Models-in-Causal-Graph" class="headerlink" title="Biased Training Models in Causal Graph"></a>Biased Training Models in Causal Graph</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143055@2x.png" alt="有偏差的训练框架"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-142859%402x.png" alt="训练简化图"></p><p>节点I: Input Image&amp;Backbone.Backbone部分使用Faster rcnn预训练好的模型，并frozen bockbone的参数。输出检测目标的bounding boxes和图像的特征图.<br>Link I-&gt;X:目标的特征，通过ROI Align提取目标对应的特征<script type="math/tex">R={r_i}</script>，获取目标粗略的分类结果<script type="math/tex">L={l_i}</script>.和MOTIFS和VCTree一样，使用以下方式，编码视觉上下文特征.</p><blockquote><p>MOTIFS中使用双向LSTM，VCTree中使用双向TreeLSTM，早期工作如VTransE中使用全连接层</p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-145409@2x.png" alt="enter description here"></p><p>节点X：目标特征。获取一组目标的特征<script type="math/tex">x_e=(x_i,x_j)</script><br>Link X-Z: 获取对应目标fine-tuned的类别，从<script type="math/tex">x_i</script>解码:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-145415@2x.png" alt="enter description here"></p><p>节点Z:目标类别，one-hot的向量<br>Link X-&gt;Y: SGG的目标特征输入<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-145423@2x.png" alt="enter description here"></p><p>Link:Z-&gt;Y：SGG的目标类别输入<br>Link:I-&gt;Y:SGG的视觉特征输入<br>节点Y:输出关系词汇<br>Training loss：使用交叉熵损失预测label，为了避免预测Y只使用单一输入的信息，尤其是只使用Z的信息，进一步 使用auxiliary cross-entropy losses, 让每一个分支分别预测y</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/1280X1280.png" alt="enter description here"></p><h2 id="Unbiased-Prediction-by-Causal-Effects"><a href="#Unbiased-Prediction-by-Causal-Effects" class="headerlink" title="Unbiased Prediction by Causal Effects"></a>Unbiased Prediction by Causal Effects</h2><p>机器学习中常见的解决长尾问题的方法:</p><ul><li>数据增强/重新采样</li><li>对数据平衡改进的loss</li><li>从无偏见中分离出有偏见的部分<br>与上述方法的区别是不需要额外训练或层来建模偏差，通过构建两种因果图将原有模型和偏差分离开。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143048@2x.png" alt="enter description here"></li></ul><h3 id="Origin-amp-Intervention-amp-Counterfactual"><a href="#Origin-amp-Intervention-amp-Counterfactual" class="headerlink" title="Origin&amp;Intervention&amp;Counterfactual"></a>Origin&amp;Intervention&amp;Counterfactual</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143107@2x.png" alt="enter description here"></p><ul><li>ntervention:清除因果图中某个节点的输入，并将其置为某个值，公式为:$do(X= \tilde x)$.某节点被干预后，需要该节点输入的其他节点也会受影响</li><li>Counterfactual:让某个节点被干预后，其他需要输入的节点还假设该节点未被干预<br>总结: Counterfactual图实际上抹除了因果图像中object feature。只用image+object label预测两个目标间的关系。</li></ul><h3 id="Total-Direct-Effect-TDE"><a href="#Total-Direct-Effect-TDE" class="headerlink" title="Total Direct Effect (TDE)"></a>Total Direct Effect (TDE)</h3><p>根据两个因果图:</p><ul><li>原始因果图</li><li>Counterfactual因果图(可以认为是偏见)<br>消除偏见:<script type="math/tex; mode=display">TDE=Y_x(u)-Y_{\tilde x,z}(u)</script></li></ul><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143120@2x.png" alt="enter description here"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143154@2x.png" alt="enter description here"></p><h1 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h1><p>对比了几种常用的优化长尾问题的方法</p><ul><li>Focal</li><li>Reweight</li><li>Resample</li><li>X2Y: 直接通过X的输出预测Y</li><li>X2Y-Tr：切断其他分支的联系，只使用X预测Y</li><li>TE:<script type="math/tex">TE=Y_x(u)-Y_{\tilde x}(u)</script></li><li>NIE:<script type="math/tex">NIE=TDE-TE</script></li><li>TDE</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/WX20221006-143120@2x.png" alt="enter description here"></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="因果图" scheme="https://oysz2016.github.io/tags/%E5%9B%A0%E6%9E%9C%E5%9B%BE/"/>
    
    <category term="场景图生成" scheme="https://oysz2016.github.io/tags/%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>《GAN》论文笔记</title>
    <link href="https://oysz2016.github.io/post/f84aad11.html"/>
    <id>https://oysz2016.github.io/post/f84aad11.html</id>
    <published>2022-10-08T08:18:38.316Z</published>
    <updated>2022-12-10T01:12:55.574Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>论文链接: <a href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf</a></p><p>论文代码: <a href="https://github.com/devnag/pytorch-generative-adversarial-networks">https://github.com/devnag/pytorch-generative-adversarial-networks</a></p><p>顾名思义, GAN是一个对抗网络。具体来说，会有一个生成器(Generator)，和一个判别器(Discirminator)，两个模型互相对抗，共同进步。举一个生活中的例子，例如造假币的人是这里的生成器，查假币的人是判别器，那么生成器的任务就是让造出的假币让判别器以为是真钱一样，而判断器的任务就是能很好的分别出生成器造出的钱是假币。</p><h1 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h1><p>输入一个n维的noise的向量z，一般是随机产生的，例如满足高斯分布/均值分布的噪声，生成器的任务是将向量z生成图片x，可以通过MLP等神经网络</p><h1 id="判别器"><a href="#判别器" class="headerlink" title="判别器"></a>判别器</h1><p>输入是图片数据，输出是一个标量，用于判断输入数据是来自真实数据还是生成器</p><h1 id="训练方式"><a href="#训练方式" class="headerlink" title="训练方式"></a>训练方式</h1><p>通过上面网络的简介知道，需要训练两个网络，所以训练方式目的也是让两个网络都能够有效的学习</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GAN/WX20221006-161252@2x.png" alt="enter description here"></p><ul><li>上式中D是判别器，G是生成器</li><li>先看前一项，$E_{x \sim p_{data}}[logD(x)]$表示从真实数据$p_{data}$中采样样本x，让判别器判断x的来源。在辨别器判断正确的情况下$D_x=1$, 则$logD(x)=0$</li><li>后面一项，$E_{z \sim p_{z}(z)}[log(1-D(G(z)))]$表示从随机噪声数据$p_{z}(z)$中生成噪声z, 先让生成器z生成类似x的数据$G(z)$,再让判别器判断生成的数据来源，在判别器完美的时候，能分出$G(z)$不是真实样本，则$D(G(z))=0$</li><li>在D犯错的时候，$log(D(x)$为负数，因此需要最大化D的损失, 而$G(z)$需要生成的和x尽可能相似，需要最小化G的损失</li><li>整个模型设计用的是博弈论的思想，min和max都需要优化，且互相对抗。感兴趣可以看看minmax算法相关的知识:<a href="https://en.wikipedia.org/wiki/Minimax">https://en.wikipedia.org/wiki/Minimax</a>。在博弈论中，在包含两个或两个以上参与者的非合作博弈中，如果每个参与者都选择了对自己最有利的策略，则最后会达到均衡点，称为纳什均衡(<a href="https://en.wikipedia.org/wiki/Nash_equilibrium">Nash equilibrium</a>)</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GAN//WX20221006-161026@2x.png" alt="enter description here"></p><p>网络的优化过程在图1中有形象化的示例，图1中，蓝色的线是判别器，黑色是真实数据，绿色是生成器</p><ul><li>在图1(a)中，生成器生成的数据和真实数据相差比较大，但判别器判断的也不是太好</li><li>图1(b)更新了判别器，能很好的区分是否是生成器生成的数据</li><li>图1(c)中更新了生成器，生成的数据和真实数据变得很相似</li><li>图1(d)中随着生成器和辨别器都进行优化，最终生成器拟合的数据和真实数据基本一致，而判别器也无法判断数据来源，则模型优化完毕</li></ul><p>代码逻辑如下所示:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GAN/WX20221006-161122@2x.png" alt="enter description here"></p><ul><li>第一个for循环是迭代的次数</li><li>然后会将判别器循环更新k次，再将生成器更新1次</li></ul><p>其中k是一个超参数，如果判别器更新的太好，生成器就没法玩了，而要是更新的太差，生成器就没有动力继续优化</p><p>算法的原理到这里基本上就整理完了，论文中第四部分章节有关于损失函数正确的理论证明，感兴趣也可以阅读下</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>gan算法实际上训练完后主要使用的是生成器，在方法中其实对生成器已经有了优化，所以为什么还要引入判别器呢？其实光从生成器的优化上，很难通过损失判断生成器的好坏，而引入判别器可以更全局的判断生成器的效果，也通过博弈的方法让生成器能达到更好的效果</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生成网络" scheme="https://oysz2016.github.io/tags/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>《ReferTransformer》论文笔记</title>
    <link href="https://oysz2016.github.io/post/149f20a.html"/>
    <id>https://oysz2016.github.io/post/149f20a.html</id>
    <published>2022-10-01T07:58:44.703Z</published>
    <updated>2022-12-07T02:05:55.627Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p><strong>论文链接</strong>: <a href="https://arxiv.org/abs/2201.00487">https://arxiv.org/abs/2201.00487</a></p><p><strong>代码链接</strong>: <a href="https://github.com/wjn922/ReferFormer">https://github.com/wjn922/ReferFormer</a></p><p>由于之前看图片相关的论文比较多，而这篇论文偏向于视频理解的任务，为了避免有其他不了解这篇论文领域的同学看的时候比较吃力，在解读这篇论文前先介绍这篇论文的任务。</p><p><strong>任务定义</strong>: R-VOS(Referring video object segmentation): 给出一种物体对应的语言描述，分割出该物体对应的mask。<br>这篇论文的相关视频在reddit上热度非常高，感兴趣的同学可以看下视频，相信也能对这篇论文应用的方向有更清晰的了解。<a href="https://www.reddit.com/r/MachineLearning/comments/t7qe6b/r_endtoend_referring_video_object_segmentation/?utm_source=share&amp;utm_medium=web2x&amp;context=3">[R] End-to-End Referring Video Object Segmentation with Multimodal Transformers</a></p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233717@2x.png" alt="enter description here"></p><h3 id="Bottom-up"><a href="#Bottom-up" class="headerlink" title="Bottom-up"></a>Bottom-up</h3><ul><li><strong>方法:</strong> 如图1(a)所示，用early-feature的方式融合视觉和语言特征，再用FCN decoder目标的掩码。</li><li><strong>缺点:</strong> early-feature的方法无法获得较好的多模态特征，无法为跨模态推理提供明确的知识，并且会遇到由于场景变化而导致预测对象的差异。</li></ul><h3 id="Top-down"><a href="#Top-down" class="headerlink" title="Top-down"></a>Top-down</h3><ul><li><strong>方法:</strong> 如图1(b)所示，两阶段的方式，先对图片/视频中所有的物体做实例分割，将实例分割的结果在视频中关联起来，形成一系列候选，再通过语言模型和Grounding Model筛选出语言描述提到的物体</li><li><strong>缺点:</strong> 相比bottom-up的方式有更好的性能，但整个pipeline由于多阶段的方式太重。例如最近的一些方法，如HTC,CFBI中都需要再ImageNet，COCO，RefCOCO中预训练，然后在R-VOS数据集中fintune。而且将R-VOS的任务拆解为几个子问题分别优化由于误差传递等问题会造成次优的解决方案。</li></ul><h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><p>该论文的方法图1(c):</p><ul><li>将文字的特征和queries的特征融合，作为conditional queries，生成的queries可以只聚焦在文字所提到的目标特征上，可以极大的减少queries的数量(例如detr中的100个)</li><li>考虑到需要从queries的特征中decode出object mask，使用instance-aware dynamic kernels从提取分割的mask特征</li><li>受启发于FPN，设计了CM-FPN(cross modal features pyramid network), 提取多模态的金字塔特征</li></ul><p>网络结构如图2所示，主要由backbone,language as queries,Cross-modal Feature Pyramid Network,Instance Sequence Matching and Loss，inference五部分组成。下面我写的尽量详细点。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233751@2x.png" alt="enter description here"></p><h3 id="backbone"><a href="#backbone" class="headerlink" title="backbone"></a>backbone</h3><p><strong>visual encoder</strong><br>使用通用的视觉backbone做视觉特征的编码器，例如可以使用经典的ResNet网络或者3D的特征编码器，如Video swin transformer。生成的特征是如下Sequence $F_v=\{F_t\}^T_{t=1}$,其中T表示T桢图像<br><strong>linguistic encoder</strong><br>使用现成的语言模型,如RoBERTa，提取文本的特征$F_e=\{F_i\}^L_{i=1}$，其中L表示L个word。在该任务中需要将$F_e$通过pooling提取句子级别的特征，这是由于该任务使用句子级别的特征和queries融合用于跨模态的特征融合</p><h3 id="language-as-queries"><a href="#language-as-queries" class="headerlink" title="language as queries"></a>language as queries</h3><p><strong>transformer encoder</strong>: encoder部分和detr中一样，先用1x1卷积进行<strong>通道数压缩</strong>,再把宽和高压缩为一个维度，将<strong>特征序列化</strong>，再做<strong>位置编码</strong><br><strong>transformer decoder</strong>: decoder部分也和detr比较类似，但有些差异。使用N个object queries提取每帧实例中的特征，区别在于queries在视频帧之间共享权重，好处是可以更灵活的处理长视频，并且对于同样的实例queries学习的特征更鲁棒。将句子级别的特征$F_e$复制N份和每一个object queries一起作为decoder的输入。对于T帧图像，会得到$N_q=T<em>N$的预测集合<br><strong>prediction heads</strong>：具有三个预测头,分别是box head，mask head，class head(二分类，输出是否是文本中提到的目标)<br><strong>dynamic convolution</strong>： 使用动态卷积生成binary segmentation masks<br><em>*illustration of conditional queries</em></em>：得益于transformer的注意力机制，输入object queries和text embedding可以让object queries聚焦于text提到的物体</p><h3 id="cross-model-Feature-Pyramid-Network"><a href="#cross-model-Feature-Pyramid-Network" class="headerlink" title="cross-model Feature Pyramid Network"></a>cross-model Feature Pyramid Network</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233822@2x.png" alt="enter description here"></p><p>该部分可以认为是这篇论文最大的创新点，提出了多模态融合的FPN网络，网络结构图如图4所示，和传统FPN网络的区别在于加入了vision-language fusion模块，该模块的结构类似transformer网络，图像特征作为encoder部分qkv的输入，文本特征作为decoder部分k和v的输入。</p><h3 id="Instance-Sequence-Matching-and-Loss"><a href="#Instance-Sequence-Matching-and-Loss" class="headerlink" title="Instance Sequence Matching and Loss"></a>Instance Sequence Matching and Loss</h3><p>前面提到过，对于T帧图像，N个object queries，会得到$N_q=T*N$个预测结果。由于视频中跨帧的物体在相邻帧间保持相对相同的位置，可以将预测结果看成是N个实例在T帧上的轨迹</p><p>因此可以用instance matching strategy的方式对预测整体监督。将预测集集表示为$y=\{y_i\}^N_{i=1}$，第i个实例的预测表示为:</p><script type="math/tex; mode=display">y_i=\{p^t_i, b^t_i, s^t_i\}^T_{t=1}</script><p>$p^t$表示一个概率的标量，用于表示实例是否在文本中被提到且在当前帧出现的概率；$b^t_i$是一个长度为4的向量，表示实例的坐标，具体定义是目标的中心坐标和宽高；<br>$s^t_i$的维度为(H/4 * W/4),表示实例的分割mask。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-235737@2x.png" alt="enter description here"></p><p>损失函数部分由3部分组成，分类loss为focal loss，坐标回归loss为L1 Loss和GIOU Loss，分割loss为DICE Loss和binary mask focal loss</p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>在推理时，模型会预测所有帧中对应实例类别的置信度，选择平均置信度最高的实例类别对应的定位和分割结果作为最终的输出结果</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>论文中有非常详细的和其他方法的对比和消融实验<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233839@2x.png" alt="enter description here"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233851@2x.png" alt="enter description here"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233900@2x.png" alt="enter description here"></p><p>表4中baseline的方法训练和推理阶段使用的都是长度为5帧的视频，baseline的方法不能有效的区分距离较近的相似物体，容易分割出最显著的区域，相比之下，通过表6(a)可以看出，只需要一个object queries就可以获得比baseline更好的效果，也证明了动态卷积对于分割任务是必要的</p><ul><li>baseline的方式使用图像level的特征分割物体，而本文的方法是用图像-文本融合后的多模态特征</li><li>本文将动态卷积核的相对坐标和掩码特征融合起来，有助于模型定位出文本中提到的目标</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233911@2x.png" alt="enter description here"></p><p>不同backbone的影响<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-233918@2x.png" alt="enter description here"></p><p><strong>object queries数量的影响</strong>，从实验结果来看数量也不是越多越好，5个是实验中的最好效果。<br><strong>训练和测试帧数的影响</strong>，最好的是5帧的参数，但理论上应该帧数增多，效果还会提升，可能是受限于计算资源的影响<br><strong>预测头的影响</strong>，类别，边框回归，分割三个任务都预测，效果是最好的<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ReferFormer/WX20221001-234022@2x.png" alt="enter description here"></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here">  </p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="视频理解" scheme="https://oysz2016.github.io/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/"/>
    
    <category term="多模态" scheme="https://oysz2016.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>《Faster R-CNN》详解</title>
    <link href="https://oysz2016.github.io/post/79b7b6f1.html"/>
    <id>https://oysz2016.github.io/post/79b7b6f1.html</id>
    <published>2019-06-08T06:35:12.157Z</published>
    <updated>2019-06-08T07:20:44.007Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><blockquote><p>最近实在是太忙了，很久没更新博客。想起前几周花了一些功夫给组内的小伙伴们做了个分享，为了做分享时自己能讲清楚，找了不少我认为描述的比较好的图片。Faster R-CNN可以说是目标检测领域的开山之作了，即使现今的顶会论文中，目标检测方面 的论文也有不少是对Faster R-CNN做的改进。这么经典的文章，细节又多如牛毛，真是常读常新。所以搬运到博客上应该也是有一些价值的。</p></blockquote><h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196152.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978195967.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196153.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978195729.jpg" alt="enter description here"></p><blockquote><p>写作的过程是将网状的思考用树状的语法结构转换成线性的文字，阅读则是其逆向过程。读文章时并不用逐字逐句的按文章的书写顺序去读。可以按照合适的方式去更好的还原作者的思考过程。及时的停止读一篇文章，能及时止损，每个人精力和注意力都是宝贵的，不应该把时间花在读了对自己没价值的文章上。</p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196154.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196156.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196019.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196293.jpg" alt="enter description here"></p><blockquote><p>检测任务目前的弊病之一就在于其网络的特征提取层直接从分类网络迁移学习来，并不是很贴合检测任务。</p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196020.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196294.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196021.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196024.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196026.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196314.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978195920.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196289.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196118.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978195946.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1559978196290.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="输入分类" scheme="https://oysz2016.github.io/categories/%E8%BE%93%E5%85%A5%E5%88%86%E7%B1%BB/"/>
    
    
    <category term="输入标签" scheme="https://oysz2016.github.io/tags/%E8%BE%93%E5%85%A5%E6%A0%87%E7%AD%BE/"/>
    
  </entry>
  
  <entry>
    <title>《GIoU》论文笔记</title>
    <link href="https://oysz2016.github.io/post/4065a784.html"/>
    <id>https://oysz2016.github.io/post/4065a784.html</id>
    <published>2019-03-17T04:25:09.918Z</published>
    <updated>2019-03-17T05:52:44.516Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>论文链接：<a href="https://arxiv.org/abs/1902.09630">https://arxiv.org/abs/1902.09630</a><br>代码链接：<a href="https://github.com/generalized-iou">https://github.com/generalized-iou</a></p><p>&emsp;&emsp;这篇论文出自CVPR2019，算是目前已被录用且公布的位数不多的目标检测相关论文了。这篇论文提出了一种优化边界框的新方式——GIoU(generalized IoU，广义IoU)。<em>目前关于IOU的新用法真是层出不穷，从<a href="https://oysz2016.github.io/post/d76cc2d4.html">Cascade R-CNN</a>到<a href="https://oysz2016.github.io/post/4395ef5a.html">IOU Net</a>再到如今的GIoU，GIoU的方法是这些论文中相对简单的，相信很多朋友了解了这篇文章的远离后，内心的OS都是“总觉得损失函数可以优化，这么简单我怎么没想到呢？”，哈哈，反正我是这样想的了。下面来看看这篇文章所提出的方法吧。</em></p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>&emsp;&emsp;目前目标检测中主流的边界框优化采用的都是BBox的回归损失(MSE loss, L1-smooth loss等)，这些方式计算损失值得方式都是检测框得“代理属性”，而忽略了检测框本身最显著的性质——IoU。如下图所示，在L1及L2范数取到相同的值时，实际上检测效果却是差异巨大的，直接表现就是预测和真实检测框的IoU值变化较大，这说明L1和L2范数不能很好的反映检测效果。</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1552801043208.jpg" alt="enter description here"></p><p>&emsp;&emsp;除了能反映预测检测框与真实检测框的检测效果外，IoU还具有<strong>尺度不变性</strong>。可是既然IOU这么好，为什么之前不直接用IoU呢，这是由于IoU有两个缺点，导致其不太适合做损失函数：</p><ul><li>但检测框与gt之间没有重合时，IoU为0。而在优化损失函数时，梯度为0，意味着无法优化</li><li>在检测框与gt之间IoU相同时，检测的效果也具有较大差异，如下图所示：<br><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1552801042668.jpg" alt="enter description here"></li></ul><p>&emsp;&emsp;基于IoU的优良特性和其作为损失函数时的致命缺点，作者提出了一个新的概念——GIoU</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>&emsp;&emsp;GIoU的定义如下图所示，</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1552801042664.jpg" alt="enter description here"></p><p>&emsp;&emsp;根据定义，GIoU具有如下性质：</p><ul><li>GIoU具有作为一个度量标准的优良性质。包括非负性，同一性，对称性，以及三角不等式的性质</li><li>与IoU相似，具有尺度不变性</li><li>GIoU的值总是小于IoU的值</li><li>对于两个矩形框A和B，0≤IoU(A，B)≤1，而-1≤GIoU≤1</li><li>在A，B没有良好对齐时，会导致C的面积增大，从而使GIoU的值变小，而两个矩形框不重合时，依然可以计算GIoU，一定程度上解决了IoU不适合作为损失函数的原因</li></ul><p>&emsp;&emsp;GIoU作为损失函数时计算方式如下的算法2</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1552801043209.jpg" alt="enter description here"></p><p>&emsp;&emsp;从算法中可以看到和GIoU的计算方式和IoU的步骤基本保持一致，在得到IoU的值后在根据上面的算法1计算GIoU的值。<strong>这里还不太清楚方向传播时，梯度是怎么计算的。等我看看源码再来更新吧</strong></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>&emsp;&emsp;作者分别在几种主流的目标检测算法上做了实验，分别是YoLo、Faster R-CNN和Mask R-CNN。这里贴上在Pascal Voc数据集上的实验结果，如下</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1552801042667.jpg" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1552799016561.jpg" alt="enter description here"></p><p>&emsp;&emsp;实验结果中在YoLo v3上可以看到GIoU相比IoU的损失函数有较大幅度的提升，而在faster r-cnn中GIoU和IoU作为损失函数的区别不大，这里作者给出的解释是faster rcnn的anchor更密集，导致不易出现与gt不重叠的检测框。<em>其实个人认为，anchor多的情形与gt不重叠的检测框才多，更根本的原因应该是RPN网络进行了一次粗检后，滤去了大部分跟gt没有重合的检测框。导致GIoU相比IoU的损失函数提升不明显吧</em></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;GIoU的方法很简单，巧妙的是优化的点。通过广义IoU作为损失函数替代bbox回归还是很有趣的。不过疑惑的是实验结果中的检测AP值都非常低，原生的faster rcnn在pascal voc上的检测效果都不会这么差。从实验对比上GIoU的损失函数相比原始的损失函数在准确率不到40%的效果上来说确实有较大幅度的提升。然而要是换到准确率较高的baseline上呢？这一点还需要实验验证。<br>&emsp;&emsp;另外总感觉这篇论文有点点到即止，没有更多的实验验证bbox作为损失函数存在缺陷的原因。</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="目标检测" scheme="https://oysz2016.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>《Mask TextSpotter》论文笔记</title>
    <link href="https://oysz2016.github.io/post/f222e8ff.html"/>
    <id>https://oysz2016.github.io/post/f222e8ff.html</id>
    <published>2019-02-17T07:18:21.458Z</published>
    <updated>2019-03-17T05:48:16.802Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>&emsp;&emsp;论文链接：<a href="https://arxiv.org/abs/1807.02242">https://arxiv.org/abs/1807.02242</a><br>&emsp;&emsp;代码链接：<a href="https://github.com/lvpengyuan/masktextspotter.caffe2">https://github.com/lvpengyuan/masktextspotter.caffe2</a></p><p>&emsp;&emsp;除了自然场景的目标检测外，文本检测也是近年来热门的研究领域。Mask TextSpotter是<strong>ECCV 2018</strong>发表的一篇文本检测文章，具有以下特点：</p><ul><li><strong>端到端的检测+识别框架</strong></li><li><strong>基于Mask R-CNN结构</strong></li><li><strong>在处理不规则的文本形状时，优于之前的方法</strong></li><li><strong>除了文本行检测外，能进行字符分割</strong></li></ul><p>&emsp;&emsp;Mask R-CNN结构是近几年来最优秀的目标检测结构了，该篇文章将其应用到文本检测这一任务上，并且所做的优化算是十分巧妙了。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>&emsp;&emsp;Mask TextSpotter提出了一种名为掩码文本检测的文本检测器，它可以<strong>检测和识别任意形状</strong>的文本。</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1550388479976.jpg" alt="enter description here"></p><p>&emsp;&emsp;上图直观的比较了不同的检测方法其场景文本定位的效果，左图是水平水平文本定位方法，中间是支持倾斜框的文本检测方法，右图是Mask TextSpotter的检测方法。</p><p>&emsp;&emsp;可以看到Mask TextSpotter相比较于其他两种方法能够提供更准确的定位图，为后续的识别提供良好的检测效果。<br>这篇文章总共有四点贡献：</p><ol><li>提出了一种端到端的文本检测加识别模型，具有简单高效的特点</li><li>该方法可以检测和识别各种形状的文本，包括水平文本、定向文本和弯曲文本</li><li>与之前的方法相比，该方法通过语义分割来实现精确的文本检测和识别</li><li>在各种基准的文本检测和定位上都达到了state-of-the-art的效果</li></ol><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>&emsp;&emsp;这部分主要介绍了场景文本检测、识别及结合的发展进程。强调了Mask TextSpotter基于Mask R-CNN，区别在于该方法不仅可以分割文本，也可以进行字符分割。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><p>&emsp;&emsp;Mask TextSpotter的架构如下图所示：</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753793.png" alt="enter description here"></p><p>&emsp;&emsp;该架构由以下4部分组成</p><ul><li>backbone：ResNet50；FPN网络（ top-down 结构）</li><li>生成文本建议：RPN</li><li>边界框回归：Fast R-CNN</li><li>文本和字符实例分割：mask branch</li></ul><p>&emsp;&emsp;首先由RPN生成大量的文本提案，然后将提案的RoI特征输入Fast R-CNN分支和mask分支中，生成准确的文本候选框、文本实例分割图和字符分割图。</p><p>&emsp;&emsp;每部分的细节如下：</p><ul><li><strong>Backbone</strong>:自然场景的文本大小不一，为了提取更高层的语义特征。采用了ResNet-50网络，并用对小目标有较好效果的FPN网络提取特征</li><li><strong>RPN</strong>:RPN网络为Fast R-CNN分支以及mask分支生成文本建议。anchor设置5种尺寸$\{32^2,64^2,128^2,256^2,512^2\}$，FPN网络中有5个层级$\{P_2,P_3,P_4,P_5\}$，3种比例$\{0.5,1,2\}$。区域特征映射方式采用ROI Align。</li><li><strong>Fast R-CNN</strong>:包括分类和回归任务，主要作用是为了后续的检测提供更精确的检测框。</li><li><strong>Mask 分支</strong>：掩码分支有两个任务，分别是全局文本实例分割和字符分割。如下图所示。Mask分支的输入是固定大小的ROI（16<em>64）,经过4个卷积层和一个反卷积层将特征图降维到38个维度（特征图大小：32</em>128）。这38个维度由以下3部分组成：<ul><li>全局文本实例分割</li><li>背景分割</li><li>10个数字，26个字母</li></ul></li></ul><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753811.png" alt="enter description here"></p><h3 id="标签的生成"><a href="#标签的生成" class="headerlink" title="标签的生成"></a>标签的生成</h3><p>&emsp;&emsp;为了满足训练的要求，ground truth要包含$P=P\{p_1,p_2…p_m\}$以及$C=\{c_1=(cc_1,cl_1),c_2=(cc_2,cl_2),…,c_n=(cc_n,cl_n)\}$。其中$p_i$表示一个文本定位的多边形。$cc_j$和$cl_j$分别是字符的类别和定位。<strong>值得一提的是并不要求所有样本都需要有标记$C$。</strong><br>首先用涵盖目标最小水平矩形面积的方法，将多边形转换为水平矩形。然后通过RPN和Fast R-CNN网络生成区域建议。对于具有ground truth P, C(可能不存在)的mask分支，需要生成两种类型的目标映射，以及RPN生成的建议：用于全局实例分割的映射和用于字符实例分割的映射。对于正样本的proposal,首先得到最匹配的水平矩形。相应的多边形及字符（如果有的话）可以进一步得到。在映射$H×W$上调整多边形和字符的proposal一致的公式如下：</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753633.png" alt="enter description here"></p><p>&emsp;&emsp;其中$(B_{x0},B_{y0})$是所有多边形和字符原始的顶点，$(B_x,B_y)$是所有多变形和字符更新后的顶点，$(r_x,r_y)$是rpoposal产生的顶点。</p><p>&emsp;&emsp;然后，全局映射图的生成规则：通过绘制zero-initialized mask的规则多边形；字符边界框的生成：固定所有字符的中心点，并将边缩短到原始边的1/4。如下图所示：</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753814.png" alt="enter description here"></p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>&emsp;&emsp;该部分讨论的是整个框架优化参数时的损失函数组成，正如上面介绍的Mask TextSpotter的框架由4部分组成，除了提取特征的网络外，其余的3部分都是损失函数的组成部分，如下所示：</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753635.png" alt="enter description here"></p><p>&emsp;&emsp;mask分支有两个任务，因此$L_{mask}$的计算如下：</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753768.png" alt="enter description here"></p><p>&emsp;&emsp;其中$L_{global}$为<a href="https://en.wikipedia.org/wiki/Cross_entropy">Cross entropy</a>损失，而$L_{char}$为<a href="https://en.wikipedia.org/wiki/Softmax_function">Softmax</a>损失。其中作者设置的超参数$α_1,α_2,β$都为1。<br>&emsp;&emsp;$L_{global}$的计算公式如下：</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753769.png" alt="enter description here"></p><p>&emsp;&emsp;$L_{char}$的计算公式如下：</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753770.png" alt="enter description here"></p><p>&emsp;&emsp;其中T为类别，X为预测的输出，Y为gt，W为权重。W的主要作用是为了样本均衡。不同类别的W计算公式如下：</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753771.png" alt="enter description here"></p><h3 id="推理-测试"><a href="#推理-测试" class="headerlink" title="推理(测试)"></a>推理(测试)</h3><p>&emsp;&emsp;不同于mask 分支的ROI来自于RPN网络，在推理阶段，mask分支的ROI来自于Fast R-CNN网络，而不是RPN网络，这是由于Fast R-CNN的输出更精确。<br>推理阶段可以分为以下几个过程：</p><ol><li>输入测试图片，获得Fast R-CNN的输出，并经过NMS;</li><li>保存下来的proposals被输入mask分支以生成全局映射(gloabal maps)和字符映射(character maps)</li><li><p>通过计算全局映射的map区域获得预测的多边形，通过pixel voting算法生成字符映射序列</p><p><strong>pixel voting</strong>算法的细节如下：</p></li></ol><ol><li>二值化背景图，阈值为192</li><li>根据二值化中的联通与划分所有字符区域</li></ol><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>&emsp;&emsp;作者总共使用了4个数据集，除了SynthText用来预训练以外，其余的三个数据集ICDAR 2013，ICDAR2015，Total_Text均做了测试实验。</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387754029.png" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387754097.png" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387754189.png" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387754099.png" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387754186.png" alt="enter description here"></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1550387753779.png" alt="enter description here"></p><p>&emsp;&emsp;通过以上实验数据比较，Mask TextSpotter水平文本、定向文本和弯曲文本等数据集上的良好性能证明了该方法对文本检测和端到端文本识别的有效性和鲁棒性。</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="目标检测" scheme="https://oysz2016.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    <category term="文本检测" scheme="https://oysz2016.github.io/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>简单线性回归</title>
    <link href="https://oysz2016.github.io/post/7fda15c2.html"/>
    <id>https://oysz2016.github.io/post/7fda15c2.html</id>
    <published>2019-01-13T09:10:18.243Z</published>
    <updated>2019-03-17T05:49:12.536Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>&emsp;&emsp;好长一段时间挺想重新系统的学习回顾下机器学习的知识，看了些深度学习的论文和框架后，觉得机器学习的知识确实太重要了。接下来的半年时间，想系统的总结实践下，也算是2019年的第一个flag吧。</p><h2 id="使用单一特征预测响应值"><a href="#使用单一特征预测响应值" class="headerlink" title="使用单一特征预测响应值"></a>使用单一特征预测响应值</h2><p>&emsp;&emsp;这是一种基于自变量值(X)来预测因变量值(Y)的方法。假设X和Y两个变量是线性相关的。线性回归就是尝试寻找一种根据特征或自变量(X)的线性函数来精确预测响应值(Y)。</p><h2 id="怎样找到最佳的拟合线"><a href="#怎样找到最佳的拟合线" class="headerlink" title="怎样找到最佳的拟合线"></a>怎样找到最佳的拟合线</h2><p>&emsp;&emsp;在这个回归任务中，我们将通过找到“最佳拟合线”来最小化预测误差——回归线应该尽量拟合X-Y的分布，即误差是最小的。例如$y_p$是预测值，$y_i$是实际值，这个过程就是使$y_p$和$y_i$之间的关系满足$min\{SUM(y_i-y_p)^2\}$</p><p>&emsp;&emsp;这里以学生分数数据集做这个实验，实验数据如下图所示：</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1547371646833.jpg" alt="enter description here"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ol><li>导入相关库</li><li>导入数据集</li><li>检查缺失数据</li><li>划分数据集</li><li>特征缩放(这里使用简单线性模型的相关库进行)</li></ol><h3 id="通过训练集来训练简单线性回回归模型"><a href="#通过训练集来训练简单线性回回归模型" class="headerlink" title="通过训练集来训练简单线性回回归模型"></a>通过训练集来训练简单线性回回归模型</h3><p>&emsp;&emsp;为了使用模型来训练数据集，这里使用python中的<code>sklearn.linear_model</code>库的<code>LinearRegression</code>类。然后实例化一个<code>LinearRegression</code>类的<code>regressor</code>对象。最后使用<code>LinearRegression</code>类的<code>fit()</code>方法。将<code>regressor</code>对象对数据集进行训练。</p><h3 id="预测结果"><a href="#预测结果" class="headerlink" title="预测结果"></a>预测结果</h3><p>&emsp;&emsp;现在将预测来自测试集的观察结果。将实际的输出保存在向量<code>Y_pred</code>中。使用前一步中训练的回归模型<code>regressor</code>的<code>LinearRegression</code>类的预测方法来对结果进行预测。</p><h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><p>&emsp;&emsp;为了直观的查看线性回归的效果，这里将对结果进行可视化。使用<code>matplotlib.pyplot</code>库对我们的训练结果和测试集结果做散点图，以查看模型的预测效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#python的数据处理库</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一步：数据预处理</span></span><br><span class="line">dataset = pd.read_csv(<span class="string">&#x27;../datasets/studentscores.csv&#x27;</span>)</span><br><span class="line">X = dataset.iloc[ : ,   : <span class="number">1</span> ].values</span><br><span class="line">Y = dataset.iloc[ : , <span class="number">1</span> ].values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = <span class="number">1</span>/<span class="number">4</span>, random_state = <span class="number">0</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步：训练集使用简单线性回归模型来训练</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor = regressor.fit(X_train, Y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步：预测结果</span></span><br><span class="line">Y_pred = regressor.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步：可视化</span></span><br><span class="line"><span class="comment"># 训练集结果可视化</span></span><br><span class="line">plt.scatter(X_train , Y_train, color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.plot(X_train , regressor.predict(X_train), color =<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集结果可视化</span></span><br><span class="line">plt.scatter(X_test , Y_test, color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.plot(X_test , regressor.predict(X_test), color =<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>&emsp;&emsp;训练集上的拟合结果：<br><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1547370582187.jpg" alt="enter description here"></p><p>&emsp;&emsp;测试集上拟合结果<br><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1547370582188.jpg" alt="enter description here"></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://oysz2016.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://oysz2016.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>(IOU-Net)《Acquisition of Localization Confidence for Accurate Object Detection》论文笔记</title>
    <link href="https://oysz2016.github.io/post/4395ef5a.html"/>
    <id>https://oysz2016.github.io/post/4395ef5a.html</id>
    <published>2019-01-12T05:40:24.263Z</published>
    <updated>2019-03-17T05:48:10.514Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>论文链接：<a href="https://arxiv.org/abs/1711.07767">https://arxiv.org/abs/1711.07767</a><br>论文代码：<a href="https://github.com/ruinmessi/RFBNet">https://github.com/ruinmessi/RFBNet</a></p><p>&emsp;&emsp;目标检测框架中目前主要依靠边界框回归和非极大值抑制来定位对象，并且非极大值抑制算法去除重复框的依据是候选框的分类置信度，在这个过程中缺少了一个边框筛选的重要参考——<strong>定位置信度</strong>。因此这篇文章提出<strong>IOU-Net</strong>，IOU-Net具有以下优点：</p><ul><li>通过在网络中加入定位置信度，在NMS算法中用定位置信度代替分类置信度作为去重复框的参考</li><li>提出了一种<strong>基于优化的边界框修正</strong>算法</li></ul><h2 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h2><p>&emsp;&emsp;作者先根据实验定性的分析了仅用分类置信度评估检测框是否准确不太合适，缺少分类置信度回带来两个<strong>缺点</strong>：</p><ol><li>分类置信度被用作对检测框排序的度量标准，并且在NMS算法中忽略了定位的精度</li><li>缺少定位置信度，使得广泛使用BBox回归缺少可解释性，这点在《cascade r-cnn》中也有提到多次使用BBox回归可能导致检测框的局部退化。</li></ol><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292464876.jpg" alt="enter description here"></p><p>&emsp;&emsp;上图中，黄色框表示gt，绿色和红色都是是模型预测得到的检测框。如图(a)所示，绿色检测框与红色检测框相比更适合作为最终的预测结果，绿色框与gt的IOU(交并比)更高，定位置信度也更高，但分类置信度却低于红色的检测框，在以往NMS算法中以分类作为检测框排序标准，会保留红色的框，而滤去绿色的，这显然是不够合理的。图(b)比较的是基于优化的边界框修正比基于回归的边界框修正有更好的效果(多次应用基于回归的边界框修正，会导致检测框退化)<br>基于上图中目前目标检测框架的不足，作者IOU-Net所做的两点改进：</p><ol><li>通过在目标检测框架中引入定位置信度。在NMS算法中，预测检测框与gt之间IOU的顺序，代替原来的以分类置信度排序</li><li>提出了一种基于优化的边界框修正算法</li></ol><p>&emsp;&emsp;IOU-Net中的IOU指的就是<strong>网络预测到的检测框与gt之间的IOU</strong></p><h2 id="深入研究目标定位"><a href="#深入研究目标定位" class="headerlink" title="深入研究目标定位"></a>深入研究目标定位</h2><p>&emsp;&emsp;作者在绪论中提到了所作的两点改进，绪论中定性的用图表表明了缺少定位置信度带来的缺点，接下来分别做了几个实验定量的验证所提出方法的有效性。以下实验所采用的训练集是MS-COCO trainval35k,测试集用minival，检测框架用FPN网络。</p><h3 id="类别不匹配和定位精度"><a href="#类别不匹配和定位精度" class="headerlink" title="类别不匹配和定位精度"></a>类别不匹配和定位精度</h3><p>&emsp;&emsp;作者先简单介绍了NMS算法的原理，及关于这方面的研究进展。随后指出NMS算法中，以分类置信度作为检测框的度量标准不太合理。</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292464564.jpg" alt="enter description here"></p><p>&emsp;&emsp;如上图所示，(a)横轴是预测的检测框与gt之间的IOU,纵轴是分类置信度。(b)横轴同样是预测的检测框与gt之间的IOU,纵轴是定位置信度。若以检测框与gt之间的IOU大于0.5作为是否检测到的阈值，作者计算了两张图横纵轴的Pearson correlation(皮尔森相关系数<a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">[wiki百科]</a>)，结论是(a)的相关的相关系数为0.217，而(b)的相关性为0.617。<br>&emsp;&emsp;作者也总结了分类置信度与定位是否准确关系不大的原因：<strong>目标检测框架中分类和定位的相关性不强的原因在于正样本和负样本的选取方式，例如获取正样本只用其与gt之间的IOU大于指定阈值即可</strong>。关于正样本的生成方式，之前我也想过为什么这样生成，如此的生成方式一定程度上会带来定位不准确的问题。但是相比较于完全使用标注的gt这种方式具有以下的优点：</p><ul><li><strong><em>可以生成多个比例的候选框，覆盖样本的不同尺度，正样本会更好的回归</em></strong></li><li><strong><em>样本均衡一直是目标检测算法的一个难题，这种方式一定程度上可以生成更多的正样本</em></strong></li></ul><p>&emsp;&emsp;很明显正样本的选取方式带来的好处远大于其缺点，作者也表明，定位不准主要是缺少定位置信度的原因</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292464535.jpg" alt="enter description here"></p><p>&emsp;&emsp;传统NMS算法中由于用分类置信度作为MNS算法中检测框的度量标准，会导致与gt有更大IOU的检测框被抑制。如上图所示，蓝色代表传统的NMS算法；黄色表示以定位置信度作为度量标注的NMS算法；绿色表示不用NMS时，理论能生成的最多检测框数量。可以看到，在传统的NMS算法中由于缺少定位置信度，保留的检测框中与gt的IOU在0.9以上的会被抑制。</p><h3 id="BBox回归的非单调性"><a href="#BBox回归的非单调性" class="headerlink" title="BBox回归的非单调性"></a>BBox回归的非单调性</h3><p>&emsp;&emsp;faster rcnn中使用了两次边界框回归，以达到位置精修的目的。但位置精修的次数是不是越多越好，《cascade r-cnn》中也提到了这样的疑惑，作者做了个实验。</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292464267.jpg" alt="enter description here"></p><p>&emsp;&emsp;上图中，横轴代表边界框回归结构的迭代次数，纵轴代表检测精度。从上图可以看到，无论是FPN网络还是Cascade R-CNN网络，在多次使用边界框回归之后，都会出现退化现象。作者解释出现这种情况的是<strong>由于缺少定位置信度，对模型不能进行细粒度(fine-grained)的控制，例如对不同的检测框采用不同的迭代次数</strong>。</p><h2 id="IOU-Net"><a href="#IOU-Net" class="headerlink" title="IOU-Net"></a>IOU-Net</h2><p>&emsp;&emsp;为了定量分析IOU-Net的有效性，接下来作者给出了IOU-Net的框架和NMS算法中怎样用IOU进行预测以及基于回归的边界框精修算法。</p><h3 id="IOU-Net架构"><a href="#IOU-Net架构" class="headerlink" title="IOU-Net架构"></a>IOU-Net架构</h3><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292464269.jpg" alt="enter description here"></p><p>&emsp;&emsp;如上图所示，IOU-Net框架中提特征的网络部分与FPN网络一致，并估计每个边界框的定位精度(IOU)，通过数据扩充生成用于训练IOU-Net的边框和标签，而不是接受来自RPN的建议(说实话这里有点奇怪，RPN网络作用之一是用来位置精修的，不用之前RPN网络中的建议，这个没太理解作者的意思)。细节实施上，作者表明在生成正样本时，从候选集中移除与gt的IOU小于0.5的样本，然后对所有样本统一采样训练，据作者所说这样可以获得更好的性能和鲁棒性</p><h3 id="使用IOU度量的NMS"><a href="#使用IOU度量的NMS" class="headerlink" title="使用IOU度量的NMS"></a>使用IOU度量的NMS</h3><p>&emsp;&emsp;这里作者主要介绍了在NMS去除重复框时，如何使用IOU作为度量标准。伪代码如下所示：</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292464539.jpg" alt="enter description here"></p><p>&emsp;&emsp;特点是采用了基于聚类的规则来更新分类置信度。具体实施过程是当boxi将boxj移除了，会更新boxi的分类置信度为max(si,sj)</p><h3 id="将边界框精修作为一个优化过程"><a href="#将边界框精修作为一个优化过程" class="headerlink" title="将边界框精修作为一个优化过程"></a>将边界框精修作为一个优化过程</h3><p>&emsp;&emsp;边界框精修的数学公式定义如下：</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292463491.jpg" alt="enter description here"></p><p>&emsp;&emsp;其中 $box_{det}$是检测到的边界框，$box_{gt}$是实际的边界框.<code>trainsform</code>是用参数<code>c</code>对边界框变换的转换函数。$crit$是两个边界框的度量标准(这个标准在faster rcnn中是<code>smooth-L1</code>)</p><p>&emsp;&emsp;基于回归的算法用前馈神经网络直接估计最优解c* 。 然而，迭代边界框回归方法易受输入分布变化的影响，并可能导致非单调的回归退化，如图4所示。为了解决这些问题，作者提出一种基于优化的边界框精修方法。 利用IoU-Net作为鲁棒定位精度（IoU）估计器的方法。 此外，IoU估计器可以用作早期停止条件，以通过自适应步骤实现迭代精修。</p><p>&emsp;&emsp;IoU-Net直接估算IoU。 虽然所提出的精确RoI池化层能够计算关于边界框坐标的IoU的梯度，我们可以直接使用梯度上升方法找到方程1的最优解。在算法2中，将IoU的估计视为优化目标，我们迭代地使用计算的梯度更新边界框坐标和最大化检测到的边界框与其匹配的真值之间的IoU。 此外，预测的IoU是每个边界框上的定位置信度的可解释指示符，并且有助于解释所做的转换。</p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292463918.jpg" alt="enter description here"></p><p>&emsp;&emsp;关于算法2中的初始坐标，初始化采用了一次边界框回归。</p><h4 id="precise-ROI-Pooling"><a href="#precise-ROI-Pooling" class="headerlink" title="precise ROI Pooling"></a>precise ROI Pooling</h4><p>&emsp;&emsp;为了细粒度的修正边界框，作者引入了一种新的ROI Pooling方法，以代替之前的ROI Pooling和ROI Align。<strong>关于ROI Pooling到PrROI Pooling我会专门写一篇博文比较其区别，这里就先简单介绍下。</strong></p><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292463702.jpg" alt="enter description here"><br>&emsp;&emsp;如上图，绿色的点代表特征图(feature map)的值，虚线代表目标在映射在特征图中实际的位置。<br>RoI Pooling：ROI Pooling的弊病，在于其在特征图中的边界框只能取整数，因此会从虚线取整到实线，映射回原图时，会带来误差。<br>ROI Align:为了避免ROI pooling中取整带来的误差。ROI Align中会保持浮点数边界不变，将特征图中的值用双线性插值，由绿色的点映射到图中的4个红点。然后再做池化<br>PrRoI Pooling: 如果付出浮点数边界内固定的点来做池化，这样对于目标实际的映射大小不具有适应性。PrRoI Pooling则使用二阶积分来做池化 </p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>&emsp;&emsp;针对作者提到的几个方法，分别做了几组实验。</p><ul><li>下面的表1总结了在不同检测框架中，使用不同NMS算法性能上的变化。可以看到IOU指导的NMS算法，在高IOU指标上性能会优于其他算法</li></ul><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292463704.jpg" alt="enter description here"></p><ul><li>基于优化的边界框精修</li></ul><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292463705.jpg" alt="enter description here"></p><ul><li>耗时实验：耗时还在可以接受的范围</li></ul><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/Blog/IOU-Net/1547292463508.jpg" alt="enter description here"></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="目标检测" scheme="https://oysz2016.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>《Receptive Field Block Net for Accurate and Fast Object Detection》论文笔记</title>
    <link href="https://oysz2016.github.io/post/f38a4f4.html"/>
    <id>https://oysz2016.github.io/post/f38a4f4.html</id>
    <published>2019-01-06T07:50:46.841Z</published>
    <updated>2019-03-17T05:48:22.182Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>论文链接：<a href="https://arxiv.org/abs/1711.07767">https://arxiv.org/abs/1711.07767</a><br>论文代码：<a href="https://github.com/ruinmessi/RFBNet">https://github.com/ruinmessi/RFBNet</a></p><p>&emsp;&emsp;这篇文章是CV领域顶会ECCV2018中关于目标检测的文章,文中以SSD模型为基础提出了RFB结构,强调兼顾速度与性能。说来也巧,因为项目需要,在看这篇论文之前正好看过提出dilated convolution的那篇文章，但是dilated convolution的结构获得更大感受野的方式确实对细粒度的分割会比较好，适用图像分割领域。当我还在想能怎么用在目标检测上时，就看到了RFB网络。虽然作者说是为了兼顾速度与性能将其应用到one stage的SSD上，但我也在two stage的faster rcnn上，复现出了较好的效果。这是一篇我个人很喜欢的文章，实验充分，模拟视觉细胞的结构让我觉得即简单又巧妙。因此简单总结下这篇文章。</p><h2 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h2><p>&emsp;&emsp;作者指出目前图像领域深度学习的发展越来越倾向于用更深的网络以达到更好的效果，然而像ResNet等很深的网络往往具有较大的计算量，导致速度受限。相比之下作者提出的RFB结构具有以下优点：</p><ol><li>模拟了人类视觉系统RFs的大小和离心率设置，增强轻量级CNN网络的特征提取能力</li><li>简单的替换了SSD的最后一级卷积层，在较少的计算增加的情况下，提升了模型的性能</li><li>除了SSD之外，也扩展到了MobileNet中取得了较好的结果，展示了结构的泛化性</li></ol><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>&emsp;&emsp;这部分就不总结了，主要介绍了one stage和two stage的目标检测模型和目前论文中在感受野上做的研究。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="视觉皮层"><a href="#视觉皮层" class="headerlink" title="视觉皮层"></a>视觉皮层</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546754014816.jpg" alt="enter description here"><br>&emsp;&emsp;如上图所示是人类感受野(pRF)的示意图,可以看到有以下规律:</p><ul><li>距离中心越远的pRF越大，即pRF大小与偏心含有正相关的关系</li><li>不同图谱的pRF大小规模不同</li></ul><h3 id="感受野块"><a href="#感受野块" class="headerlink" title="感受野块"></a>感受野块</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546754512995.jpg" alt="enter description here"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546759194131.jpg" alt="enter description here"><br>&emsp;&emsp;作者提出的RFB结构的原理如上图所示，该结构的特点有：</p><ul><li>多分支卷积层：根据之前人类感受野(pRF)的示意图，为了仿照不同图谱的pRF大小规模不同，作者提出用不同大小的卷积核以实现多大小的pRF，这一方法应该优于共享固定大小的RFs。这一结构参考了Inception的结构。</li><li>膨胀卷积和池化层：<strong>膨胀卷积的基本意图在于生成分辨率更高的特征图</strong>，在相同计算量的情况下获得更大的感受野。而膨胀卷积核的大小和扩张与pRFs在视觉皮层的大小和偏心具有相似的功能关系。然后再将不同膨胀卷积处理过的层融合起来，以达到视觉皮层中感受野的效果。rFB的结构如下图所示：</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546755296592.jpg" alt="enter description here"></p><h3 id="RFB检测框架"><a href="#RFB检测框架" class="headerlink" title="RFB检测框架"></a>RFB检测框架</h3><p>&emsp;&emsp;作者提出的RFB的结构是在SSD的基础上改的，做的修改及替换如下图所示：</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546755756172.jpg" alt="enter description here"></p><ul><li>轻量级的结构：这里主要说的是SSD的有点，这里不赘述</li><li>多尺度结构中的RFB：在原始的SSD中，有着层叠的卷积层，形成一系列空间分辨率连续下降、感受野不断增大的feature map。在作者的实现中，保持了相同的SSD级联结构，<strong>但具有较大感受也的卷积层被RFB结构替代</strong>。作者还指出最后基层卷积层的特征图太小，适合用5X5大小的卷积核。<strong>这部分论文里Fig.4的a图中用的确实是5x5的卷积核，但是给出的代码中却用两个3x3的卷积核替代了，这部分我有点疑惑。</strong></li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>&emsp;&emsp;这一部分就是各种各样的实验图表了，也不赘述。从以下图表可以看到实验结果确实很惊艳，用了RFB结构的网络mAP会有不小的提升。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546756443789.jpg" alt="enter description here"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546756466104.jpg" alt="enter description here"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546756478399.jpg" alt="enter description here"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/RFB/1546757250177.jpg" alt="enter description here"></p><p>&emsp;&emsp;作者还给出了一张目前目标检测算法的准确率和耗时的图片，对比的多是one stage的模型，可以作为参考</p><p><img data-src="https://user-gold-cdn.xitu.io/2019/1/6/1682321abe0c6f46?w=717&amp;h=489&amp;f=png&amp;s=96823" alt=""></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="目标检测" scheme="https://oysz2016.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>Faster R-CNN代码的caffe封装</title>
    <link href="https://oysz2016.github.io/post/fc7cd866.html"/>
    <id>https://oysz2016.github.io/post/fc7cd866.html</id>
    <published>2019-01-06T03:59:02.718Z</published>
    <updated>2019-03-17T05:48:40.004Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>目前部署服务的主流语言还是C++，因此项目上线前，在需要部署调试的时候需要对Faster R-CNN工程化为<code>C++</code>代码。这篇博文总结的部分主要将python版本的demo.py代码及其相关的部分改写成<code>C++</code>版本。</p><h2 id="封装1"><a href="#封装1" class="headerlink" title="封装1"></a>封装1</h2><p>封装听起来很复杂，其实就是隐藏代码的实现细节，只暴露出对应的接口。Faster R-CNN具有很广泛的流行度了，相关的资料在可以说在目标检测模型里是最多的。<br>为了方便理解，首先给出<code>C++</code>工程demo的目录结构。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">└── lib</span><br><span class="line">│   │── faster_rcnn<span class="selector-class">.cpp</span></span><br><span class="line">│   │── faster_rcnn<span class="selector-class">.hpp</span></span><br><span class="line">│   │── CMakeLists<span class="selector-class">.txt</span></span><br><span class="line">│—— CMakeLists<span class="selector-class">.txt</span></span><br><span class="line">│—— <span class="selector-tag">main</span>.cpp</span><br></pre></td></tr></table></figure><p>其中faster_rcnn.cpp与faster_rcnn.hpp是对应的demo接口，main.cpp可以直接调用。<br>faster_rcnn.cpp文件如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">ifndef</span> FASTER_RCNN_HPP</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> FASTER_RCNN_HPP</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span>  <span class="comment">// for snprintf</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;boost/python.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;caffe/caffe.hpp&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;gpu_nms.hpp&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/highgui/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/imgproc/imgproc.hpp&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> caffe;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> max(a, b) (((a)&gt;(b)) ? (a) :(b))</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> min(a, b) (((a)&lt;(b)) ? (a) :(b))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//background and car</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> class_num=<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * ===  Class  ======================================================================</span></span><br><span class="line"><span class="comment"> *         Name:  Detector</span></span><br><span class="line"><span class="comment"> *  Description:  FasterRCNN CXX Detector</span></span><br><span class="line"><span class="comment"> * =====================================================================================</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Detector</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Detector</span>(<span class="type">const</span> string&amp; model_file, <span class="type">const</span> string&amp; weights_file);</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">Detect</span><span class="params">(<span class="type">const</span> string&amp; im_name)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">bbox_transform_inv</span><span class="params">(<span class="type">const</span> <span class="type">int</span> num, <span class="type">const</span> <span class="type">float</span>* box_deltas, <span class="type">const</span> <span class="type">float</span>* pred_cls, <span class="type">float</span>* boxes, <span class="type">float</span>* pred, <span class="type">int</span> img_height, <span class="type">int</span> img_width)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">vis_detections</span><span class="params">(cv::Mat image, <span class="type">int</span>* keep, <span class="type">int</span> num_out, <span class="type">float</span>* sorted_pred_cls, <span class="type">float</span> CONF_THRESH)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">boxes_sort</span><span class="params">(<span class="type">int</span> num, <span class="type">const</span> <span class="type">float</span>* pred, <span class="type">float</span>* sorted_pred)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    shared_ptr&lt;Net&lt;<span class="type">float</span>&gt; &gt; net_;</span><br><span class="line">    <span class="built_in">Detector</span>()&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Using for box sort</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Info</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> score;</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span>* head;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">compare</span><span class="params">(<span class="type">const</span> Info&amp; Info1, <span class="type">const</span> Info&amp; Info2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Info1.score &gt; Info2.score;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure><p>其对应的faster_rcnn.hpp文件为</p><figure class="highlight hpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">ifndef</span> FASTER_RCNN_HPP</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> FASTER_RCNN_HPP</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span>  <span class="comment">// for snprintf</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;boost/python.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;caffe/caffe.hpp&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;gpu_nms.hpp&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/highgui/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/imgproc/imgproc.hpp&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> caffe;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> max(a, b) (((a)&gt;(b)) ? (a) :(b))</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> min(a, b) (((a)&lt;(b)) ? (a) :(b))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//background and car</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> class_num=<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * ===  Class  ======================================================================</span></span><br><span class="line"><span class="comment"> *         Name:  Detector</span></span><br><span class="line"><span class="comment"> *  Description:  FasterRCNN CXX Detector</span></span><br><span class="line"><span class="comment"> * =====================================================================================</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Detector</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Detector</span>(<span class="type">const</span> string&amp; model_file, <span class="type">const</span> string&amp; weights_file);</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">Detect</span><span class="params">(<span class="type">const</span> string&amp; im_name)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">bbox_transform_inv</span><span class="params">(<span class="type">const</span> <span class="type">int</span> num, <span class="type">const</span> <span class="type">float</span>* box_deltas, <span class="type">const</span> <span class="type">float</span>* pred_cls, <span class="type">float</span>* boxes, <span class="type">float</span>* pred, <span class="type">int</span> img_height, <span class="type">int</span> img_width)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">vis_detections</span><span class="params">(cv::Mat image, <span class="type">int</span>* keep, <span class="type">int</span> num_out, <span class="type">float</span>* sorted_pred_cls, <span class="type">float</span> CONF_THRESH)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">boxes_sort</span><span class="params">(<span class="type">int</span> num, <span class="type">const</span> <span class="type">float</span>* pred, <span class="type">float</span>* sorted_pred)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    shared_ptr&lt;Net&lt;<span class="type">float</span>&gt; &gt; net_;</span><br><span class="line">    <span class="built_in">Detector</span>()&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Using for box sort</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Info</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> score;</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span>* head;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">compare</span><span class="params">(<span class="type">const</span> Info&amp; Info1, <span class="type">const</span> Info&amp; Info2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Info1.score &gt; Info2.score;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure><p>初次在linux上运行C++代码时，真是各种蒙圈，看着Makefile文件也搞不明白其中关键字代表的意义，使用cmake后，感觉容易了很多。如下是lib文件夹底下，即<code>faster_rcnn.cpp</code>对应的<code>CMakeLists.txt</code>文件。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span> (VERSION <span class="number">2.8</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> (SRC_LIST faster_rcnn.cpp)</span><br><span class="line"><span class="keyword">include_directories</span> ( <span class="string">&quot;$&#123;PROJECT_SOURCE_DIR&#125;/../../caffe-fast-rcnn/include&quot;</span></span><br><span class="line">    <span class="string">&quot;$&#123;PROJECT_SOURCE_DIR&#125;/../../lib/nms&quot;</span> </span><br><span class="line">    /usr/local/<span class="keyword">include</span> </span><br><span class="line">    /usr/<span class="keyword">include</span>/python2.<span class="number">7</span></span><br><span class="line">    /usr/local/cuda/<span class="keyword">include</span> )</span><br><span class="line"><span class="keyword">add_library</span>(faster_rcnn SHARED <span class="variable">$&#123;SRC_LIST&#125;</span>)</span><br></pre></td></tr></table></figure><p>然后依次执行<code>cmake.</code>和<code>make</code>就完成编译了。接下来是调用<code>faster_rcnn.cpp</code>接口的<code>main.cpp</code>文件。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;faster_rcnn.hpp&quot;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    string model_file = <span class="string">&quot;/home/ouyang/Program/py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt&quot;</span>;</span><br><span class="line">    string weights_file = <span class="string">&quot;/home/ouyang/Program/py-faster-rcnn/output/faster_rcnn_end2end/voc_2007_trainval/vgg16_faster_rcnn_iter_170000.caffemodel&quot;</span>;</span><br><span class="line">    <span class="type">int</span> GPUID=<span class="number">2</span>;</span><br><span class="line">    Caffe::<span class="built_in">SetDevice</span>(GPUID);</span><br><span class="line">    Caffe::<span class="built_in">set_mode</span>(Caffe::CPU);</span><br><span class="line">    Detector det = <span class="built_in">Detector</span>(model_file, weights_file);</span><br><span class="line">    det.<span class="built_in">Detect</span>(<span class="string">&quot;/home/ouyang/Program/py-faster-rcnn/data/demo/90.jpg&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>里面的pt文件和model文件的路径替换成对应的就好。然后是main.cpp对应的CMakeLists.txt文件。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#This part is used for compile faster_rcnn_demo.cpp</span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span> (VERSION <span class="number">2.8</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">project</span> (main_demo)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(main main.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">include_directories</span> ( <span class="string">&quot;$&#123;PROJECT_SOURCE_DIR&#125;/../caffe-fast-rcnn/include&quot;</span></span><br><span class="line">    <span class="string">&quot;$&#123;PROJECT_SOURCE_DIR&#125;/../lib/nms&quot;</span> </span><br><span class="line">    <span class="string">&quot;$&#123;PROJECT_SOURCE_DIR&#125;/lib&quot;</span> </span><br><span class="line">    /usr/local/<span class="keyword">include</span> </span><br><span class="line">    /usr/<span class="keyword">include</span>/python2.<span class="number">7</span></span><br><span class="line">    /usr/local/cuda/<span class="keyword">include</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(main /home/ouyang/Program/py-faster-rcnn/Cplusplus2/lib/libfaster_rcnn.so</span><br><span class="line">    /home/ouyang/Program/py-faster-rcnn/caffe-fast-rcnn/build/lib/libcaffe.so</span><br><span class="line">    /home/ouyang/Program/py-faster-rcnn/lib/nms/libgpu_nms.so </span><br><span class="line">    /usr/local/lib/libopencv_highgui.so </span><br><span class="line">    /usr/local/lib/libopencv_core.so  </span><br><span class="line">    /usr/local/lib/libopencv_imgproc.so </span><br><span class="line">    /usr/local/lib/libopencv_imgcodecs.so</span><br><span class="line">    /usr/lib/x86_64-linux-gnu/libglog.so</span><br><span class="line">    /usr/lib/x86_64-linux-gnu/libboost_system.so</span><br><span class="line">    /usr/lib/x86_64-linux-gnu/libboost_python.so</span><br><span class="line">    /usr/lib/x86_64-linux-gnu/libglog.so</span><br><span class="line">    /usr/lib/x86_64-linux-gnu/libpython2.<span class="number">7</span>.so</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>相同的执行<code>cmake.</code>和<code>make</code>完成该部分的编译。</p><h2 id="封装2"><a href="#封装2" class="headerlink" title="封装2"></a>封装2</h2><p>上面<code>C++</code>封装的Faster R-CNN结构简单，代码量也少，部署起来很方便。但在项目中也有着致命的弱点，检测速度大概比python版的Faster R-CNN慢了一个数量级。因此很有必要寻求另外的封装方式，这里参考<a href="https://github.com/D-X-Y/caffe-faster-rcnn">D-X-Y</a>纯<code>C++</code>版的Faster R-CNN代码，该代码将所有代码都改写为了C++版本，不像上一部分的代码中调用了很多python的库。<br>D-X-Y所封装的代码中对应原python版本中demo.py的接口在<code>G:\gitProgram\caffe-faster-rcnn\src\api\FRCNN\frcnn_api.cpp</code>和<code>G:\gitProgram\caffe-faster-rcnn\include\api\FRCNN\frcnn_api.hpp</code>。这里我将所需要的文件整合到一起，目录结构为：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">└── lib</span><br><span class="line">│   │── frcnn_api<span class="selector-class">.cpp</span></span><br><span class="line">│   │── frcnn_api<span class="selector-class">.hpp</span></span><br><span class="line">│   │── CMakeLists<span class="selector-class">.txt</span></span><br><span class="line">│—— CMakeLists<span class="selector-class">.txt</span></span><br><span class="line">│—— <span class="selector-tag">main</span><span class="selector-class">.cpp</span></span><br><span class="line">└── include</span><br><span class="line">│   │── api</span><br><span class="line">│   │    api<span class="selector-class">.hpp</span></span><br><span class="line">│   │   └── FRCNN</span><br><span class="line">│   │   │   │── frcnn_api<span class="selector-class">.hpp</span></span><br><span class="line">│   │   │   │── rpn_api<span class="selector-class">.hpp</span></span><br><span class="line">│   │── caffe</span><br><span class="line">│   │   │──proto</span><br><span class="line">│—— libcaffe<span class="selector-class">.so</span>.<span class="number">1.0</span>.<span class="number">0</span></span><br><span class="line">│—— libcaffe<span class="selector-class">.so</span></span><br><span class="line">│—— test.prototxt</span><br></pre></td></tr></table></figure><br>上述代码结构中的<code>include</code>来自D-X-Y代码中的<code>include</code>,<code>/include/caffe/proto</code>来自D-X-Y代码中编译后产生，对应的路径为<code>./caffe-faster-rcnn/.build_release/src/caffe/proto</code>。<br><strong>第二种方式就不贴对应的代码了，在这里我也将两种方式对应的代码放在github上，供大家参考</strong><a href="https://github.com/oysz2016/cpp_faster-rcnn">代码</a></p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号</font></strong></p></blockquote><p><img data-src="https://blog-1258449291.cos.ap-chengdu.myqcloud.com/小书匠/1552801555033.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习笔记" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="代码部署" scheme="https://oysz2016.github.io/tags/%E4%BB%A3%E7%A0%81%E9%83%A8%E7%BD%B2/"/>
    
  </entry>
  
  <entry>
    <title>写博客的第100天&amp;北漂的第30天</title>
    <link href="https://oysz2016.github.io/post/6341d9b.html"/>
    <id>https://oysz2016.github.io/post/6341d9b.html</id>
    <published>2018-08-13T15:32:35.365Z</published>
    <updated>2022-11-27T02:15:34.326Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="d50580c7158a15db14a06dc3f972ec347978e8f1d88a5da7d3f459c450d2f3da">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">Here&#39;s something encrypted, password is required to continue reading.</summary>
    
    
    
    <category term="随笔" scheme="https://oysz2016.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="https://oysz2016.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
</feed>
