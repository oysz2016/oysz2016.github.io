<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>冲弱&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/ab8296a41c8b88ea9f8a771cd548cb5e</icon>
  <subtitle>多阅读 多积累</subtitle>
  <link href="https://oysz2016.github.io/atom.xml" rel="self"/>
  
  <link href="https://oysz2016.github.io/"/>
  <updated>2023-10-05T02:42:42.156Z</updated>
  <id>https://oysz2016.github.io/</id>
  
  <author>
    <name>冲弱</name>
    <email>ouyang-sz@foxmail.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>BLIP——统一理解与生成的多模态模型</title>
    <link href="https://oysz2016.github.io/post/23099eb6.html"/>
    <id>https://oysz2016.github.io/post/23099eb6.html</id>
    <published>2023-09-28T10:07:18.324Z</published>
    <updated>2023-10-05T02:42:42.156Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2201.12086"><strong>https://arxiv.org/abs/2201.12086</strong></a><br /><strong>代码链接:</strong><a href="https://github.com/salesforce/BLIP"><strong>https://github.com/salesforce/BLIP</strong></a><br /> <strong>作者blog链接:</strong><a href="https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/"><strong>https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/</strong></a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>BLIP的来源于标题中的<strong>B</strong>ootstrapping <strong>L</strong>anguage-<strong>I</strong>mage <strong>P</strong>re-training for Unified Vision-Language Understanding and Generation。从标题可以看出来BLIP也是文本-图像的预训练模型，并且目的是统一视觉-语言的理解和生成任务。同时支持和理解&amp;生成任务正是BLIP和之前任务的差异。<br />BLIP可以看作是<a href="https://zhuanlan.zhihu.com/p/655552483">ALBEF</a>的续作，一作是同一个人，并且都是多模态的领域，多模态任务展现出了比单模态更好的效果，但存在一些局限性:</p><ul><li><strong>模型角度: </strong>目前多模态的任务主要分为两种，分别是encoder-based model和encoder-decoder model, 都存在一些问题<ul><li><strong>encoder-based model: </strong>典型代表是CLIP, 因为预训练的时候没有生成任务，所以在下游任务中很难支持生成任务。例如CLIP不能完成image caption任务.</li><li><strong>encoder-decoder model: </strong>典型代表是SimVLM，该类方法目前在图文检索任务中的效果非常差</li></ul></li><li><strong>数据角度: </strong>通过互联网上爬取到的图像文本对，含有很多噪声</li></ul><p>相对于ALBEF，BLIP中提出了MED和CapFilt两个模块解决上面提到的两个局限性</p><h2 id="BLIP"><a href="#BLIP" class="headerlink" title="BLIP"></a>BLIP</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/1.png" alt=""></p><p>BLIP的网络结构如上图所示, 还是和ALBEF一样，将其拆分为几个部分。BLIP网络的构成包含3个encoder和1个decoder、3个预训练任务还有数据相关的CapFilt模块</p><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>MED的全称是multimodal mixture of encoder-decoder，即包含encoder和decoder的多模态融合，encoder的任务可以做多模态的理解，而decoder任务可以做多模态的生成。分别有以下部分:</p><ul><li><strong>单模态encoder: </strong>该部分包含两个encoder，分别是image encoder和text encoder，也是Figure 2中的前两个模块。分别对使用VIT和BERT提取图像和文本的特征。</li><li><strong>基于图像的文本encoder: </strong>该部分和Text encoder的区别在于引入了image encoder产生的特征做cross attention。因为包含图像和文本特征，会用来做图像文本匹配(ITM)任务</li><li><strong>基于图像的文本解码器: </strong>该部分和基于图像的文本encoder的区别在于将自注意力层替换为了因果自注意力层。用于做语言模型(LM)的任务.</li></ul><p>需要注意的是text相关的encoder和decoder有三个，text encoder和Image-grounded Text encoder的共有结构特征是共享的，为了标记差异，在文本的开头分别用”[CLS]”和”[Encoder]”标记。而Image-grounded Text decoder中使用”[Decoder]”</p><h3 id="Pre-training-Objectives"><a href="#Pre-training-Objectives" class="headerlink" title="Pre-training Objectives"></a>Pre-training Objectives</h3><p>在预训练中有三个优化目标，有两个基于理解的预训练任务和一个基于生成的预训练任务组成。计算量比较大的image encoder只需要运算一次。</p><h4 id="Image-Text-Contrastive-Loss-ITC"><a href="#Image-Text-Contrastive-Loss-ITC" class="headerlink" title="Image-Text Contrastive Loss(ITC)"></a>Image-Text Contrastive Loss(ITC)</h4><p>该部分和ALBEF中一致，目的都是为了对齐视觉和文本模态的特征，并且也使用了momentum encoder产生伪标签作为监督信号。在代码实现上也和ALBEF中的ITC loss完全一致<br /><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/2.png" alt=""></p><h4 id="Image-Text-Matching-Loss-ITM"><a href="#Image-Text-Matching-Loss-ITM" class="headerlink" title="Image-Text Matching Loss (ITM)"></a>Image-Text Matching Loss (ITM)</h4><p>该部分同样了ALBEF中一致，训练目标都是判断图像和文本是否匹配，并且也挖掘了难负样本。代码实现上也和ALBEF中ITM loss的计算完全一致<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/3.png" alt=""></p><h4 id="Language-Modeling-Loss-LM"><a href="#Language-Modeling-Loss-LM" class="headerlink" title="Language Modeling Loss (LM)"></a>Language Modeling Loss (LM)</h4><p>该部分不同于ALBEF中的LM loss，在ALBEF中参考的是Bert中的完形填空任务，目的是完成语言模型的训练，并且能对齐图像和文本的特征。但和Bert类似的任务用途是做语义的理解，并不能做生成任务。因此在BLIP中更类似于GPT中预测下一个token的任务，区别在于BLIP中做的是image caption的任务，即生成图像对应的文本描述。<strong>在LM中和ITM中共享了具有共同结构的参数，可以提高模型训练效率，也能从多任务学习中获益.</strong><br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/4.png" alt=""></p><h3 id="CapFilt"><a href="#CapFilt" class="headerlink" title="CapFilt"></a>CapFilt</h3><p>在ALBEF中也讨论了用于预训练的图像-文本对是从互联网上获取，但这部分数据质量不高，在ALBEF中使用了momentum distillation产生软标签减小噪声数据的影响。momentum的trick在BLIP中也有，但BLIP中有更进一步减少脏数据的影响——试图将预训练数据的质量变高。粗略的理解可以看下图Figure 1，在MED的预训练任务中包含生成字幕的预训练任务也有判断图文是否匹配的预训练任务，因此可以让模型生成图片的描述(Captioner)，再通过Filter用于判断图像和文本是否匹配。在图1中，原始的图文不匹配，在最终预训练时会被过滤掉，而Captioner生成的文本和图片匹配，则在最终预训练时会保留生成的数据。<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/5.png" alt=""></p><p>更具体的CapFilt可以参考下图，原始的数据集包含两部分:</p><ul><li>互联网上收集的(Iw,Tw)，存在脏数据</li><li>人工标注(Ih,Th), 可以认为是干净数据</li></ul><p>在Model pretraining模块中会使用包含互联网收集和人工标注两部分数据用于MED部分的预训练，然后会只使用人工标注的干净数据做不同预训练任务的微调:</p><ul><li>只微调MED中的ITC&amp;ITM任务，让模型有更好的图像文本理解能力，用于在Filtering。判断图像和文本是否匹配，文本的来源可以是互联网收集的和图像匹配的文本，也可以是captioning生成的文本</li><li>只微调MED中的LM任务，让模型有更好的生成字幕能力</li></ul><p>经过Filtering后会保留下来的数据有三部分:</p><ul><li>人工标注的数据(Iw,Tw)</li><li>互联网收集的图像和文本匹配的数据(Iw,Tw)</li><li>Captioner生成的和图像匹配的数据(Iw,Ts)</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/6.png" alt=""></p><h2 id="Experiments-and-Discussions"><a href="#Experiments-and-Discussions" class="headerlink" title="Experiments and Discussions"></a>Experiments and Discussions</h2><h3 id="Effect-of-CapFilt"><a href="#Effect-of-CapFilt" class="headerlink" title="Effect of CapFilt"></a>Effect of CapFilt</h3><p>关于CapFilter的效果在下表Table 1中可以看到，<strong>分别使用Captioner和Filter都可以提升下游任务上的效果，并且增大数据集的规模，提升会更明显。</strong></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/7.png" alt=""></p><p>下图展示了互联网上搜集到的文本和Captioner生成文本数据中一些噪声的示例。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/8.png" alt=""></p><h3 id="Diversity-is-Key-for-Synthetic-Captions"><a href="#Diversity-is-Key-for-Synthetic-Captions" class="headerlink" title="Diversity is Key for Synthetic Captions"></a>Diversity is Key for Synthetic Captions</h3><p>这一部分将CapFilt中Captioner生成字幕采取nucleus sampling(top-p)和beam search两种解码方式的差异，对比如下<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/9.png" alt=""></p><p>Nucleus的方式效果更好，相比Beam search的方法，前者生成的文本更加多样(不过也带来了更多的噪声数据)</p><h3 id="Parameter-Sharing-and-Decoupling"><a href="#Parameter-Sharing-and-Decoupling" class="headerlink" title="Parameter Sharing and Decoupling"></a>Parameter Sharing and Decoupling</h3><p>在预训练阶段，使用14M的数据做预训练。与不共享相比，共享除了Self Attention之外的能获得更好的效果，也能减少模型参数量，提升效率。而如果SA和CA层共享，由于编码和解码任务的冲突，模型的能力会变差</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/10.png" alt=""></p><p>在CapFilt阶段，下表中研究了captioner和filter保持和预训练中一样的参数共享方式或者解耦的差异，结果保持一样的参数共享方式，性能会有退化。作者将其归因于<strong>confirmation bias</strong>。因为如果Captioner和Filter也存在参数共享，Captioner生成的字幕会变得不容易被Filter判断为有噪声的数据。<br /></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/11.png" alt=""></p><h2 id="Comparison-with-State-of-the-arts"><a href="#Comparison-with-State-of-the-arts" class="headerlink" title="Comparison with State-of-the-arts"></a>Comparison with State-of-the-arts</h2><p>主要比较了几个多模态的任务:</p><ul><li>Image-Text Retrieval</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/12.png" alt=""></p><ul><li>Image Captioning</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/13.png" alt=""></p><ul><li>Visual Question Answering (VQA)</li><li>Natural Language Visual Reasoning (NLVR2)</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/14.png" alt=""></p><ul><li>Visual Dialog (VisDial)</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/15.png" alt=""></p><ul><li>Zero-shot Transfer to Video-Language Tasks</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/BLIP/16.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>BLIP和ALBEF的结构很相似，但关注点从除了单模态和多模态特征融合到开始关注文本的生成能力，其中一个预训练任务从类似Bert替换成了类似GPT，也顺应了NLP领域中的这一趋势。并且和其他大模型一样，更加关注数据的质量，用CapFilt模块获取了质量更高的数据。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="多模态" scheme="https://oysz2016.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>ALBEF:Align before Fuse</title>
    <link href="https://oysz2016.github.io/post/13dc672a.html"/>
    <id>https://oysz2016.github.io/post/13dc672a.html</id>
    <published>2023-09-11T02:33:27.961Z</published>
    <updated>2023-09-28T10:06:49.485Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span><br>本文首发于公众号“<a href="https://mp.weixin.qq.com/s/QhDyTSgs6-4asN4XFY1LYA">CVTALK</a>”。</p><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2107.07651">https://arxiv.org/abs/2107.07651</a></p><p><strong>代码链接:</strong><a href="https://github.com/salesforce/ALBEF">https://github.com/salesforce/ALBEF</a></p><p><strong>作者Blog链接:</strong><a href="https://blog.salesforceairesearch.com/align-before-fuse/">https://blog.salesforceairesearch.com/align-before-fuse/</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>语言和视觉是人类感知世界的两个最基本的渠道，因此多模态的算法里经常会将语言和视觉的模态结合。这一类方法被称为<strong>VLP(Vision-and-Language Pre-training)</strong>。然而现有的方法主要有以下局限.</p><p>之前多模态的方法大致可以分为两类，一类例如VL-Bert等，<strong>重点在于使用transformer作为encoder获取图像和文本交互的特征</strong>，由于视觉文本特征在transformer之前并没有对齐，因此在transformer任务中获取交互特征很具有挑战。为了让模型更容易学习交互特征，这类方法往往需要高分辨率的图像作为输入，并且需要预训练目标检测器(在<a href="https://arxiv.org/abs/2102.03334">ViLT</a>中尝试移除目标检测器，但效果有明显下降)。<strong>这类方法在需要结合视觉和文本的推理任务上效果较好，例如NLVR和VQA等任务中。</strong><br>第二种方法例如CLIP和ALIGN，<strong>重点在于分别学习图像和文本模型的任务，做特征对齐，使用对比学习让两个模态的特征都具有其他模态的信息</strong>。<strong>这类方法在图像-文本的检索任务上表现良好，但由于相比第一种方法缺乏图像和文本的交互特征，在更复杂的图像-文本交互任务上表现一般。</strong></p><p>在作者的<a href="https://blog.salesforceairesearch.com/align-before-fuse/">blog</a>中提到了之前方法的另外一点局限性:</p><blockquote><p>预训练的数据主要来源于互联网，数据来源并不干净。MLM等任务容易对噪声过拟合，从而影响到最终的特征。</p></blockquote><h2 id="ALBEF"><a href="#ALBEF" class="headerlink" title="ALBEF"></a>ALBEF</h2><p>ALBEF的名字来源于<u><strong>AL</strong></u>ign the image and text representations <u><strong>BE</strong></u>fore <u><strong>F</strong></u>using。论文的标题中也强调了Align before Fuse。区别于其他多模态方法，这篇文章提出在融合视觉-文本两个模态的特征之前，先做不同模态之间特征的对齐。出发点结合了上文提到的两类VLP方法的优点，并且减缓了脏数据对预训练的影响。</p><p>ALBEF的网络结构如下图所示:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/1.png" alt=""></p><p>ALBEF的结构并不复杂，弄清楚几个模块和loss也就明白其中的奥妙了。ALBEF包含3个encoder、3个loss和动量蒸馏模块。</p><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>encoder部分如下:</p><ul><li>image encoder: 12层的ViT-B/16</li><li>text encoder: bert中的前6层</li><li>multimodal encoder: Bert中的后6层，因为不同特征间需要attention，相比另外两个encoder多了cross attention的结构</li></ul><h3 id="Pre-training-Objectives"><a href="#Pre-training-Objectives" class="headerlink" title="Pre-training Objectives"></a>Pre-training Objectives</h3><p>训练目标主要包含三个预训练任务:</p><ul><li>image-Text contrastive learning(ITC): <strong>目标是在特征融合前对齐视觉和文本的特征以获取更好的单模态特征。</strong>受启发于moco，维护了两个队列分别存储动量编码器中的图像和文本特征。使用infoNCE loss分别计算image-to-text和text-to-image的损失。具体可以看下代码</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/2.png" alt="维护batch size大小的队列"></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/3.png" alt="计算encoder的特征和momentum encoder特征，infoNCE loss，更新队列"></p><ul><li>Image-Text Matching(ITM): <strong>预测图像和文本是否匹配。</strong>一般的ITM任务相对简单，loss能降到非常低，ALBEF为了让这项预训练任务变得更有意义，挖掘了困难的负样本。具体做法是在ITC任务中infoNCE loss有相似度的信息，选取和相似度最高，但不匹配的样本作为负样本。损失函数是交叉熵损失。</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/4.png" alt=""></p><ul><li>Masked Language Modeling(MLM): <strong>训练语言模型。</strong>随机将15%的文本mask，利用图像和文本的上下文还原mask的区域，使用交叉熵损失。为了避免脏数据的影响，使用的是伪标签。具体而言存在teacher模型，从student模型采用指数滑动平均(Exponential Moving Average, EMA)的方式更新其参数，而student模型的学习标签是teacher模型预测的embedding。</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/5.png" alt="MLM任务loss"></p><h3 id="动量蒸馏"><a href="#动量蒸馏" class="headerlink" title="动量蒸馏"></a>动量蒸馏</h3><p>由于预训练的图像文本对基本都来自互联网，存在脏数据。一般表现在文本中包含图像中没有的信息，或者图像中包含文本没有的信息。对于MLM和ITC任务会有较大的影响。<strong>因此受启发于moco，使用动量更新的方式构建teacher网络，利用teacher网络产生的伪标签作为监督信号。</strong>具体的实现方式可以参考上面ITC和MLM的代码。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="评测场景"><a href="#评测场景" class="headerlink" title="评测场景"></a>评测场景</h3><p>预训练好的模型应用在了5个场景，用于评估效果:</p><ul><li><strong>Image-Text Retrieval:</strong> 包含两个任务，图像到文本的检索和文本到图像的检索</li><li><strong>Visual Entailment:</strong> 视觉推理任务，预测图片和文本的关系是包含、中立还是相反的。</li><li><strong>Visual Question Answering:</strong> 提供图片和问题，预测答案</li><li><strong>Natural Language for Visual Reasoning:</strong> 预测提供的文本是否描述了图片</li><li><strong>Visual Grounding:</strong> 在图像中定位出文本中描述的物体的坐标</li></ul><h3 id="Evaluation-on-the-Proposed-Methods"><a href="#Evaluation-on-the-Proposed-Methods" class="headerlink" title="Evaluation on the Proposed Methods"></a>Evaluation on the Proposed Methods</h3><p>展示了ALBEF使用不同模块在下游任务上的效果</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/6.png" alt=""></p><h3 id="Evaluation-on-Image-Text-Retrieval"><a href="#Evaluation-on-Image-Text-Retrieval" class="headerlink" title="Evaluation on Image-Text Retrieval"></a>Evaluation on Image-Text Retrieval</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/7.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/8.png" alt=""><br>上面的图分别展示了微调和zero-shot在图文检索任务和之前方法的对比</p><h3 id="Evaluation-on-VQA-NLVR-and-VE"><a href="#Evaluation-on-VQA-NLVR-and-VE" class="headerlink" title="Evaluation on VQA, NLVR, and VE"></a>Evaluation on VQA, NLVR, and VE</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/9.png" alt=""></p><p><strong>使用4M的图片做预训练取得了比之前模型更好的效果，并且数据集增大到14M性能普遍有进一步提升。并且由于没有使用检测器，并且输入图片分辨率更低，推理耗时比VILLA减少10倍</strong></p><h3 id="Weakly-supervised-Visual-Grounding"><a href="#Weakly-supervised-Visual-Grounding" class="headerlink" title="Weakly-supervised Visual Grounding"></a>Weakly-supervised Visual Grounding</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/10.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/11.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/12.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/13.png" alt=""></p><p>在不同任务上可视化cross attention的特征图，和文本描述的区域匹配</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/14.png" alt=""><br>研究了在文本-图像检索任务上困难负样本在图文检索任务上的影响</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/ALBEF/15.png" alt=""><br>在NLVR任务中研究了文本分配(text assignment, TA任务)和不同特征共享方式的影响，细节配置可以看文章的第五部分(Downstream V+L Tasks)</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>ALBEF比之前多模态的方法效果好有几个关键点:</p><ul><li><strong>在融合不同模态特征前，做了特征的对齐</strong></li><li><strong>使用momentum distillation避免了噪声数据在预训练的影响</strong></li><li><strong>区别于之前ITC任务使用了难负例</strong></li></ul><p><strong>并且由于没有目标检测器，下游任务使用时使用更小的图片输入，耗时会极大的缩小</strong></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="多模态" scheme="https://oysz2016.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>HuggingGPT——GPT与HuggingFace的火花</title>
    <link href="https://oysz2016.github.io/post/17a33c92.html"/>
    <id>https://oysz2016.github.io/post/17a33c92.html</id>
    <published>2023-07-31T08:08:42.430Z</published>
    <updated>2023-08-06T14:18:51.093Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2303.17580">https://arxiv.org/abs/2303.17580</a></p><p><strong>代码链接:</strong> <a href="https://github.com/microsoft/JARVIS">https://github.com/microsoft/JARVIS</a></p><p>上一篇介绍过<a href="https://mp.weixin.qq.com/s/VXe6CHI_29Rw8xaOjfbqOQ">Visual Programming</a>, HiggingGPT的思想和其非常类似，也是用大语言模型做任务的拆解，再调用HuggingFace上的模型完成任务。</p><p><strong>因为太相似了，因此这篇文章主要介绍下HuggingGPT的实现方式，相对篇幅会短点，如果你对这类的文章感兴趣，可以看梳理的更详细的Visual Programming。</strong></p><p>HuggingGPT论文中的提供的例子还是很惊艳的，面对更加问题更加复杂的问题都能解决。而且HuggingGPT不光局限于视觉这一个模态，而是将任务划分为了NLP、CV、audio、video四种场景，每个场景都可以调用HuggingFace上的模型解决</p><p>HuggingGPT完成一个任务可以分为以下几步:</p><ul><li><strong>任务规划:</strong> 使用ChatGPT分析请求，将请求拆解成一系列下游任务</li><li><strong>模型选择:</strong> 根据拆解出的任务，和HuggingFace上模型的描述，选择合适的模型</li><li><strong>任务执行:</strong> 根据拆解的任务和选择的模型，调用模型并将结果返回和ChatGPT</li><li><strong>生成答案:</strong> ChatGPT汇总各个模型的结果，返回最终的答案</li></ul><h2 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h2><h3 id="任务规划"><a href="#任务规划" class="headerlink" title="任务规划"></a>任务规划</h3><p>有两种方式，分别是<strong>Specification-based Instruction</strong>和<strong>Demonstration-based Parsing</strong></p><p><strong>Specification-based Instruction:</strong> 提供规范的模版，期望LLM可以遵循一些特殊的规范(例如JSON)解析任务, 格式为<code>[&#123;&quot;task&quot;: task, &quot;id&quot;, task_id, &quot;dep&quot;: dependency_task_ids, &quot;args&quot;: &#123;&quot;text&quot;: text, &quot;image&quot;: URL, &quot;audio&quot;: URL, &quot;video&quot;: URL&#125;&#125;]</code></p><ul><li>task: 任务的类型，可以选择的类型如下:</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/HuggingGPT/1.png" alt=""></p><ul><li>id: 对任务唯一的标识</li><li>dep: 依赖</li><li>augs: 参数</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/HuggingGPT/2.png" alt=""></p><p><strong>Demonstration-based Parsing:</strong> 为了让GPT更好的理解任务规划的意图,可以在prompt中提供一些演示。例如希望做什么样的任务，这个任务期望的任务规划结果</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/HuggingGPT/3.png" alt=""></p><h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>完成任务规划后，需要选择最优的模型。HuggingGPT使用的是Hugging Face上的模型描述。让GPT选择最贴近任务，且效果最好的模型。因为文本长度的限制，不能把所有模型描述都作为prompt，需要根据先任务类型做筛选，再根据下载量排序，选择top-K的模型id和描述作为候选.<br>输入的prompt是提供一个模型列表,要求从中选择出最好的模型。输出是模型的id及选择的原因<code>&#123;&quot;id&quot;: &quot;id&quot;, &quot;reason&quot;: &quot;your detail reason for the choice&quot;&#125;</code></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/HuggingGPT/4.png" alt=""></p><h3 id="任务执行"><a href="#任务执行" class="headerlink" title="任务执行"></a>任务执行</h3><p>完成前两步后，这一步就是具体的模型执行。其中一个挑战是不同任务之间可能会存在依赖关系,解决方法是使用特殊的符号<code>&lt;resource&gt;</code>，代表着依赖关系，例如<code>&lt;resource&gt;-task_id</code>代表依赖task_id的任务。HuggingGPT在执行的时候会将该符号替换为先决任务。在执行的过程中，能保证先执行前置任务.</p><h3 id="生成答案"><a href="#生成答案" class="headerlink" title="生成答案"></a>生成答案</h3><p>Hugging GPT汇总之前三个步骤的信息，生成最终的答案.Prompt中也要求，直截了当的回答问题，并以第一人称的视角展示分析和模型的推理结果，如果包含输出文件的路径，需要补充出来，且如果回答不了问题，也需要告知</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/HuggingGPT/5.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="定性结果"><a href="#定性结果" class="headerlink" title="定性结果"></a>定性结果</h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/HuggingGPT/6.png" alt=""></p><p>任务: 生成一张女孩在读书的图像，且女孩的姿势和示例中的男孩相同，用你的声音描述生成的新图像</p><p>任务规划: </p><ul><li>1: 检测姿态</li><li>2: 根据姿态和文本”a reading girl”生成图像, 依赖任务1</li><li>3: 图像分类，作为描述图片的一部分信息, 依赖任务2</li><li>4: 目标检测, 获取物体框的类别和坐标，依赖任务2</li><li>5: 图生文, 作为描述图片的一部分信息, 依赖任务2</li><li>6: 文生音, 结合所有信息，及完成任务的过程总结输出，并转为音频, 依赖任务5</li></ul><p>模型选择：按task类型在Hugging Face选择模型</p><p>任务执行&amp;回答问题</p><h3 id="定性评估"><a href="#定性评估" class="headerlink" title="定性评估"></a>定性评估</h3><p>因为任务规划是整个方法的第一个模块，也决定了整个pipeline的效果，因此也可以看作是最重要的模块。主要分为三种类型:</p><ul><li><strong>Single Task:</strong> 问题的执行步骤只能拆分成一个任务，当标注的任务名称和预测的完全一致时，认为是正确。 评测精度/召回/F1</li><li><strong>Sequential Task:</strong> 问题的执行步骤能被拆分成一系列顺序执行的任务。评测精度/召回/F1的编辑距离</li><li><strong>Graph Task:</strong> 问题的执行步骤能被拆分成有向无环图组成的任务，因为图中有多种拓扑结构，仅使用F1不能反映LLM任务规划的能力，因为使用的是效果更好的LLM模型——GPT4评估所使用的LLM模型任务规划的好坏。</li></ul><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>使用GPT-4生成规划任务的伪标签，也邀请一些专家标注了一些复杂的请求。对比了Alpaca-7b、Vicuna-7b和GPT3.5三个模型的好坏</p><p>下面对比的是用GPT-4生成伪标签的数据集上，三个模型的评测效果。GPT3.5在基本所有任务上都优于另外两个模型。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/HuggingGPT/7.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/HuggingGPT/8.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/HuggingGPT/9.png" alt=""></p><p>下面对比的是在人工生成的模型上，Alpaca-7b、Vicuna-7b、GPT3.5和GPT-4模型的效果对比, 前三个模型的表现和用GPT-4生成伪标签的相似，并且GPT-4的效果大幅领先。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/HuggingGPT/10.png" alt=""></p><h2 id="存在的一些问题"><a href="#存在的一些问题" class="headerlink" title="存在的一些问题"></a>存在的一些问题</h2><ul><li>严重依赖LLM的能力，因此提高LLM任务规划的能力非常重要</li><li>整个系统的效率不高，整个系统高度依赖LLM，在解决一个较复杂问题的流程中会多次调用LLM，影响耗时。</li><li>Token长度的限制，在HuggingGPT中使用了一些规则让LLM对于判断的模型描述大大减少。但仍然存在可能超出长度限制的问题，因此总结模型描述也是一个问题。(LLM也具有总结一段文本的能力，应该也可以用LLM先总结描述，再执行判断)</li><li>整个系统效果不太稳定，因此LLM的输出结果是不可控的</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>HuggingGPT在该方法不足的部分提到了关于效率的问题，依赖LLM模型确实会带来耗时的增加。人类在思考的时候也会有类似的问题，《思考的快与慢》的序中有这样一则点评描写两种思维系统在所花时间上的差异。</p><blockquote><p>生活就像点菜，饥饿时菜会点得特别多，但吃一阵就会意识到浪费；如果慢条斯理地盘算怎么点菜，别人已经要吃完了。这就是决策的复杂性。</p></blockquote><p>有的场景更适合系统一，而有的场景更适合系统二。这两种解决问题的方式都很有价值, 而且这两篇文章类似系统二的思考方式可能是现有条件下实现通用智能最好的方式</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="通用人工智能" scheme="https://oysz2016.github.io/tags/%E9%80%9A%E7%94%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="多步骤推理" scheme="https://oysz2016.github.io/tags/%E5%A4%9A%E6%AD%A5%E9%AA%A4%E6%8E%A8%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Visual Programming——实现通用人工智能的另一种方式</title>
    <link href="https://oysz2016.github.io/post/516c1237.html"/>
    <id>https://oysz2016.github.io/post/516c1237.html</id>
    <published>2023-07-15T00:57:01.619Z</published>
    <updated>2023-07-31T08:10:23.199Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>论文链接: <a href="https://arxiv.org/abs/2211.11559">https://arxiv.org/abs/2211.11559</a></p><p>visual programming获得了2023年的best paper， 足以证明其论文质量。但这篇论文比较特殊，在其他模型强调自己模型结构新颖的时候，这篇文章的标题中的<strong>without training</strong>显得格格不入。</p><p><strong>如果抱着学习一些算法上的巧妙思路去阅读这篇文章可能会有些失望，但如果作为一个务实派，站在解决实际问题的角度，这篇文章的前瞻性，思考问题的方式和解决问题的工程能力绝对会让你眼前一亮！</strong></p><p><strong>既然更重要的是思考问题和解决问题的方式，区别于之前的一些论文解读文章，这篇文章在写的时候会更关注出发点和如何解决。</strong> 如果你只关心这篇文章大概是怎么做的，可以看<code>前言-&gt;VisProg方法总结</code>这一个小节。</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>AI的概念自1995年诞生以来，全球的研究者一直致力于创造的算法拥有的智能能和人类的智能相当甚至是超越，但是长久以来，受限于现有技术、 算力和研究者对通用智能的理解等因素。我们追求的一直是<strong>通用(general)</strong> 的智能， 而得到的是<strong>特定任务(task-specific)</strong> 或者是<strong>狭隘(narrow)</strong> 的智能。</p><p><strong>当前AI算法的任务普遍存在以下两个问题:</strong></p><ul><li><strong>定义明确</strong></li><li><strong>应用范围狭隘</strong></li></ul><p><strong>这两个问题导致目前模型和任务一一对应，扩展性弱。而现实世界中的问题定义不明确，场景多样。</strong> 理想和实际是割裂的，为了解决部分现实世界中的问题，需要对收集数据对场景定制模型，或者手动的写一些规则，调用多个模型适应现实不同的场景。</p><h3 id="VisProg方法总结"><a href="#VisProg方法总结" class="headerlink" title="VisProg方法总结"></a>VisProg方法总结</h3><p><strong>VisProg(Visual Programming)的目的是实现通用的智能，但区别于现在大模型想把尽量多的知识放在一个模型中的做法。VisProg利用现有的语言模型，将一个复杂的任务分解成多个子任务。</strong></p><p><strong>VisProg的思路很简单， 避免了任何模型的训练，使用的是大语言模型和in-context learning的技术，将一个复杂的任务拆解成类似python的模块化程序，然后逐个执行，最终解决问题。生成的模块化步骤可以用单个或者多个视觉模型/图像处理/python函数解决，并且前一步输出的结果可以给之后的步骤使用。</strong></p><h3 id="端到端和多步骤推理"><a href="#端到端和多步骤推理" class="headerlink" title="端到端和多步骤推理"></a>端到端和多步骤推理</h3><p>在心理学原理中有一个描述思维的<strong>双过程理论(dual-process theory)</strong>，《思考的快与慢》中也将思考的系统分为系统一和系统二:</p><ul><li><strong>系统一:</strong> 大脑对现状和刺激做出的快速、无意识的反应。可以看作是潜意识，大脑对该问题熟悉到使用历史知识做出反应可以得到正确的结果</li><li><strong>系统二:</strong> 大脑在解决复杂问题时缓慢、“需要动脑”和有逻辑的思考。</li></ul><p><strong>端到端的模型可以看作是系统一，而VisProg的多步骤推理可以看作系统二。</strong></p><p>下面尝试带入到一个“需要动脑”的具体任务中: 标记下图中’生活大爆炸’中的7个主角</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/1.jpg" alt=""></p><p>在我们思考这个问题的时候，会将这个任务拆解成如下几步:</p><ul><li>找出图像中的人脸</li><li>在大脑储备的知识中查找生活大爆炸的7个主角分别是谁，得到搜索结果</li><li>将人脸和大脑搜索引擎的结果匹配，给每个人脸一个分类结果</li><li>在图片中标记出人脸坐标和人名</li></ul><p><strong>在我们完成上述任务中思考的分布可以看作系统二，也就是多步骤推理；而每一步的具体执行可以看作系统一.也就是使用具体的端到端模型.</strong></p><h2 id="VisProg"><a href="#VisProg" class="headerlink" title="VisProg"></a>VisProg</h2><p><strong>得益于最近的一些进展:</strong></p><ul><li><strong>HuggingFace上有大量现成的视觉和语言模型</strong></li><li><strong>GPT等技术的发展，可以用自然语言产生分步的步骤或者具体的代码</strong></li></ul><p>VisProg是一个<strong>模块化</strong>且<strong>可解释</strong>的视觉系统. <strong>使用GPT3的语言理解和规划能力，将自然语言指令分解为具体的一些步骤，使用训练好的模型、图像处理代码和数学逻辑运算，执行拆解的步骤，以完成自然语言指令的任务。</strong> VisProg结构和可视化效果如下图所示:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/2.png" alt=""></p><p><strong>图片左侧是VisProg的网络结构图， 右侧是在4个任务上的可视化效果。</strong> 以VQA的任务为例，问题是照片上是否有领带和眼镜， 生成的步骤为:</p><ul><li>定位图片中的领带</li><li>计算含有领带的定位框数量， 记为answer0</li><li>定位图片中的眼镜</li><li>计算含有眼镜的定位框数量， 记为answer1</li><li>执行判断逻辑，answer0&gt;0 and answer1&gt;0，则answer2=’yes’，否则answer2=’no’</li><li>最终结果为answer2</li></ul><p>接下来看下图片左侧的算法结构</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/3.png" alt=""></p><p>VisProg由<strong>程序生成器</strong>和<strong>程序解释器</strong>两部分组成:</p><ul><li><p><strong>程序生成器（Program Generator）</strong>:</p><ul><li><strong>输入包含两部分，分别是自然语言指令和In-Context指令对，自然语言指令描述的是需要解决的问题，In-Contex指令对是和需要解决问题类似指令对。</strong> 期望GPT3能在推理时激活输入数据、预测标签对应的分布， 以及这种数据+label的语言表达形式。下面是一个具体的例子。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/4.png" alt=""><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/5.png" alt=""></li><li>输出是类似Python代码的执行步骤</li></ul></li><li><p><strong>程序解释器（Program interpreter):</strong></p><ul><li><strong>输入是图片和执行步骤， 输出是答案和可视化的视觉逻辑</strong></li></ul></li></ul><h3 id="Program-Execution"><a href="#Program-Execution" class="headerlink" title="Program Execution"></a>Program Execution</h3><p>在VisProg中， 每个模块都被实现为python代码。</p><ul><li><code>execute</code>解析步骤，返回输入变量名，输出变量名</li><li><code>inputs.append(state[var_name])</code>从state获取输入变量对应的值</li><li><code>output = some_computation(inputs)</code> 载入模型执行计算</li><li><code>state[output_var_name] = output</code>更新state中的变量</li><li><code>step_html = self.html(inputs，output)</code>HTML可视化每一步的输入输出</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/6.png" alt=""></p><h3 id="Visual-Rationale"><a href="#Visual-Rationale" class="headerlink" title="Visual Rationale"></a>Visual Rationale</h3><p>将每个模块的输入和输出可视化出来，用于分析逻辑和模块输出是否存在问题，可以调整指令</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/7.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>为了验证其有效性在4个任务上做了实验，并且关于in-context的数量做了消融实验。</p><h3 id="VQA"><a href="#VQA" class="headerlink" title="VQA"></a>VQA</h3><p><strong>在做VQA这个任务的时候，选择的是<a href="https://cs.stanford.edu/people/dorarad/gqa/index.html">GQA</a>的数据集</strong>。 GQA更适合VisProg将问题拆解为多步骤的算法。具体GQA和之前数据集的区别，可以去看<a href="https://cs.stanford.edu/people/dorarad/gqa/index.html">官网</a>或者参考<a href="https://zhuanlan.zhihu.com/p/64183181">这篇文章</a>。</p><p>VisProg使用的算法能力是VILT-VQA模型，和实验中需要对比的算法是同一模型，<strong>区别是VisProg不是直接将一个复杂的任务，让VILT-VQA返回输出，而是会做拆解。</strong> 例如对于具体的问题<code>图中的卡车是在戴头盔的人左边还是右边</code>，VisProg会先对戴头盔的人定位，再判断这些人的是否存在卡车。</p><p><strong>评估部分:</strong> GQA的数据集比较大，有约100种类型的VQA。为了省钱没有测试所有的数据，从每个类别中随机抽了5个验证集和20个测试集评估算法的效果。</p><p><strong>Prompt</strong> 随机抽了一些样本，人工标注了需要回答特定问题所需要的推理步骤</p><p>和用VILT在VQA任务上fintune之后模型的效果对比如下:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/9.png" alt=""></p><ul><li><strong>curated:</strong> 从大量的context examples中挑选20个比较好的例子作为输入</li><li><strong>random:</strong> 随机选24个content作为输入</li><li><strong>voting:</strong> 随机选24个context，但运行5次，投票的方式选出最优的</li></ul><h3 id="zero-shot-reasoning-on-image-pairs"><a href="#zero-shot-reasoning-on-image-pairs" class="headerlink" title="zero-shot reasoning on image pairs"></a>zero-shot reasoning on image pairs</h3><p><strong>区别于VQA中单张图片的问答，该任务特指对多张图像的问答。</strong> 现实场景中会存在这样的需求场景，例如用户需要手机相册帮忙回忆下”我在去完埃菲尔铁塔后的第二天，去参观了哪些地标。”，手机相册实现这样的智能需要具有解析多张图片的推理能力。</p><p>VisProg完成该任务的时候同样没有训练模型，GPT3得到执行步骤后，使用的是VILT-VQA+python运算。</p><p><strong>评估部分:</strong> 从NLVR2的开发集随机抽取了250条样本用于构造context的prompt，在完整的测试集上测试。</p><p>和用VILT在NLVR数据集上finetune的对比结果如下:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/10.png" alt=""></p><h3 id="Knowledge-tagging"><a href="#Knowledge-tagging" class="headerlink" title="Knowledge tagging"></a>Knowledge tagging</h3><p>在日常生活中，我们经常需要知道图片中人或物的名字， 例如需要识别名人、政治家，影视作品中某个角色的扮演者等等。<strong>为了完成此任务，不仅需要定位出人、物的位置，还需要从外部知识库中查找。</strong></p><p>为了使用VisProg解决这一问题，<strong>使用GPT-3作为知识的来源</strong>，例如问GPT-3生活大爆炸中主要角色的名字有哪些，得到人脸检测坐标后，再通过CLIP分类将人脸和角色名对应起来。</p><p><strong>评估部分:</strong> 在46张图片中，人工编写了100条指令，需要外部知识标记253个对象实例。通过taggeing和Localization两个任务的判断该任务的效果</p><p><strong>prompt:</strong> 在该任务上创建了14个in-context的例子</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/11.png" alt=""></p><ul><li>Original: 原始的结果</li><li>Modified: 根据Visual Rationale的结果调整后的结果</li></ul><h3 id="Image-editing"><a href="#Image-editing" class="headerlink" title="Image editing"></a>Image editing</h3><p>文生图这一领域由于DALL-E和Stable Diffusion的提出进展非常迅速，<strong>但是构造好的prompt和让模型按prompt生成图片还是有很多挑战。</strong> 在现实应用中有很多需要图像编辑的场景， 这里举一些例子:</p><ul><li>出于隐私保护的考虑，需要将某人的人脸换成一个表情包里的表情做遮挡</li><li>为了高亮显示某个人物/物体，需要将其他部分模糊掉</li><li>做一些P图，让某人戴上墨镜</li></ul><p>在VisProg中，需要先检测/分割出需要编辑的图像区域，再调用stable diffusion生成新的图像做mask的替换</p><p><strong>评估部分:</strong> 收集了65张图像对应107条指令，人工对效果打分</p><p><strong>Prompt:</strong> 在该任务上创建了10个in-context的例子</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/12.png" alt=""></p><h3 id="in-content数量的影响"><a href="#in-content数量的影响" class="headerlink" title="in-content数量的影响"></a>in-content数量的影响</h3><p><strong>该实验验证的是in-content示例数量在GQA和NLVRv2任务上的消融实验</strong>，有几个结论:</p><ul><li><strong>content examples的数量能提升最终的性能</strong></li><li><strong>运行多次后投票能提升最终的性能</strong></li><li><strong>context examples的数量对性能的影响，在NLVRv2任务上更容易达到的饱和。</strong> 原因是NLVRv2任务相对比较简单，需要的步骤比较少，因此用较少的context就能取得不错的效果</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/8.png" alt=""></p><h2 id="视觉原理"><a href="#视觉原理" class="headerlink" title="视觉原理"></a>视觉原理</h2><p>视觉原理模块主要有两个作用，分别是<strong>错误分析</strong>和<strong>指令调优</strong>。</p><h3 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h3><p>下图是在四个任务下，<strong>根据每一步可视化的结果分析造成错误的归因，有了错误原因可以针对性的调优。</strong> 例如在多图问答任务中，将VILT-VQA模型替换为VILT-NLVR模型优24%的性能提升。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/13.png" alt=""></p><h3 id="指令调优"><a href="#指令调优" class="headerlink" title="指令调优"></a>指令调优</h3><p><strong>对于利用知识定位物体和图像编辑任务，实验了调整指令对最终结果的影响</strong>，如下图所示，</p><ul><li>将IBM的CEO改成最近担任过IBM CEO的指令</li><li>明确替换的类别，加上table-merged，修改之前会关注到地毯</li><li>将标记煮咖啡的物品，替换为标记煮咖啡的厨房用品。执行步骤会从直接定位煮咖啡的目标，变为，先检测所有的厨房用品，再选择哪个是咖啡机</li><li>标记三强争霸赛的冠军改为4位三强争霸赛的冠军</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VisProg/14.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>作者期望VisProg能有效的扩展目前人工智能应用范围狭隘的问题，能解决目前AI无法下手需要多步决策的场景。</p><p>这篇文章更多的创新在于<strong>前瞻性</strong>(CVPR2023的截稿日期为2022.11.11，早于2023年4月份开始流行，目前144k star的AutoGPT! 甚至早于2022年11月问世的chatGPT!)和<strong>提出了解决通用问题的另一种思路</strong>，整个系统的工作原理很容易讲明白。</p><p>理解了论文中的Figure.1的图，就会发现是一个简单粗暴有效的方法。甚至我刚开始看的想着可能会有些其他的东西，但算法的设计上确实没太多可以讲的。<strong>VisProg将一个具体问题拆解成Python语言的形式很巧妙，因为编程实际上也是一种思维方式， 把一个复杂的问题，拆解成一步步可以用代码解决的问题。 而且简单有效并不是坏事， 这篇更多的是关于实际问题的思考方式， 和作者关于通用人工智能实现方式的尝试。</strong> </p><p><strong>简而言之，这篇文章及其类似方法提供的思路可能是现有条件下实现通用智能最好的方式！</strong></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="CVPR2023" scheme="https://oysz2016.github.io/tags/CVPR2023/"/>
    
    <category term="通用人工智能" scheme="https://oysz2016.github.io/tags/%E9%80%9A%E7%94%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="多步骤推理" scheme="https://oysz2016.github.io/tags/%E5%A4%9A%E6%AD%A5%E9%AA%A4%E6%8E%A8%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>一篇文章搞懂iBOT</title>
    <link href="https://oysz2016.github.io/post/e2effb33.html"/>
    <id>https://oysz2016.github.io/post/e2effb33.html</id>
    <published>2023-07-01T12:52:46.193Z</published>
    <updated>2023-07-02T02:17:51.834Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>本文首发于公众号“<a href="https://mp.weixin.qq.com/s/0JXKGXerN07NLbCRJJSpOg">CVTALK</a>”。</p><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2111.07832">https://arxiv.org/abs/2111.07832</a></p><p><strong>代码链接:</strong><a href="https://github.com/bytedance/ibot">https://github.com/bytedance/ibot</a></p><p>iBOT完整的名字是<strong>I</strong>MAGE <strong>B</strong>ERT PRE-TRAINING WITH <strong>O</strong>NLINE <strong>T</strong>OKENIZER， 和BEIT: BERT Pre-Training of Image Transformers的名字很相似，iBOT确实受启发于BEIT，另外还使用到了对比学习之前的SOTA——DINO。<strong>可以看作是BEIT和DINO的结合</strong>，既然和另外两篇文章关系这么大，正好笔者之前也没有写过BEIT和DINO的文章，在介绍iBOT的时候会尽量把BEIT和DINO也给尽量详细的总结出来。</p><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><h3 id="BEIT"><a href="#BEIT" class="headerlink" title="BEIT"></a>BEIT</h3><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2106.08254">https://arxiv.org/abs/2106.08254</a></p><p><strong>代码链接:</strong><a href="https://github.com/microsoft/unilm/tree/master/beit">https://github.com/microsoft/unilm/tree/master/beit</a></p><p>BEIT这篇论文的标题是<strong>BEIT: BERT Pre-Training of Image Transformers</strong>，从标题可以看出BEIT受启发于之前在文本领域大火的BERT模型，使用Transformer在图像场景中使用类似BERT中完形填空的方式做预训练。BEIT的名字来源于 <strong><em>B</em></strong>idirectional <strong><em>E</em></strong>ncoder representation from <strong><em>I</em></strong>mage <strong><em>T</em></strong>ransformers， 即图像transformer的双向编码器。</p><p><strong>既然BEIT的思路受启发于Bert，Bert可以说是非常耳熟能详了，为了更好的理解BEIT算法，在回顾的时候，笔者也尽量按照类比Bert中的做法说明BEIT在图像领域中应用的差异。</strong></p><p>在Bert或者说NLP中，一般前两步都是Tokenization和Numeralization:</p><ul><li><strong>Tokenization</strong>: 分词化，目的是将输入文本划分成一个个小的单元，保证每个单元拥有相对完整和独立的语义。根据划分粒度的差异可以分为字、字词(subword)、词三种粒度。</li><li><strong>Numeralization</strong>: 数字化，经过了分词后，为了方便后续的算法处理，需要将词转化为数字。做法是构造一个词表-索引对应的字典，将分词后的数据转化为数字。</li></ul><p>在BEIT中，也有类似的操作，叫做image patch和visual token:</p><ul><li><strong>image patch</strong>: 和VIT中一样，将H<em>W的图像划分成P </em> P个小的单元，每个单元的大小为$N=HW/P^2$。比较经典的划分方式是将224 <em> 224的图片，划分patch数量为16 </em> 16，则每个单元格大小为14 * 14</li><li><strong>visual token</strong>: NLP中的分词得到的单元是可以穷举的，而在图像中，每一个patch的信息不能直接像NLP中根据分词的结果构建索引。这里使用了<a href="https://arxiv.org/abs/2102.12092">discrete vaiational autoencoder</a>(dVAE)。<strong>BEIT中并没有训练dVAE模型，而是直接使用的DALLE中训练好的。</strong> <strong>dVAE的思想仿照了词汇表，构建了一个特征向量表</strong>，称为<strong>visual codebook</strong>，大小是8192，代表着存储了8192个特征向量。<strong>使用的方法是经过image patch后的图片经过encoder得到feature map，用这个feature map和codebook中的特征向量比对，选取codebook中最相似的特征向量作为后续使用的特征向量，而这个特征向量也会对应一串数字。</strong> 这样就可以完成图片特征的向量化</li></ul><blockquote><p>在图像中使用visual token有两个好处。<strong>第一个显而易见从feature map转换为对应的数字可以节省计算空间</strong>，<strong>第二个是区别于MAE的方法，MAE中从像素还原图片，会促使模型学习到非常细节的高频特征，而BEIT从visual token还原出图像，迫使模型学习低频的特征，模型可能具有更好的分类能力。</strong></p></blockquote><p>Bert中的预训练任务是将token mask掉，用其他没有mask掉的token预测被mask掉的部分，完成完形填空。回到BEIT中也是类似的思路，将image patch转换为visual token之后，也可以通过mask掉部分image patch，<strong>通过其他的image patch预测出被mask掉的部分，完成图像的’完形填空’。</strong></p><p>BEIT的结构如下图所示:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/1.png" alt=""></p><p>图中上半部分是dVAE的训练流程，<strong>再强调下，BEIT中没有训练dVAE</strong>，因此decoder复原图像的部分在BEIT的流程中实际是不存在的；其余的部分就是和image transformer的部分。dVAE和image transformer一样对整张图划分为4<em>4的网络，通过dVAE会得到每个patch的visual token，image transformer中的训练目标是预测被mask掉部分的visual token。图中<br><em>*Masked Image Modeling Head</em></em>是一个softmax分类器，目标是被mask掉部分经过transformer后的embedding和分类出的类别和visual token是一致的。</p><h3 id="DINO"><a href="#DINO" class="headerlink" title="DINO"></a>DINO</h3><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2104.14294">https://arxiv.org/abs/2104.14294</a></p><p><strong>代码链接:</strong><a href="https://github.com/facebookresearch/dino">https://github.com/facebookresearch/dino</a></p><p>DINO是一篇用对比学习做自监督的文章，模型的结构和BYOL相似。网络的架构如下:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/2.png" alt=""></p><p>DINO名字的来源是self-<strong><em>di</em></strong>stillation with <strong><em>no</em></strong> labels， 没有标签数据的自蒸馏，<strong>原因是采用自监督学习的方式，并且自己跟自己学习。</strong>既然是蒸馏的任务，那就需要student和teacher了，DINO将MOCO中的encoder和momentum encoder分别改成了student和teacher的命名方式。</p><p>DINO的伪代码如下图所示:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/3.png" alt=""></p><p><strong>DINO继承了其他对比学习的方法中伪代码清晰易懂的优良传统，既然DINO和BYOL相似，可以看作是BYOL的改进工作，下面在介绍DINO伪代码的同时，将其与BYOL的差异一并阐述:</strong></p><ul><li>初始化teacher和student网络，teacher网络的参数和student一致。student网络结构和BYOL基本一致，<strong>区别在于backbone部分使用了VIT，并且没有使用projection head和prediction head作为限制模型坍塌的结构</strong></li><li>一个batch的图片x， 经过两种不同的增广分别得到x1， x2；</li><li>x1和x2分别经过student网络和teacher网络，得到s1，s2，t1，t2;</li><li><strong>算loss的时候平均s1&amp;s2和t1&amp;t2的交叉熵loss，值得注意的是在交叉熵损失里，加入了centering和sharpening避免模型坍塌</strong>。DINO的作者认为这是比BN和添加projection head&amp;prediction head更好的方式。下面具体介绍下:<ul><li><strong>centering是将一个batch中的样本特征算一个均值， 计算loss时样本的特征需要减去该均值，centering实际上的作用和BN比较相似</strong></li><li><strong>sharpening是将centering之后的特征除以特定的值<code>tpt</code>做为最后计算交叉熵的特征</strong></li><li>具体代码为: <code>t = softmax((t - C) / tpt， dim=1)</code></li></ul></li><li>梯度反传并更新student网络</li><li>与BYOL和MOCO一样，采用动量更新的方式，更新teacher网络的参数</li><li><strong>采用动量的方式，更新<code>center</code>的参数。</strong></li></ul><h2 id="IBOT"><a href="#IBOT" class="headerlink" title="IBOT"></a>IBOT</h2><p>介绍完了DINO和BEIT之后，终于要迎来主角iBOT了。这里引用iBOT论文中的原话总结iBOT:</p><blockquote><p>self-distillation as a token-generation self-supervised objective </p></blockquote><p>即结合了自蒸馏和重建visual token的自监督算法。</p><p>iBOT的网络结构图如下图所示:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/4.png" alt=""></p><p><strong>在前面的方法一样，为了加深理解，在介绍iBOT方法的时候，将其与DINO和BEIT做对比:</strong></p><ul><li>和DINO一样，一个batch的图像x，经过两种不同的增广方式得到$\mu$和$\nu$</li><li>和DINO一样，$\mu$和$\nu$分别要胫骨student和teacher两个网络，不一样的是student中的图片要和BEIT中一样mask掉一部分</li><li><strong>student和teacher网络的输出会有两部分，一部分是和DINO中类似的$h_{s}^{[CLS]}$，另一部分是和BEIT中类似的$h_{t}^{patch}$</strong></li><li>训练的目标有两部分，<strong>一部分是和DINO类似</strong>，student的$\mu$和$\nu$经过image path，mask之后输出$\hat{\mu}_{s}^{[CLS]}$和$\hat{\nu}_{s}^{[CLS]}$， teacher类似输出$\mu_{s}^{[CLS]}$和$\nu_{s}^{[CLS]}$， 平均$\hat{\mu}_{s}^{[CLS]}$&amp;$\mu_{s}^{[CLS]}$和$\hat{\nu}_{s}^{[CLS]}$&amp;$\nu_{s}^{[CLS]}$的交叉熵损失。<strong>第二部分和BEIT类似</strong>，student的$\mu$和$\nu$经过image path，mask之后输出被mask的patch的特征$\hat{\mu}_{s}^{patch}$和$\hat{\nu}_{s}^{patch}$，teacher类似，和student对应的patch输出为$\mu_{s}^{patch}$和$\nu_{s}^{patch}$， 期望是student得到的visual token和teacher没有被mask部分的visual token一致。两部分loss的公式如下:<ul><li><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/13.png" alt=""></li><li><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/14.png" alt=""></li></ul></li><li>和DINO中一样， teacher网络的参数通过student的参数做动量更新</li><li><strong>和BEIT中不一样的是iBOT强调了MIM的学习，dVAE的部分也是参与训练的</strong>。论文题目中也特地提到了online tokenizer。</li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p><strong>iBOT也算是对比学习的一种方法，因此比较重要的指标就是能否提取到比较好的图片表征</strong>，下面是和其他方法在imageNet分类任务上的对比，iBOT是优于DINO和其他方法的。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/5.png" alt=""></p><h4 id="KNN-amp-linear-prob"><a href="#KNN-amp-linear-prob" class="headerlink" title="KNN&amp;linear prob"></a>KNN&amp;linear prob</h4><p>下面是用KNN和linear probing的imageNet上能取得的效果。<strong>KNN的实验是将网络backbone的特征冻结，模型的输出训练KNN进行分类</strong>；<strong>linear probing也是将backbone的特征冻结，加一个linear prob层，只训练linear prob完成分类。</strong><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/6.png" alt=""></p><h4 id="finetune"><a href="#finetune" class="headerlink" title="finetune"></a>finetune</h4><p>fintune的效果，可以看做是用所有的数据finetune整个网络<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/7.png" alt=""></p><h4 id="Semi-supervised-Learning"><a href="#Semi-supervised-Learning" class="headerlink" title="Semi-supervised Learning"></a>Semi-supervised Learning</h4><p>增加一个linear prob层用于分类，只使用1%或者10%的数据训练网络的backbone和linear prob分类器。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/8.png" alt=""></p><h3 id="下游任务"><a href="#下游任务" class="headerlink" title="下游任务"></a>下游任务</h3><p>看完了分类后，下面是在其他的下游任务上的效果</p><h4 id="目标检测和分割任务"><a href="#目标检测和分割任务" class="headerlink" title="目标检测和分割任务"></a>目标检测和分割任务</h4><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/9.png" alt=""></p><h4 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h4><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/10.png" alt=""></p><h3 id="MIM学习到模式的可视化"><a href="#MIM学习到模式的可视化" class="headerlink" title="MIM学习到模式的可视化"></a>MIM学习到模式的可视化</h3><p>和dVAE一样，展示了一些MIM所学习到的东西，<strong>作者发现iBOT对于局部语义有非常好的可视化效果</strong>，下图左边两张是车灯和狗的耳朵，聚类出的图片也基本属于这两个类别；右边两张展示的则是局部的纹理，聚类出的图片也是非常相似的。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/11.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>之前介绍的SAM中使用的image encoder是MAE，<strong>因为MAE能复原图像，表明MAE学习到了比较好的图像表征。</strong>对比学习的训练方式也能让模型学习到很好的图像表征，在DINO和在iBOT基础上优化的DINO2对特征做的可视化图片都能表明，对比学习的任务学出来的特征甚至比一些专门做分割训练的模型，具有更好的特征。<strong>iBOT将MIM和Contrastive Learning两种思想结合起来还是非常有意思的，而且论文中提到，student和teacher网络中MIM和Contrastive Learning的MLP层共享参数，能获得更好的效果，说明两个任务结合在一起是可以互相促进的。也许这也是图片领域foundation model的一种实现路径!</strong></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/iBOT/12.png" alt="DINO中attention可视化图"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="大模型" scheme="https://oysz2016.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="对比学习" scheme="https://oysz2016.github.io/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="MIM" scheme="https://oysz2016.github.io/tags/MIM/"/>
    
  </entry>
  
  <entry>
    <title>一篇文章搞懂Segment Anything(SAM)</title>
    <link href="https://oysz2016.github.io/post/70b40a92.html"/>
    <id>https://oysz2016.github.io/post/70b40a92.html</id>
    <published>2023-06-18T08:29:45.648Z</published>
    <updated>2023-07-02T02:21:36.137Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>本文首发于公众号“<a href="https://mp.weixin.qq.com/s?__biz=MzU4NjU4MDY1OA==&amp;mid=2247484634&amp;idx=1&amp;sn=b102b609e965dd916e00042f12c9a2fb&amp;chksm=fdf85307ca8fda1126191dc4ed7452a03c418bc6f1c43a45cf984b722aa07321281503264fe0&amp;scene=178&amp;cur_album_id=2977728123201126402#rd">CVTALK</a>”。</p><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2304.02643">https://arxiv.org/abs/2304.02643</a></p><p><strong>代码链接:</strong><a href="https://github.com/lllyasviel/ControlNet">https://github.com/lllyasviel/ControlNet</a></p><p><strong>Demo链接:</strong><a href="https://segment-anything.com/demo">https://segment-anything.com/demo</a></p><p>SAM从任务、模型、数据三部分展开写作，和模型的创新比较起来，任务定义和数据的工作更加出彩，官网也给出了demo，能直观感受SAM的效果，这篇blog也会围绕这几部分展开。</p><h2 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h2><p>demo中有开放point, box, everything三种方式。由于text prompt效果不太稳定，demo和代码中都没有该部分。</p><ul><li><strong>鼠标悬停:</strong> 显示的是悬停位置的分割结果，例如下图中将鼠标放到手的位置.<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/1.png" alt=""></li></ul><ul><li><strong>点击:</strong> 分割包含该点的物体，会按最小分割的结果展示出来，如果想分割的物体大于展示的结果，可以在物体的其他部分也点击下。</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/2.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/3.png" alt=""></p><ul><li><strong>box:</strong> 框定一个box，分割box中的物体<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/4.png" alt=""></li></ul><ul><li><strong>everything:</strong> 将图片中所有物体的分割都展示出来<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/5.png" alt=""></li></ul><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/6.png" alt=""></p><p>任务的设计灵感来自于NLP领域，例如NLP中可以通过预测next token作为预训练任务，而在下游任务中可以使用prompt engineering做应用。因此，为了建立分割的基础模型，任务的设计目标是也需要具有类似的能力。<br>这里作者扩展了下NLP里prompt在图像分割里的用法， prompt可以是以下几种类型：</p><ul><li>point</li><li>box</li><li>mask</li><li>任意格式的文本</li></ul><p>为了支持以下的几种输入prompt格式，要求模型能够区分具有混淆意义的prompt，例如下图中，一个point的prompt可能有多种分割方式.这多种分割方式对于模型来说都是有效的。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/7.png" alt=""></p><p>预训练:  将上面提到的多种sequence的prompt告诉模型，训练目标是让模型输出对应promt的分割结果，并且期望模型输出的结果和GT尽可能一致。区别于之前的交互式分割算法，SAM基本能治通过一次交互就能得到很合理的分割结果。要达到这个目的，需要设计非常独特的模型结构和loss。</p><p>zero-shot transfer：需要模型对任何prompt，得到合适的分割结果。例如，如果要做实例分割，可以把检测得到的box作为prompt，SAM就能去做实例分割</p><p>related tasks: 分割里有很多子任务，例如边缘分割，语义分割等，SAM能完成所有已知的分割任务和还没有作为一个方向的分割任务。之前已经有类似的可以做多种分割的模型(solo), 但是这些模型有多个子子输出，然后做排列组合可以得到多种分割结果。而SAM通过prompt将多个分割任务合并在一起。</p><p>总而言之，作者是希望SAM能够分割一切，并且能相CLIP一样，能应用到最开始没有想到的领域。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/8.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/9.png" alt=""></p><p>模型的结构如上图所示. prompt会经过<code>prompt encoder</code>, 图像会经过<code>image encoder</code>。然后将两部分embedding经过一个轻量化的<code>mask decoder</code>得到融合后的特征。encoder部分使用的都是已有模型，decoder使用transformer。这部分论文中介绍的相对比较少，下面会结合代码一起梳理下:</p><ul><li><p><strong>image encoder：</strong> 使用的是用ViT走位backbone的MAE模型。在交互式分割的展示中，image encoder只会运行一次。在实验中，分别有用到ViT-H, ViT-L, ViT-B三种大小的模型作为image encoder。代码如下,<a href="https://github.com/facebookresearch/segment-anything/blob/6fdee8f2727f4506cfbbe553e23b895e27956588/segment_anything/build_sam.py#L47">build_sam#L47</a> </p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sam_model_registry = &#123;</span><br><span class="line">    <span class="string">&quot;default&quot;</span>: <span class="keyword">build_sam_vit_h,</span></span><br><span class="line"><span class="keyword"></span>    <span class="string">&quot;vit_h&quot;</span>: <span class="keyword">build_sam_vit_h,</span></span><br><span class="line"><span class="keyword"></span>    <span class="string">&quot;vit_l&quot;</span>: <span class="keyword">build_sam_vit_l,</span></span><br><span class="line"><span class="keyword"></span>    <span class="string">&quot;vit_b&quot;</span>: <span class="keyword">build_sam_vit_b,</span></span><br><span class="line"><span class="keyword"></span>&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>prompt encoder：</strong> prompt总共有point,box, mask, text四种，会将其分为三类。<strong>pint和box可以作为一类使用position encodings</strong>, <strong>text可以使用CLIP作为encoder</strong>, <strong>而mask是一种密集型的prompt，可以使用卷积作为encoder</strong>.<a href="https://github.com/facebookresearch/segment-anything/blob/6fdee8f2727f4506cfbbe553e23b895e27956588/segment_anything/modeling/prompt_encoder.py#LL128C5-L128C5">prompt_encoder.py#LL128C5-L128C5</a> prompt_encoder的代码如下所示，其中用position embedding分别实现了point和box query两种稀疏embedding，用卷积实现了mask query密集embedding.，</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">def forward(</span><br><span class="line">        self,</span><br><span class="line">        points: Optional[Tuple[torch.Tensor, torch.Tensor]],</span><br><span class="line">        boxes: Optional[torch.Tensor],</span><br><span class="line">        masks: Optional[torch.Tensor],</span><br><span class="line">    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:</span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="comment">&quot;</span></span><br><span class="line">        Embeds different types of prompts, returning both sparse <span class="built_in">and</span> dense</span><br><span class="line">        embeddings.</span><br><span class="line"></span><br><span class="line">        Arguments:</span><br><span class="line">          points (tuple(torch.Tensor, torch.Tensor) <span class="built_in">or</span> none): point coordinates</span><br><span class="line">            <span class="built_in">and</span> labels <span class="keyword">to</span> embed.</span><br><span class="line">          boxes (torch.Tensor <span class="built_in">or</span> none): boxes <span class="keyword">to</span> embed</span><br><span class="line">          masks (torch.Tensor <span class="built_in">or</span> none): masks <span class="keyword">to</span> embed</span><br><span class="line"></span><br><span class="line">        Returns:</span><br><span class="line">          torch.Tensor: sparse embeddings <span class="keyword">for</span> the points <span class="built_in">and</span> boxes, with shape</span><br><span class="line">            BxNx(embed_dim), where <span class="keyword">N</span> <span class="keyword">is</span> determined by the <span class="keyword">number</span> of <span class="built_in">input</span> points</span><br><span class="line">            <span class="built_in">and</span> boxes.</span><br><span class="line">          torch.Tensor: dense embeddings <span class="keyword">for</span> the masks, in the shape</span><br><span class="line">            Bx(embed_dim)<span class="keyword">x</span>(embed_H)<span class="keyword">x</span>(embed_W)</span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="comment">&quot;</span></span><br><span class="line">        bs = self._get_batch_size(points, boxes, masks)</span><br><span class="line">        sparse_embeddings = torch.<span class="built_in">empty</span>((bs, <span class="number">0</span>, self.embed_dim), device=self._get_device())</span><br><span class="line">        <span class="keyword">if</span> points <span class="keyword">is</span> not None:</span><br><span class="line">            coords, labels = points</span><br><span class="line">            point_embeddings = self._embed_points(coords, labels, pad=(boxes <span class="keyword">is</span> None))     # position embedding</span><br><span class="line">            sparse_embeddings = torch.<span class="keyword">cat</span>([sparse_embeddings, point_embeddings], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> boxes <span class="keyword">is</span> not None:</span><br><span class="line">            box_embeddings = self._embed_boxes(boxes)   # position embedding</span><br><span class="line">            sparse_embeddings = torch.<span class="keyword">cat</span>([sparse_embeddings, box_embeddings], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> masks <span class="keyword">is</span> not None:</span><br><span class="line">            dense_embeddings = self._embed_masks(masks)    # conv embedding</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dense_embeddings = self.no_mask_embed.weight.reshape(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).<span class="built_in">expand</span>(</span><br><span class="line">                bs, -<span class="number">1</span>, self.image_embedding_size[<span class="number">0</span>], self.image_embedding_size[<span class="number">1</span>]</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sparse_embeddings, dense_embeddings</span><br></pre></td></tr></table></figure></li><li><p><strong>mask decoder：</strong>: 使用一个transformer将image embedding和prompt embedding做双向的cross-attention；并且也有prompt embedding的self-attention。也有MLP和linear classifier分类分割区域。mask decoder, <a href="https://github.com/facebookresearch/segment-anything/blob/6fdee8f2727f4506cfbbe553e23b895e27956588/segment_anything/modeling/transformer.py#L151">transformer.py#L151</a>这里的queries是query embedding，keys是image embedding，query_pe和queries一样，key_pe是需要加到image embedding上的位置编码。query embedding会经过self attention。query embedding和image embedding会做双向的cross-attention, 具体实现方式是如上代码所示，image embedding会作为query，query embedding会作为key和value；同样的，query embedding会作为query，image embedding会作为key和value。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">def forward(</span><br><span class="line">        self,</span><br><span class="line">        image_embedding: Tensor,</span><br><span class="line">        image_pe: Tensor,</span><br><span class="line">        point_embedding: Tensor,</span><br><span class="line">    ) -&gt; Tuple[Tensor, Tensor]:</span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="comment">&quot;</span></span><br><span class="line">        Args:</span><br><span class="line">          image_embedding (torch.Tensor): image <span class="keyword">to</span> attend <span class="keyword">to</span>. Should <span class="keyword">be</span> shape</span><br><span class="line">            B <span class="keyword">x</span> embedding_dim <span class="keyword">x</span> h <span class="keyword">x</span> <span class="keyword">w</span> <span class="keyword">for</span> any h <span class="built_in">and</span> <span class="keyword">w</span>.</span><br><span class="line">          image_pe (torch.Tensor): the positional encoding <span class="keyword">to</span> <span class="built_in">add</span> <span class="keyword">to</span> the image. Must</span><br><span class="line">            have the same shape <span class="keyword">as</span> image_embedding.</span><br><span class="line">          point_embedding (torch.Tensor): the embedding <span class="keyword">to</span> <span class="built_in">add</span> <span class="keyword">to</span> the query points.</span><br><span class="line">            Must have shape B <span class="keyword">x</span> N_points <span class="keyword">x</span> embedding_dim <span class="keyword">for</span> any N_points.</span><br><span class="line"></span><br><span class="line">        Returns:</span><br><span class="line">          torch.Tensor: the processed point_embedding</span><br><span class="line">          torch.Tensor: the processed image_embedding</span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="comment">&quot;</span></span><br><span class="line">        # BxCxHxW -&gt; BxHWxC == B <span class="keyword">x</span> N_image_tokens <span class="keyword">x</span> C</span><br><span class="line">        bs, <span class="keyword">c</span>, h, <span class="keyword">w</span> = image_embedding.shape</span><br><span class="line">        image_embedding = image_embedding.flatten(<span class="number">2</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        image_pe = image_pe.flatten(<span class="number">2</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        # Prepare queries</span><br><span class="line">        queries = point_embedding</span><br><span class="line">        <span class="built_in">keys</span> = image_embedding</span><br><span class="line"></span><br><span class="line">        # Apply transformer blocks <span class="built_in">and</span> final layernorm</span><br><span class="line">        <span class="keyword">for</span> layer in self.layers:</span><br><span class="line">            queries, <span class="built_in">keys</span> = layer(</span><br><span class="line">                queries=queries,</span><br><span class="line">                <span class="built_in">keys</span>=<span class="built_in">keys</span>,</span><br><span class="line">                query_pe=point_embedding,</span><br><span class="line">                key_pe=image_pe,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        # Apply the final attention layer from the points <span class="keyword">to</span> the image</span><br><span class="line">        q = queries + point_embedding</span><br><span class="line">        <span class="keyword">k</span> = <span class="built_in">keys</span> + image_pe</span><br><span class="line">        attn_out = self.final_attn_token_to_image(q=q, <span class="keyword">k</span>=<span class="keyword">k</span>, v=<span class="built_in">keys</span>)</span><br><span class="line">        queries = queries + attn_out</span><br><span class="line">        queries = self.norm_final_attn(queries)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> queries, <span class="built_in">keys</span></span><br></pre></td></tr></table></figure></li><li><p><strong>解决混淆的输入</strong>: 对于一个prompt，模型会输出3个mask，实际上也可以输出更多的分割结果，3个可以看作一个物体的整体、部分、子部分，基本能满足大多数情况。使用IOU的方式，排序mask。在反向传播时，参与计算的只有loss最小的mask相关的参数.</p></li><li><p><strong>高效</strong>: 这里主要指的是<strong>prompt encoder</strong>和<strong>mask decoder</strong>。在web浏览器上，CPU计算只用约50ms</p></li><li><p><strong>loss和训练细节</strong>: 主要使用的是focal loss和dice loss。每一个mask，会随机产生11种prompt与之配对。</p></li></ul><h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><h3 id="数据引擎"><a href="#数据引擎" class="headerlink" title="数据引擎"></a>数据引擎</h3><p>不像CLIP中图像文本对通过互联网容易获取，分割的数据获取成本巨大。SAM开源了一个10亿张图片的分割数据集。在SAM中设计了一个数据引擎用于获取分割的数据，数据引擎主要分为以下三部分:</p><ul><li><p>辅助标注: 简单来说就是用可以获取到的开源分割数据训练一个初始的SAM模型V0版本，再用V0在没有分割标注的数据上生成预标注，人工check模型的结果并作修改和确认。得到新的数据后，再将新的数据加入到训练集重新训练SAM得到V1版本,再循环标注数据和迭代模型。总共进行6次训练。开始的时候数据集比较少，使用的ViT-B模型，最终会使用ViT-H模型。 这里面还有一些效率提升的数据，例如随着模型的迭代，每个mask的标注耗时从34s到14s。SAM模型在每张图片上生成的mask从20到44个。在该阶段数据集最终有12万张图片，430万个mask</p></li><li><p>半自动化标注: 通过第一阶段后，已经有一个不错的SAM模型能生成分割结果。<strong>半自动化标注</strong>的目的是增加mask的多样性。具体做法是训练一个检测模型，用于检测SAM生成的mask结果是否可信，只保留可信的mask结果，然后将图片给人工标注。人工标注会在可信的mask基础上标注出其他的分割框。经过5次的迭代后，数据集新增了18万张图片，590万mask。</p></li></ul><p><strong>自动标注</strong>: 经过前面两个阶段后，SAM有了较好的结果，能分割出图片中的目标，并且对于混淆的prompt也有了较好的输出。这个模型可以自动的对一些图片做标注。自动标注的时候需要有一些筛选策略，模型输出的结果可能还是会出现一些错误。主要有以下三种方式做筛选</p><ul><li>SAM模型有一个IOU prediction的模块能输出mask的confidence，如下图所示<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/10.png" alt=""></li></ul><ul><li><p>stable mask的判断，具体的方法是在得到分割结果前对logit加正向和负向的扰动，如果两次扰动生成的分割结果IOU大于0.95，则认为生成的mask是可靠的</p></li><li><p>NMS过滤掉重复的mask</p></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/11.png" alt=""></p><h3 id="数据质量"><a href="#数据质量" class="headerlink" title="数据质量"></a>数据质量</h3><p> 图像: 包含11M高分辨率(3300<em>4950)的图像，其他的一些开源数据集，例如COCO分辨率较低(480</em>640)<br>Mask: 包含1.1B的mask，99.1%都是模型生成的。作者实验了下，只使用模型生成的mask和即使用模型生成也使用人工标注的mask，模型的效果是相当的。因此发布的数据集里只包含模型生成的mask<br>Mask 质量: 抽取了一部分mask数据做人工的精标，精标前后有94%的mask具有90%以上的IOU。而其他的一些开源数据集只有85-91%的IOU</p><p>下面也从mask的数量，每种mask尺寸的占比和mask占外接矩形比例等多方面和其他数据集做了对比<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/12.png" alt=""></p><p>数据来源分布<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/13.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/14.png" alt=""></p><p>不同性别，肤色，年龄人群分割效果的差异对比<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/15.png" alt=""></p><h2 id="zero-short-Transfer实验"><a href="#zero-short-Transfer实验" class="headerlink" title="zero-short Transfer实验"></a>zero-short Transfer实验</h2><p>评估的数据集都是SAM模型训练时的不同，并且包含水下，第一视角等没有在SAM中出现过场景的图片</p><h3 id="point-mask"><a href="#point-mask" class="headerlink" title="point mask"></a>point mask</h3><p>这里对比的是用point作为prompt对比分割的结果, 在绝大部分数据集中都优于RITM(当前的SOTA) </p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/16.png" alt=""></p><h3 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h3><p>SAM在训练的时候就是采用的包含point prompt的方式，作者这里还对比了一些在训练时没有包含的方式，边界检测就是其中一种。SAM在使用边界检测时，使用方式是在图片上铺上16*16均匀的point prompt，每个prompt产生3个mask，再经过NMS后。通过Sobel filtering得到边缘检测的结果。SAM的结果倾向于提取更丰富的边缘，因此在指标上recall和专门做边缘检测的模型相当，precision会低些。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/17.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/18.png" alt=""></p><h3 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h3><p>分割的结果取bbox，就能做目标检测了.整体指标低于ViTDet，但是在中等常见和不太常见的目标上效果优于ViTDet</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/19.png" alt=""></p><h3 id="实例分割"><a href="#实例分割" class="headerlink" title="实例分割"></a>实例分割</h3><p>先用一个目标检测算法，用目标检测得到的box作为prompt输入到SAM，就可以做实例分割了。实验的结果分为了定量(用测试集的GT)和定性(人来评判好坏)两种。定量的指标不如BiTDet—H,定性的指标SAM优于ViTDet。作者给出的解释是COCO数据集标注效果一般(在人看来甚至不如SAM和ViTDet模型输出的结果)，因此ViTDet在COCO上做训练时拟合到了一些错误的偏差，但错误的偏差和标注相似，因此定量的指标不如ViTDet</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/20.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/21.png" alt=""></p><h3 id="Text-to-Mask"><a href="#Text-to-Mask" class="headerlink" title="Text to Mask"></a>Text to Mask</h3><p>这里指的是用文本作为prompt，然后分割出文本提到的目标。作者在训练的时候取的是图片中目标尺寸大于100*100的目标，用CLIP提取image embedding(text embedding也行，因为CLIP的image embedding和text embedding是对齐的)，作为prompt encoder模块的输出，用于训练SAM模型。这一部分没有和其他方法对比，也由于效果不太稳定，在官方的demo中没有展示</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/22.png" alt=""></p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/SAM/23.png" alt=""></p><p>有以下的结论:<br>左边的图，数据来源的影响:</p><ul><li>加入半自动标注的数据和自动标注的数据性能都有很大的提升</li><li>只用模型生成的数据与额外加上人工标注的数据差异不大</li></ul><p>中间的图, 数据量的影响:</p><ul><li>数据量从0.1M到1M，模型性能提升很大</li><li>数据量从1M到11M，模型性能变化不明显，实际使用中1M差不多足够</li></ul><p>右边的图, image encoder的影响:</p><ul><li>ViT-B到ViT-L提升很大</li><li>ViT-L到ViT-H提升一般，实际使用ViT-L足够</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>SAM的热度也非常高，同样作为FB的工作，SAM仅仅放出来两个月，github上star的数量已经超过了detectron2三年的总和。SAM的期望是能将该模型作为图像领域的基础模型(foundation model)，像CLIP那样能在各个领域大放异常，或者像GPT一样能统一NLP领域。SAM也确实在很多场景得到了应用，例如开源的SD中也融合了SAM，可以做很多有趣的应用，例如从假人模特身上用SAM得到衣服的mask，再结合ControlNet，就可以生成不同的人穿着同样的衣服。</p><p>最开始自媒体宣传的文章也是《CV领域不存在了》,《CV界的GPT3》类似的标题，SAM确实是在统一上迈出了很大的一步，但实际上CV领域的统一还有很多挑战。NLP领域中的Bert用完型填空和GPT预测下一个token的预训练在非常多的任务上表现了很好的泛化性，甚至在一些没有训练过的任务上能取得比一些专家模型更好的效果。</p><ul><li><p>任务和数据上的不统一，CV领域的分类是输出类别，检测输出bbox，分割输出mask。虽然个别任务可以复用，但是整体缺乏一个通用的任务。任务上的不统一，数据上也很难做到统一，分类的任务有很多数据，但是检测和分割的数据就要少非常多，并且标注成本巨大。单纯训练分类作为backbone也很难解决其他任务，检测和分割的算法依然需要做大量的优化</p></li><li><p>CV领域的任务缺乏孕育大模型的土壤，CV任务一直在考虑模型的计算量，显存占用。如果将每个像素看作一个token，一张512*512的图片就有26万个token。如果transformer最开始出现在CV领域，面临的问题是显存和计算量都比resnet差，并且效果也远不如resnet。如果没有transformer极大的促进了NLP领域的发展，CV领域可能也不会重新思考transfomer能增大感受野，能有更好的泛化能力。</p></li><li><p>还没有找到CV领域【高维】的任务。NLP领域的完形填空和对话确实是一种很高维的任务。模型能完成这些任务，一些NER或者RE之类的底层任务也能很好的被解决。目前CV领域有一些尝试做foundation model，例如对比学习或者像SAM，在一些任务上表现了不错的泛化性，可能是这些方法能统一其他任务，但目前的发展还不太够，也可能是其他一些还没出现/发展起来的任务。但这种【高维】的任务一定能通过一些方式降维解决目前几乎所有的CV基础任务。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="分割" scheme="https://oysz2016.github.io/tags/%E5%88%86%E5%89%B2/"/>
    
    <category term="大模型" scheme="https://oysz2016.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>stable diffusion(五)——ControlNet原理与实践</title>
    <link href="https://oysz2016.github.io/post/b70cbd0a.html"/>
    <id>https://oysz2016.github.io/post/b70cbd0a.html</id>
    <published>2023-06-02T10:27:59.547Z</published>
    <updated>2023-06-04T00:27:42.636Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>这个系列会分享下stable diffusion中比较常用的几种训练方式，分别是Dreambooth、textual inversion、LORA、Hypernetworks和ControlNet。在<a href="https://civitai.com/">https://civitai.com/</a>选择模型时也能看到它们的身影。本文该系列的第五篇</p><ul><li><a href="https://mp.weixin.qq.com/s/NE3Gkr64G3XADVdujtzRXw">Dreambooth</a></li><li><a href="https://mp.weixin.qq.com/s/hJaXd7Eb44CXjl4QwnSZWg">Textual Inversion</a></li><li><a href="https://mp.weixin.qq.com/s/mtdieY-sqIKCiUK16ItQ3g">LoRA</a></li><li><a href="https://mp.weixin.qq.com/s/FBQbQ72SPTa17mScITYF5Q">Hypernetworks</a></li><li><u>ControlNet</u></li></ul><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><strong>论文链接:</strong><a href="https://arxiv.53yu.com/abs/2302.05543">https://arxiv.53yu.com/abs/2302.05543</a></p><p><strong>代码链接:</strong><a href="https://github.com/lllyasviel/ControlNet">https://github.com/lllyasviel/ControlNet</a></p><p>ControlNet可以从提供的参考图中获取布局或者姿势等信息, 并引导diffusion model生成和参考图类似的图片。这种功能对于生成模型来说至关重要, ControlNet开源到stable diffusion中时, 很多自媒体文章的标题都是《XX行业要结束了》, 有点杀死比赛的意思。训练和测试过stable diffusion的朋友应该知道, prompt至关重要, 而调整prompt确实是一件费时费力的事情, 想要控制好一些细节非常不容易。往往都是生成大量图片, 能得到几个满意的。而有了ControlNet则可以精确的控制生成图片的细节(当然, 也依赖于你已经有一张参考图了)</p><p>扩散模型中会需要使用text作为条件引导模型生成, 而ControlNet在引导的时候会增加一个条件实现更加可控的text-to-image生成.</p><p>diffusion model进展非常迅速, 针对diffusion model的一些痛点, 作者提出了几个问题和训练扩散模型时的限制, 并着手用ControlNet解决:</p><ul><li>基于prompt的控制是否能满足我们的需求?</li><li>在图像处理中, 有许多有明确定义的长期任务, 是否可以用大模型来使这些任务达到新的高度?</li><li>应该用什么样的框架处理用户想要的各种各样条件？</li><li>在特定场景, 新提出的框架怎么能继承大模型(diffusion model)的能力.</li></ul><p>三点限制:</p><ul><li>特定场景的数据量少, 在较少的数据量上很容易出现过拟合</li><li>大的计算集群很奢侈, (stable diffusion base模型需要15万个A100显卡小时)</li><li>end2end是必要的, 一些特定场景需要将原始的输入转为更高语义的表达, 手工处理的方式不太可行</li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>对于上面的4个问题和3点限制, 作者提出了ControlNet, ControlNet具有以下的特点:</p><ul><li>end2end</li><li>control large im  age diffusion model(例如stable diffusion)</li><li>learn task-specific input conditions</li></ul><p>为了能继承大模型的能力, ControlNet将原来大模型的参数分为两部分, 分别是<strong>可训练</strong>和<strong>冻结</strong>的。对比下添加ControlNet结构前后的网络图很容易明白。</p><p>之前stable diffusion的模型结构如下图所示:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/1.png" alt=""></p><p>图(a)是之前stable diffusion的输出, 图(b)和图(a)的区别在于添加了ControlNet的结构, 具体而言是将<code>neural network block</code>复制了一份, 作为<code>trainable copy</code>,并且<code>neural network block</code>的网络参数会被冻结住。而且<code>trainable copy</code>前后会有<code>zero convolution</code>, <code>zero convolution</code>其实是1*1的卷积。最后会将<code>neural network block</code>和<code>trainable copy</code>的特征相加。</p><p><code>neural network block</code>的作用可以用下面的公式表示<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/2.png" alt=""><br>$x$是一个2D的feature, 其shape为{h,w,c}分别代表height, width, channel.$\theta$代表<code>neural network block</code>的参数, y表示经过<code>neural network block</code>后的特征。</p><p>添加了control后, 网络会从<code>公式1</code>变为下面的<code>公式2</code><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/3.png" alt=""></p><p>$Z$代表<code>zero convolution</code>, $\theta_{z1}$,$\theta_{z2}$,$\theta_{c}$分别代表第一个, 第二个<code>zero convolution</code>的参数和<code>trainable copy</code>的参数。</p><p>在初始化训练的时候, <code>zero convolution</code>的weight和bias会被初始化为0, 因此ControlNet具有以下的特点:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/4.png" alt=""></p><p>实际上是为了让初始化的时候添加了ControlNet的模型和原始的模型效果是一样的, 后续再慢慢finetune</p><p>可以简化成下面的表达式<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/5.png" alt=""></p><p>$y_c$是加上ControlNet的输出, 而$y$是原始的输出, 模型初始化时两者是相等的</p><p>文章中特地解释了<code>zero convolution</code>梯度更新的问题, 在模型开始迭代前, 有W=0, B=0.则</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/6.png" alt=""></p><p>可以看到权重W的梯度是I, 而偏置B的梯度是1.说明<code>zero convolution</code>是可以正常更新的</p><p>ControlNet的详细结构如下图所示:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/7.png" alt=""></p><p>灰色部分是原来stable diffusion的结构, 蓝色部分是从U-Net的encode对应部分copy, 经过<code>zero convolution</code>后和U-Net的decode相加。  </p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/8.png" alt=""></p><p>了解了结构后, 下面是训练的细节。$\epsilon-\epsilon_{\theta}$和原始的stable diffusion是一样的, 是每次生成噪声和实际噪声的差值。区别在于加入了ControlNet新加的控制条件$c_f$。其他的参数都和stable diffusion一样, $z_t$代表噪声图片, $t$代表迭代次数,$c_t$代表text prompt。</p><p>在实际训练的时候, 会随机让50%的text prompt变为空, 促使模型更多的关注$c_f$,让模型学习用$c_f$控制图像生成。</p><p> 虽然ControlNet新加了些模型结构, 但由于大部分参数都被冻结, 因此训练时实际上只用原来stable diffusion模型的23%显存和34%的训练时间, 可以在单张40GB的A100卡上做训练</p><p><strong>计算资源的限制</strong>: 如果数据量比较少, 可以只连接U-Net中的SD Middle Block, 而连接decoder的<code>zero convolution</code>都不用, 可以减少参数量。可以将ControlNet的训练速度提高1.6倍(在RTX 3070TI的消费级显卡上也可以做训练)。</p><p><strong>有大量的训练资源</strong>: 如果有大量的训练资源(8张A100), 为了获取更好的效果可以将原始U-Net模型中的被冻结参数的decoder部分也参与模型训练。</p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p>这部分没有定量的分析, 更多的是通过一些case表明ControlNet能有效的控制生成的图像, 因为毕竟新加了个条件, 没有现成的数据集用来评测。</p><h3 id="有无ControlNet结构对比"><a href="#有无ControlNet结构对比" class="headerlink" title="有无ControlNet结构对比"></a>有无ControlNet结构对比</h3><p>下面的图最左边是ControlNet使用的canny图；中间是没有controNet, 只通过text prompt控制生成的房子及周边环境；右边是使用了ControlNet的图, 可以看到不实用ControlNet, 房子的结构可能会发生一些改变, 而加上了ControlNet后, 房子的结构始终固定, 改变只有房子的外观和周边的环境。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/9.png" alt=""></p><h3 id="突然收敛现象"><a href="#突然收敛现象" class="headerlink" title="突然收敛现象"></a>突然收敛现象</h3><p>作者发现ControlNet在训练的时候会出现突然收敛, 这种收敛一般发生在5000-10000步, 而且收敛之后如果继续迭代模型的变化并不大。例如下面的图, 提供了苹果的canny图和text prompt: apple, 但模型最开始学习到的是苹果电脑相关的, 到6100步图片中仍然没有学习到生成提供的canny图类似的图片, 但仅仅过了33步后到6133步模型突然就学会了！而且该现象和batch size关系不大, 也就是说如果增大batch size 模型差不多也会在6133步收敛, 好处是大batch size收敛出的模型效果更好。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/10.png" alt=""></p><h3 id="canny-edge-based模型和训练数据大小的关系"><a href="#canny-edge-based模型和训练数据大小的关系" class="headerlink" title="canny-edge-based模型和训练数据大小的关系"></a>canny-edge-based模型和训练数据大小的关系</h3><p>在50k数据之前增加数据效果会有明显的提升, 但50k之后狮子生成的已经很好了, 差异在于增大数据后背景会更加好点。有点像突然收敛之后继续迭代的情况。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/11.png" alt=""></p><h3 id="和其他用图像作为条件引导生成方式的对比"><a href="#和其他用图像作为条件引导生成方式的对比" class="headerlink" title="和其他用图像作为条件引导生成方式的对比"></a>和其他用图像作为条件引导生成方式的对比</h3><h4 id="depth"><a href="#depth" class="headerlink" title="depth"></a>depth</h4><p>在stable diffusion V2.0中有个很重要的更新——Depth-to-Image Diffusion, 可以推断输入图像的深度信息, 然后利用文本和深度信息生成新图像。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/12.png" alt=""></p><p>这种方式和depth-base ControlNet很像, 对比如下图所示：<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/13.png" alt=""></p><p>从生成效果上两者看起来都非常好(硬要比的话, ControlNet效果稍好些)。从训练成本上, stable diffusion V2.0需要大量A100的训练集群在1200万的训练数据训练2000GPU-hours, 而Depth-based ControlNet在stable diffusion V1.5的基础上预训练, 使用单张RTX 3090TI在20万数据上训练1周。这么对比确实ControlNet太香了</p><h4 id="segmentation"><a href="#segmentation" class="headerlink" title="segmentation"></a>segmentation</h4><p>下面的图对比的是PITI(Pretraining-Image-to-Image)和segmentation-base ControlNet</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/14.png" alt=""></p><h4 id="Scribbles"><a href="#Scribbles" class="headerlink" title="Scribbles"></a>Scribbles</h4><p>下面的图对比的是Sketch-guided diffusion和scribbles-base ControlNet</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/15.png" alt=""></p><h4 id="canney"><a href="#canney" class="headerlink" title="canney"></a>canney</h4><p>下面的图对比的是Taming Transformer和canney-base ControlNet<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/16.png" alt=""></p><h3 id="控制条件对比"><a href="#控制条件对比" class="headerlink" title="控制条件对比"></a>控制条件对比</h3><p>不同控制条件下生成效果的对比, 作者给了一个房间布局的图, 然后分别用cannry, depth, HED, Nromal, Line和Scribbles的方式做对比。其实这几个模型都有更适合的应用场景, 关以房间布局的例子而言, Line和HED是更合适的, 这个从Line和HED的图像特征就能看出。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/17.png" alt=""></p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>更多的应用,下面的图展示的是把原图的一部分mask掉, 然后随便画几笔草图, 生成的图就是没mask掉的部分结合草图的完成效果。(太牛了)</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/18.png" alt=""></p><p>当图像细节比较简单时, ControlNet可以实现精确的控制<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/19.png" alt=""></p><p>如果需要兼顾和提示图片类似与多样性两种需求, 可以在生成图片中灵活的设置ControlNet参与图片生成的步数。例如下面的图, 和原始图相似, 但也保证了多样性</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/20.png" alt=""></p><h2 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h2><p>ControlNet可能会曲解canney和depth等图像特征的含义, 例如下面的图是一个水杯的canney图, ControlNet生成的是一个农田, 加上<code>a glass of water</code>后, 生成的图片稍微好点, 但仍然和实际的有差异。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%BA%94%29%E2%80%94%E2%80%94ControlNet%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/21.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>ControlNet的原理到这里就介绍完了, 做的改进很简洁, 只需要较小的数据集就可以用作训练(50k, 相比diffusion model亿级别的确实小不少)。用的计算资源较少, 并且做一些参数调整后甚至可以在消费级的显卡上做训练。总共有10个不同图像特征的模型, 可以适应多种生成需求。ControlNet在stable diffusion中已经开源了, 并且在webui上很容易添加和尝试,这部分有非常多的blog和视频有讲解, 就不再赘述了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="diffusion model" scheme="https://oysz2016.github.io/tags/diffusion-model/"/>
    
  </entry>
  
  <entry>
    <title>stable diffusion(四)——Hypernetworks原理与实践</title>
    <link href="https://oysz2016.github.io/post/f079523.html"/>
    <id>https://oysz2016.github.io/post/f079523.html</id>
    <published>2023-06-02T10:27:34.297Z</published>
    <updated>2023-06-02T10:37:34.479Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>这个系列会分享下stable diffusion中比较常用的几种训练方式，分别是Dreambooth、textual inversion、LORA和Hypernetworks。在<a href="https://civitai.com/">https://civitai.com/</a>选择模型时也能看到它们的身影。本文该系列的第四篇</p><ul><li><a href="https://mp.weixin.qq.com/s/NE3Gkr64G3XADVdujtzRXw">Dreambooth</a></li><li><a href="https://mp.weixin.qq.com/s/hJaXd7Eb44CXjl4QwnSZWg">Textual Inversion</a></li><li><a href="https://mp.weixin.qq.com/s/mtdieY-sqIKCiUK16ItQ3g">LoRA</a></li><li><u>Hypernetworks</u></li></ul><h2 id="Hypernetworks"><a href="#Hypernetworks" class="headerlink" title="Hypernetworks"></a>Hypernetworks</h2><p>hypernetworks是一种fine tune的技术，最开始由<a href="https://blog.novelai.net/novelai-improvements-on-stable-diffusion-e10d38db82ac">novel AI</a>开发。hypernetworks是一个附加到stable diffusion model上的小型网络，用于修改扩散模型的风格。</p><h3 id="How-does-Hypernetworks-work"><a href="#How-does-Hypernetworks-work" class="headerlink" title="How does Hypernetworks work?"></a>How does Hypernetworks work?</h3><p>既然Hypernetworks会附加到diffusion model上，那么会附加到哪一部分呢？答案仍然是UNet的cross-attention模块，Lora模型修改的也是这一部分，不过方法略有不同。</p><p>hypernetworks是一个很常见的神经网络结构，具体而言，是一个带有dropout和激活函数的全联接层。通过插入两个网络转换key和query向量，下面的两张图是添加Hypernetworks之前和之后的网络图<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E5%9B%9B%29%E2%80%94%E2%80%94Hypernetworks%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/1.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E5%9B%9B%29%E2%80%94%E2%80%94Hypernetworks%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/2.png" alt=""></p><p>在训练期间stable diffusion model的参数被冻结，而Hypernetworks网络参数可以改变。<strong>由于Hypernetworks参数量很小，在有限的资源下也可很快完成训练。</strong>在按默认的超参数配置的情况下Hypernetworks的文件大小在116Mb。</p><p>checkpoints: 包含生成图像所需要的所有网络结构和参数，模型大小一般在2～7GB，而Hypernetworks一般低于200Mb。Hypernetworks无法单独运行，需要使用checkpoints生成图像，</p><p>LoRA: Lora模型和Hypernetworks比较相似，都是通过作用于UNet的cross-attention模块，改变生成图像的风格。区别在于LoRA改变的是cross-attention的权重，而Hypernetworks插入了其他的模块。LoRA的结果通常比Hypernetworks更好，而且模型结构都很小，基本都低于200Mb。值得注意的是，LoRA指的是一种数据存储的方式，没有定义训练过程，因此可以和Dreambooth和其他的训练方式结合起来使用</p><p>embeddings: 是textual inversion方法的结果，和Hypernetworks方法一样，不会改变模型，会定义一些新的关键字实现某些样式。embedding和Hypernetworks作用的地方不同。embedding在text encoder中创造新的embedding达到左右模型生成图片的效果。根据一些参考资料，embedding的效果比Hypernetworks要稍好一些。</p><h3 id="训练Hypernetworks"><a href="#训练Hypernetworks" class="headerlink" title="训练Hypernetworks"></a>训练Hypernetworks</h3><p>和embedding(Textual Inversion)比较类似, 训练在同一个tag下，首先第一步需要创建Hypernetworks模型，下面这饿参数需要设置下，有些有推荐值，没有推荐值的可以按照自己炼丹的经验进行设置，或者多尝试几组，下面是我配置的参数<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E5%9B%9B%29%E2%80%94%E2%80%94Hypernetworks%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/3.png" alt=""></p><p>创建完模型后，就可以开始训练了，训练的参数设置也和Textual Inversion十分相似，这里就不赘述了</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E5%9B%9B%29%E2%80%94%E2%80%94Hypernetworks%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/4.png" alt=""></p><p>设置完后，选择底部的<strong>Train Hypernetworks</strong>就可以开始训练了</p><p>先来看看人/物的生成，这里同样适用了Halley的照片作为训练集</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E5%9B%9B%29%E2%80%94%E2%80%94Hypernetworks%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/5.png" alt=""></p><blockquote><p>prompt: blue_sky, cloud, building, Halley, fence, no_humans, outdoors, road, scenery, shiba_inu, sky, tree <hypernet:Hallery:1></p></blockquote><p>头还是挺像的，但身子就不太像了-.-</p><p>下面是风格的生成<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E5%9B%9B%29%E2%80%94%E2%80%94Hypernetworks%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/6.png" alt=""></p><blockquote><p>prompt: beamed_eighth_notes, book, clock, eighth_note, musical_note,plant,potted_plant <hypernet:style_zhu2:1><br>看起来效果没有textual inversion和Dream booth好，也可能是调参数的问题</p></blockquote><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这一系列文章总结了Dreambooth、textual inversion、LoRA和Hypernetworks, 从我自己的实践来看，光生成效果这块dream booth&gt;textual inversion&gt;Hypernetworks。当然我并没有在数据集和参数上做过多的尝试，为了对比效果数据集选用的是同样的，人物和风格的数据都在20张左右，调参根据其他blog/论文和我自己的经验做了预设。有可能在特定的数据集和参数组合下，会发挥出更好的效果。</p><p>在讲Hypernetworks的同时也对比下该方法和之前文章中提到过的几种方法的差异:</p><p>checkpoints: 包含生成图像所需要的所有网络结构和参数，模型大小一般在2～7GB，而Hypernetworks一般低于200Mb。Hypernetworks无法单独运行，需要使用checkpoints生成图像.</p><p>LoRA: Lora模型和Hypernetworks比较相似，都是通过作用于UNet的cross-attention模块，改变生成图像的风格。区别在于LoRA改变的是cross-attention的权重，而Hypernetworks插入了其他的模块。LoRA的结果通常比Hypernetworks更好，而且模型结构都很小，基本都低于200Mb。值得注意的是，LoRA指的是一种数据存储的方式，没有定义训练过程，因此可以和Dreambooth和其他的训练方式结合起来使用</p><p>embeddings: 是textual inversion方法的结果，和Hypernetworks方法一样，不会改变模型，会定义一些新的关键字实现某些样式。embedding和Hypernetworks作用的地方不同。embedding在text encoder中创造新的embedding达到左右模型生成图片的效果。根据一些参考资料，embedding的效果比Hypernetworks要稍好一些。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="diffusion model" scheme="https://oysz2016.github.io/tags/diffusion-model/"/>
    
  </entry>
  
  <entry>
    <title>stable diffusion(三)——LoRA原理与实践</title>
    <link href="https://oysz2016.github.io/post/bc4a4885.html"/>
    <id>https://oysz2016.github.io/post/bc4a4885.html</id>
    <published>2023-05-04T12:41:34.633Z</published>
    <updated>2023-05-05T23:46:24.875Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><h2 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h2><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a></p><p><strong>代码链接:</strong> <a href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a></p><p>LoRA是一种fine tune扩散模型的训练技术。通过对标准的checkpoint模型做微小的修改，可以比checkpoint模型小10到100倍。区别于Dreambooth和textual inversion技术，Dreambooth训练得到的模型很强大，但模型也很大(2-7GB)；而textual inversion生成的模型很小(大约100KB)，但模型效果一般。而LoRA则在模型大小(大约200MB)和效果上达到了平衡。</p><p>和textual inversion一样，不能直接使用LoRA模型，需要和checkpoint文件一起使用。</p><h3 id="How-does-LoRA-work"><a href="#How-does-LoRA-work" class="headerlink" title="How does LoRA work?"></a>How does LoRA work?</h3><p>LoRA通过在checkpoint上做小的修改替换风格，具体而言修改的地方是UNet中的cross-attention层。该层是图像和文本prompt交界的层。LORA的作者们发现微调该部分足以实现良好的性能。</p><p>cross attention层的权重是一个矩阵，LoRA fine tune这些权重来微调模型。那么LoRA是怎么做到模型文件如此小？LoRA的做法是将一个权重矩阵分解为两个矩阵存储，能起到的作用可以用下图表示，参数量由(1000<em> 2000)减少到(1000</em> 2+2000*2), 大概300多倍！</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%B8%89%29%E2%80%94%E2%80%94LoRA%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/2.png" alt=""></p><p>思路其实很简单，LoRA的论文中表明，在对cross attention finetune的时候能大大减少参数量，但不会对性能产生什么负面影响。C站和huggingface上有大量LoRA模型，可见LoRA模型的受欢迎程度。</p><h3 id="cross-attention"><a href="#cross-attention" class="headerlink" title="cross attention"></a>cross attention</h3><p>cross-attention是扩散模型中关键的技术之一，在LoRA中通过微调该模块，即可微调生成图片的样式，而在Hypernetwork中使用两个带有dropout和激活函数的全链接层，分别修改cross attention中的key和value，也可以定制想要的生成风格。可见cross attention的重要性。</p><p>在讲cross-attention之前，先看看经典的transformer中attention的含义，attnetion实际上用了三个QKV矩阵，来计算不同token之间的彼此的依赖关系，Q和K可以用来计算当前token和其他token的相似度，这个相似度作为权值对V进行加权求和，可以作为下一层的token。更通俗点说，Q和k的作用是用来在token之间搬运信息，而value本身就是从当前token当中提取出来的信息. 比较常见的是self-attention，该注意力是一个sequence内部不同token间产生注意力，而cross-attention的区别是在不同的sequence之间产生注意力。</p><p>扩散模型中cross-attention可以用下面这张图说明，出自<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf">High-Resolution Image Synthesis with Latent Diffusion Models</a></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%B8%89%29%E2%80%94%E2%80%94LoRA%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/3.png" alt=""></p><p>先说下这张图的整体的含义，<strong>x是原始的图片，$\tilde{x}$是生成的图片,$\varepsilon$是编码器，$D$是解码器</strong>。<strong>Diffusion Process</strong>即前向过程，会随机添加噪声。<strong>Denoising U-Net</strong>即反向过程，目的是将随机噪声的分布，逐渐去躁生成真实的样本。具体细节可以看之前的文章<a href="https://mp.weixin.qq.com/s/AOPdEToYl7CUd5ygsoRWTQ">扩散模型汇总——从DDPM到DALLE2</a>。</p><p><strong>cross-attention的部分主要体现在上图右侧，和添加进U-Net中作为QKV</strong>。生成模型可以建模成$p(z|y)$,$z$是隐变量(Latent Variable), y是条件。在DDPM中有将time也建模出来，用于告知U-Net模型现在是反向传播的第几步，则建模为$\epsilon_{\theta}(z_t,t,y)$。为了能从多个不同的模态获取y,使用了领域专用编码器(domain specific encoder)$\tau_{\theta}$, 可以编码<strong>Semantic Map</strong>, <strong>Text</strong>, <strong>Representations</strong>, <strong>Images</strong>。得到了$y$和完全是噪声的$z_T$, 再经过cross attention即可融合两部分的特征，可以看作是$y$指导反向过程中$z_T$如何一步步变成去躁后的$z$。下面贴下cross-attention的代码实现。<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">CrossAttention</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>, <span class="title">query_dim</span>, <span class="title">context_dim</span>=<span class="type">None</span>, <span class="title">heads</span>=8, <span class="title">dim_head</span>=64, <span class="title">dropout</span>=0.):</span></span><br><span class="line"><span class="class">        super().__init__()</span></span><br><span class="line"><span class="class">        inner_dim = dim_head * heads</span></span><br><span class="line"><span class="class">        context_dim = default(<span class="title">context_dim</span>, <span class="title">query_dim</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        self.scale = dim_head ** -0.5</span></span><br><span class="line"><span class="class">        self.heads = heads</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        self.to_q = nn.<span class="type">Linear</span>(<span class="title">query_dim</span>, <span class="title">inner_dim</span>, <span class="title">bias</span>=<span class="type">False</span>)</span></span><br><span class="line"><span class="class">        self.to_k = nn.<span class="type">Linear</span>(<span class="title">context_dim</span>, <span class="title">inner_dim</span>, <span class="title">bias</span>=<span class="type">False</span>)</span></span><br><span class="line"><span class="class">        self.to_v = nn.<span class="type">Linear</span>(<span class="title">context_dim</span>, <span class="title">inner_dim</span>, <span class="title">bias</span>=<span class="type">False</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        self.to_out = nn.<span class="type">Sequential</span>(</span></span><br><span class="line"><span class="class">            <span class="title">nn</span>.<span class="type">Linear</span>(<span class="title">inner_dim</span>, <span class="title">query_dim</span>),</span></span><br><span class="line"><span class="class">            nn.<span class="type">Dropout</span>(<span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        )</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">x</span>, <span class="title">context</span>=<span class="type">None</span>, <span class="title">mask</span>=<span class="type">None</span>):</span></span><br><span class="line"><span class="class">        h = self.heads</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        q = self.to_q(<span class="title">x</span>)</span></span><br><span class="line"><span class="class">        context = default(<span class="title">context</span>, <span class="title">x</span>)</span></span><br><span class="line"><span class="class">        k = self.to_k(<span class="title">context</span>)</span></span><br><span class="line"><span class="class">        v = self.to_v(<span class="title">context</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        q, k, v = map(<span class="title">lambda</span> <span class="title">t</span>: <span class="title">rearrange</span>(<span class="title">t</span>, &#x27;<span class="title">b</span> <span class="title">n</span> (<span class="title">h</span> <span class="title">d</span>) -&gt; (<span class="title">b</span> <span class="title">h</span>) n d&#x27;, h=h), (<span class="title">q</span>, <span class="title">k</span>, <span class="title">v</span>))</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        sim = einsum(&#x27;<span class="title">b</span> <span class="title">i</span> <span class="title">d</span>, <span class="title">b</span> <span class="title">j</span> <span class="title">d</span> -&gt; <span class="title">b</span> <span class="title">i</span> <span class="title">j&#x27;</span>, <span class="title">q</span>, <span class="title">k</span>) * self.scale</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        if exists(<span class="title">mask</span>):</span></span><br><span class="line"><span class="class">            mask = rearrange(<span class="title">mask</span>, &#x27;<span class="title">b</span> ... -&gt; <span class="title">b</span> (...)&#x27;)</span></span><br><span class="line"><span class="class">            max_neg_value = -torch.finfo(<span class="title">sim</span>.<span class="title">dtype</span>).max</span></span><br><span class="line"><span class="class">            mask = repeat(<span class="title">mask</span>, &#x27;<span class="title">b</span> <span class="title">j</span> -&gt; (<span class="title">b</span> <span class="title">h</span>) () j&#x27;, h=h)</span></span><br><span class="line"><span class="class">            sim.masked_fill_(~<span class="title">mask</span>, <span class="title">max_neg_value</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # attention, what we cannot get enough of</span></span><br><span class="line"><span class="class">        attn = sim.softmax(<span class="title">dim</span>=-1)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        out = einsum(&#x27;<span class="title">b</span> <span class="title">i</span> <span class="title">j</span>, <span class="title">b</span> <span class="title">j</span> <span class="title">d</span> -&gt; <span class="title">b</span> <span class="title">i</span> <span class="title">d&#x27;</span>, <span class="title">attn</span>, <span class="title">v</span>)</span></span><br><span class="line"><span class="class">        out = rearrange(<span class="title">out</span>, &#x27;(<span class="title">b</span> <span class="title">h</span>) n d -&gt; b n (<span class="title">h</span> <span class="title">d</span>)&#x27;, h=h)</span></span><br><span class="line"><span class="class">        return self.to_out(<span class="title">out</span>)</span></span><br></pre></td></tr></table></figure></p><h3 id="训练LoRA"><a href="#训练LoRA" class="headerlink" title="训练LoRA"></a>训练LoRA</h3><p>区别于之前介绍的Textual Inversion、Dreambooth方法。LoRA是一种训练技巧，可以和其他的方法结合，例如在Dreambooth中，训练的时候可以选择LoRA的方式。在之前Dreambooth的篇章中，介绍了人物的训练，这篇填上风格训练的坑。数据的处理方式和之前的处理一样，训练数据改用特定风格的图片，Concepts中选用<code>Training Wizard (Object/Style)</code>。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%B8%89%29%E2%80%94%E2%80%94LoRA%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/4.png" alt=""></p><p>下面是训练过程中的图片<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%B8%89%29%E2%80%94%E2%80%94LoRA%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/5.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%B8%89%29%E2%80%94%E2%80%94LoRA%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/6.png" alt=""></p><p>为了和Textual Inversion方法对比，选用了和之前一样的prompt</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%B8%89%29%E2%80%94%E2%80%94LoRA%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/7.png" alt=""></p><blockquote><p>prompt: flower, grass, outdoors, playground, a photo of style_zhu cartoon</p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%28%E4%B8%89%29%E2%80%94%E2%80%94LoRA%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/8.png" alt=""></p><blockquote><p>prompt: beamed_eighth_notes, book, clock, eighth_note, musical_note,plant,potted_plant, a photo of style_zhu cartoon</p></blockquote><p>实验了一些prompt的生成效果，和textual inversionx相比Dreambooth+LoRA的方法产生的图片细节更好，也更完整,整体看起来更接近想要学习出的绘画风格。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="diffusion model" scheme="https://oysz2016.github.io/tags/diffusion-model/"/>
    
  </entry>
  
  <entry>
    <title>stable diffusion(二)——Textual Inversion原理与实践</title>
    <link href="https://oysz2016.github.io/post/a1a6d829.html"/>
    <id>https://oysz2016.github.io/post/a1a6d829.html</id>
    <published>2023-04-15T14:38:20.386Z</published>
    <updated>2023-05-06T00:03:40.529Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>这个系列会分享下stable diffusion中比较常用的几种训练方式，分别是Dreambooth、textual inversion、LORA和Hypernetworks。在<a href="https://civitai.com/">https://civitai.com/</a>选择模型时也能看到它们的身影。本文该系列的第二篇</p><ul><li><a href="https://mp.weixin.qq.com/s/NE3Gkr64G3XADVdujtzRXw">Dreambooth</a></li><li><u>Textual Inversion</u></li></ul><h2 id="Textual-Inversion"><a href="#Textual-Inversion" class="headerlink" title="Textual Inversion"></a>Textual Inversion</h2><p><strong>论文地址:</strong><a href="https://arxiv.org/abs/2208.01618">https://arxiv.org/abs/2208.01618</a></p><p><strong>代码地址:</strong><a href="https://github.com/rinongal/textual_inversion">https://github.com/rinongal/textual_inversion</a></p><p><strong>embedding是textual inversion的结果，因此textual inversion也可以称为embedding。该方法只需要3-5张图像，通过定义新的关键词，就能生成和训练图像相似的风格。</strong></p><p>扩散模型中有很多可以定义模型的方法，例如像LoRA，Dreambooth，Hypernetworks。<strong>但textual inversion方法很不一样的地方在于它不用改变模型</strong>。<br>接下来看看这种方法是怎么做的。</p><h2 id="How-does-textual-inversion-work"><a href="#How-does-textual-inversion-work" class="headerlink" title="How does textual inversion work?"></a>How does textual inversion work?</h2><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/1.png" alt=""></p><p>首先需要定义一个在现有模型中没有的关键词，新的关键词会和其他的关键词一样，生成Tokenizer(用不同的数字表示)；然后将其转换为embedding；<strong>text transformer会映射出对于新给的关键词最好的embedding向量。不用改变模型，可以看作在模型中寻找新的表征来表示新的关键字</strong></p><p>textual inversion可以用来训练新的object，下面的示例是在模型中注入玩具猫的例子<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/2.png" alt=""></p><p>textual inversion也可以用来训练画风，下面的示例是学习模版图的画风，迁移到其他图片中<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/3.png" alt=""></p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><h3 id="人物-目标"><a href="#人物-目标" class="headerlink" title="人物/目标"></a><strong>人物/目标</strong></h3><p>该部分和之前介绍的Dreambooth中一样，不再赘述。<br>选用的依然是Halley的图片<br>在webui上点击”Train”-&gt;”Create embedding”<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/4.png" alt=""></p><p>“Name”处为需要学习的目标设置一个新的名字;”Initialization text”无需更改;”Number of vectors per token”表示新建立的”Name”占几个prompt，理论上越大越好，但stable diffusion只支持75个prompt, 会导致可以设置的其他prompt变小，一般设置2-8即可。接着点击创建，则会生成模型文件</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/5.png" alt=""></p><p>接下来是训练的设置，下图箭头处的几个地方需要根据自己的实际情况设置下:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/6.png" alt=""></p><p>“Prompt template”需要注意下, 提供了几种可选的训练模式:</p><ul><li>style_filewords.txt: 表示训练画风</li><li>subject_filewords.txt: 表示训练人物或物体</li></ul><p>由于我在这里训练的是Halley的图片，所以选择了object_filewords.txt。<br>“Max Steps”代表迭代次数，要设置合适的次数，少了会欠拟合，多了会过拟合。一般来说”画风”需要的迭代次数比”人物”要少。是否过拟合/欠拟合，可以通过生成的日志观察。</p><p>设置完成后就可以”Train Embedding”了<br>下面的图片是训练过程中可视化生成的一些图片(有点像tensorboard的功能，webui的交互做的还是挺好的)<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/7.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/8.png" alt=""></p><p>训练完成后，可以在txt2img的界面，查看和添加自己训练好的prompt<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/9.png" alt=""></p><p>接下来验证下训练好的模型的效果<br>先来看看原始模型的输出:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/10.png" alt=""></p><blockquote><p>prompt: blue_sky, cloud, building, dog, fence, no_humans, outdoors, road, scenery, shiba_inu, sky, tree</p></blockquote><p>将prompt中的<code>dog</code>替换为训练的关键词<code>Halley</code>即可</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/11.png" alt=""></p><blockquote><p>prompt: blue_sky, cloud, building, Halley, fence, no_humans, outdoors, road, scenery, shiba_inu, sky, tree</p></blockquote><h3 id="风格"><a href="#风格" class="headerlink" title="风格"></a>风格</h3><p>风格的训练和人物的方式基本一样，可以找些比较有特点的画风数据。<br>周末逛一家拼图店看到了一些很有趣的画风，在网上爬了些数据，首先还是将数据缩放成512*512<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/12.png" alt=""></p><p>后续的步骤: 图像预处理(Use deepbooru for caption, 生成一些图片的关键词)-&gt;训练配置开始训练</p><p>下面图片是训练过程中产生的一些图片，感觉画风的迭代次数可以少一点，相比人物更容易过拟合些。新建的prompt: <code>style_zhu</code><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/13.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/14.png" alt=""></p><p>prompt可以自己随便设计些，主要用于检验模型是否能学习到画风, 例如我想生成一些有书、时钟、植物的图片 </p><p>先来看看不添加训练画风的prompt生成的图片, 看起来是比较符合日常生活中这几样常见物品的组合<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/15.png" alt=""></p><blockquote><p>prompt: beamed_eighth_notes, book, clock, eighth_note, musical_note,plant,potted_plant</p></blockquote><p>添加训练的prompt, 效果还是非常惊艳的<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/16.png" alt=""></p><blockquote><p>prompt: beamed_eighth_notes, book, clock, eighth_note, musical_note,plant,potted_plant, style_zhu</p></blockquote><p>解析来我想画一张有画、草地，游乐园的照片，不添加训练画风prompt生成的图片</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/17.png" alt=""></p><blockquote><p>prompt: flower, grass, outdoors, playground<br>接下来是添加画风生成的图片<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/18.png" alt=""></p><p>prompt: flower, grass, outdoors, playground, style_zhu</p></blockquote><p><strong>站在一个绘画小白的角度，我觉得每一张图片甚至都可以当作可以接近成品的画</strong>, 这里贴一下生成的一批图的缩略图</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Textual%20Inversion%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/19.png" alt=""></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>从<code>Halley</code>的训练和生成结果来看，textual inversion效果比不上Dreambooth, 主观感受Dreambooth的生成结果感觉和实际的<code>Halley</code>相似度差不多80%以上，textual inversion的感觉是有点像，但一眼能看出是两只不同的狗。<br>textual的特点是不改变模型，在原有模型的基础上学习一个新的映射关系，虽然效果比不上Dreambooth，但优势是训练更快速些，而且生成的模型非常小。<strong>例如我训练生成的textual inversion文件只有4kb, 而Dreambooth有4GB。</strong> 在如此小的模型文件下，textual inversion生成的画风效果还是非常不错的。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="diffusion model" scheme="https://oysz2016.github.io/tags/diffusion-model/"/>
    
  </entry>
  
  <entry>
    <title>stable diffusion(一)——Dreambooth原理与实践</title>
    <link href="https://oysz2016.github.io/post/7d24ce99.html"/>
    <id>https://oysz2016.github.io/post/7d24ce99.html</id>
    <published>2023-04-09T11:15:42.159Z</published>
    <updated>2023-05-06T00:05:00.363Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>这个系列会分享下stable diffusion中比较常用的几种训练方式，分别是Dreambooth、textual inversion、LORA和Hypernetworks。在<a href="https://civitai.com/">https://civitai.com/</a>选择模型时也能看到它们的身影。本文该系列的第一篇</p><ul><li><u>Dreambooth</u></li></ul><h2 id="Dreambooth"><a href="#Dreambooth" class="headerlink" title="Dreambooth"></a>Dreambooth</h2><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2208.12242">https://arxiv.org/abs/2208.12242</a></p><p><strong>代码链接:</strong><a href="https://github.com/XavierXiao/Dreambooth-Stable-Diffusion">https://github.com/XavierXiao/Dreambooth-Stable-Diffusion</a></p><p><a href="https://dreambooth.github.io/">https://dreambooth.github.io/</a><br>dreambooth是2022年谷歌发布的，该方式通过向模型注入自定义的主题来fine-tune diffusion model的技术。为什么叫这个名字，google团队给出了解释:</p><blockquote><p>It’s like a photo booth, but once the subject is captured, it can be synthesized wherever your dreams take you.</p></blockquote><p>先来看看效果，仅使用三张图片作为输入，就能在不同的prompt下生成对应的图片。下面的例子用一只狗做示例，生成的图片确实就像狗在不同场景下的照片，Dreambooth模型就像一个照相亭一样。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Dreambooth%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/1.png" alt=""></p><h2 id="How-does-Dreambooth-work"><a href="#How-does-Dreambooth-work" class="headerlink" title="How does Dreambooth work?"></a>How does Dreambooth work?</h2><p>要训练自己数据最直观的方法，就是把自己的图片加入模型迭代时一起训练。但会带来两个问题，一个是过拟合，另一个是语言漂移(<a href="https://arxiv.org/abs/1909.04499">language drift</a>)。<br>而Dreambooth的优势就在于能避免上述的两个问题</p><ul><li>可以用一个罕见的词来代表图片的含义，保证新加入的图片对应的词在模型中没有太多的意义</li><li>为了保留类别的含义，例如上图中的“狗”，模型会在狗的类别基础上微调，并保留对应词的语义，例如给这只狗取名为”Devora”, 那么生成的”Devora”就会特指这只狗。</li></ul><p>区别于textual inversion方法，Dreambooth使用的是一个罕见的词，而textual inversion使用的是新词。Dreambooth会对整个模型做微调，而textual inversion只会对text embedding部分调整</p><h2 id="训练dreambooth"><a href="#训练dreambooth" class="headerlink" title="训练dreambooth"></a>训练dreambooth</h2><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a><strong>数据</strong></h3><ul><li>3-10张图片, 最好是不同角度，且背景有变化的图片</li><li>独特的标识符(unique identifier)</li><li>类的名字(class name)<br>先了解下几个prompt:<br>instance prompt: a photo of [unique identifier] [class name]<br>class prompt: a photo of [class name]</li></ul><p>这里我用无敌帅气的“Halley”的照片</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Dreambooth%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/2.jpeg" alt=""></p><p>那么在这个例子里，instance prompt: a photo of Halley Dog; class prompt: a photo of Dog</p><p>对图片进行裁剪到512*512的，这里推荐一个网站<a href="https://www.birme.net/?target_width=512&amp;target_height=512">https://www.birme.net/?target_width=512&amp;target_height=512</a>可以很方便的做裁剪</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Dreambooth%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/3.png" alt=""></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h3><p>在webui上依次点击”Extensions”-&gt;”Available”-&gt;”Load from”。会搜索出非常多的插件，选择”Dreambooth”并”Install”</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Dreambooth%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/4.png" alt=""></p><p>安装完成后依次点击”Extensions”-&gt;”Installed”-&gt;”Apply and restart UI”</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Dreambooth%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/5.png" alt=""></p><p><strong>可能会遇到些坑，例如我在重新启动UI时，页面崩溃了，发现时代码中也有检测extension并安装的逻辑，注释掉相应的代码就行了</strong></p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>接下来就是Dreambooth的训练页面</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Dreambooth%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/6.png" alt=""></p><p>首先创建一个空的模型，设定好Name和原始的Checkpoint就行</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Dreambooth%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/7.png" alt=""></p><p>然后点击”Dreambooth”-&gt;”Train”,有如下几个地方需要设置，大多数填的内容在上面的篇幅都有介绍到。需要注意的是有两种风格可以选，分别是”Person”和”Object/Style”,根据自己的需求选择对应的就ok</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Dreambooth%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/8.png" alt=""></p><p>然后”Settings”，选择训练时候的一些设置，这里推荐一些参数,”use LORA”需要勾选上，混合精度选上fp16。”Saving”中选上”Generate lora weights when saving during training.”,”Generate”中”Image Generation Scheduler”选择”DDIM”</p><p>模型训练:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Dreambooth%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/9.png" alt=""></p><h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a><strong>效果</strong></h3><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Dreambooth%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/10.png" alt=""></p><blockquote><p>prompt: (4k,masterpiece,best quality), a photo of Halley Dog, black pupils brown whites, Sleeping in a blanket in the snow</p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Dreambooth%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/11.png" alt=""></p><blockquote><p>prompt:(best quality), a photo of Halley Dog, black pupils brown whites, eat dog biscuits</p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Dreambooth%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/12.png" alt=""></p><blockquote><p>prompt:(best quality), a photo of Halley Dog, black pupils brown whites, eat dog biscuits</p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/stable%20diffusion%E2%80%94%E2%80%94Dreambooth%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/13.png" alt=""></p><blockquote><p>prompt:(best quality), a photo of Halley Dog, black pupils brown whites, A chef Halley Dog cooking scallops, wearing a chef hat, in a kitchen, professional</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="diffusion model" scheme="https://oysz2016.github.io/tags/diffusion-model/"/>
    
  </entry>
  
  <entry>
    <title>AI绘画——webui</title>
    <link href="https://oysz2016.github.io/post/c9403f5d.html"/>
    <id>https://oysz2016.github.io/post/c9403f5d.html</id>
    <published>2023-03-26T03:03:52.886Z</published>
    <updated>2023-03-26T03:40:29.745Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span><br>最近半年text to image的应用非常火热，区别于之前视觉里的分类和检测等任务，虽然机器能识别图里的东西是什么很有趣，但是这些任务一个3岁左右的小孩基本也能在很短的时间给出正确的答案。<strong>而text to image所做的任务要更加复杂一些，这些任务基本都需要受过专业训练的人员花费较长时间才能完成。</strong> <strong>而且webui等工具的开源，让更多非专业人士能切实到文本生成图片的震撼。</strong></p><p>春节前后就开始体验webui了，这段时间事情有点多，搁置了更新，这次先更新下webui生成图片的效果。下次更新stable diffusion的技术细节!</p><p>本文使用webui生成图片，网上有挺多安装教程了，这里不赘述了。</p><h1 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h1><p>首先可以去<a href="https://civitai.com/">https://civitai.com/</a>下载需要的模型。可惜的是前几天已经禁止国内IP访问了，好在有些可以部分替代的网站，例如<a href="https://www.4b3.com/ai-drawing-model">https://www.4b3.com/ai-drawing-model</a></p><h1 id="text2image"><a href="#text2image" class="headerlink" title="text2image"></a>text2image</h1><p>生成的时候可以用多组关键词和多个模型生成图片，模型可以设置权重搭配比例，生成混搭的风格。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/AI%E7%BB%98%E7%94%BB%E2%80%94%E2%80%94webui/1.png" alt=""></p><blockquote><p>模型: C站上moxing的模型</p><p>prompt: ((4k,masterpiece,best quality)), shuimobysim, traditional chinese ink painting, lotus, hanfu, maxiskit, dress conservatively<br>1 girl, solo, white hair, long hair, fox ears, white, bikini, fish, many fish near girl, look at viewer, tease<br>Negative prompt: (watermark),sketch, duplicate, ugly, huge eyes, text, logo, monochrome, worst face, (bad and mutated hands:1.3), (worst quality:2.0), (low quality:2.0), (blurry:2.0), horror, geometry, (bad hands), (missing fingers), multiple limbs, bad anatomy, (interlocked fingers:1.2), Ugly Fingers, (extra digit and hands and fingers and legs and arms:1.4), crown braid, ((2girl)), (deformed fingers:1.2), (long fingers:1.2),(bad-artist-anime),extra fingers,fewer fingers,hands up,bad hands, bad feet,shoes, stone, ((bad toe))<br>Steps: 30, Sampler: DPM++ SDE Karras, CFG scale: 3, Seed: 2171486205, Size: 384x896, Model hash: 56be194f47, Model: cetusMix_cetusVersion2Fp16, Denoising strength: 0.5, Clip skip: 2, ENSD: 31337, Hires upscale: 2, Hires upscaler: Nearest, AddNet Enabled: True, AddNet Module 1: LoRA, AddNet Model 1: Moxin_Shukezouma11(494301de3d6e), AddNet Weight A 1: 0.7, AddNet Weight B 1: 0.7, AddNet Module 2: LoRA, AddNet Model 2: Moxin_10(17cd20c7b6ea), AddNet Weight A 2: 0.3, AddNet Weight B 2: 0.3, AddNet Module 3: LoRA, AddNet Model 3: chilloutmixss_xss10(9d82c7787e79), AddNet Weight A 3: 0.5, AddNet Weight B 3: 0.5, AddNet Module 4: LoRA, AddNet Model 4: firekeeperLoraFrom_fierkeeper16(acd58eb24484), AddNet Weight A 4: 0.8, AddNet Weight B 4: 0.8</p></blockquote><p>下面的图使用了两个lora模型，一个是之前比较火热的KoreanDoll，用来生成脸部和身体，另一个是英雄联盟里一些角色的模型，用来生成风格。两种模型混搭后可以出现类似cosplay的效果。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/AI%E7%BB%98%E7%94%BB%E2%80%94%E2%80%94webui/2.png" alt=""></p><blockquote><p>模型: C站上samira的模型</p><p>prompt: (8k, RAW photo:1.2),best quality, ultra high res, sky, field, grass, samira (league of legends), league of legends, 1girl, jewelry, tattoo, eyepatch, earrings, green eyes, braid, long hair, dark skin, gloves, armor, navel, bracelet, lips, hair over shoulder, smile, looking at viewer, arm tattoo, mole, mole above mouth <lora:samira_v1:1> <lora:KoreanDollLikeness:0.3><br>Negative prompt: sword, holding, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, censored, letterbox, blurry<br>Steps: 20, Sampler: DPM++ SDE Karras, CFG scale: 7, Seed: 1059483644, Size: 512x512, Model hash: fc2511737a, Model: chilloutmix_NiPrunedFp32Fix, Denoising strength: 0.7, Clip skip: 2, ENSD: 31337, Hires resize: 768x1024, Hires upscaler: Latent</p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/AI%E7%BB%98%E7%94%BB%E2%80%94%E2%80%94webui/3.png" alt=""></p><blockquote><p>模型: C站上Jinx的模型</p><p>prompt: JinxLol,mature female,1girl, solo,looking at viewer, navel, gloves, fingerless gloves, character name, midriff, bare shoulders, looking at viewer, gun, crop top, belt,outdoors,<lora:JinxLolEp8dim8:1>, <lora:Jinx:1> <lora:KoreanDollLikeness:0.3><br>Negative prompt: (low quality, worst quality:1.3), (lowres), blurry,text,watermark,signature,artist name,letterboxed, female pubic hair,realism<br>Steps: 20, Sampler: DPM++ 2M Karras, CFG scale: 7, Seed: 2754482513, Size: 512x768, Model hash: fc2511737a, Model: chilloutmix_NiPrunedFp32Fix, Denoising strength: 0.5, Clip skip: 2, Hires upscale: 2, Hires upscaler: Latent</p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/AI%E7%BB%98%E7%94%BB%E2%80%94%E2%80%94webui/4.png" alt=""></p><blockquote><p>模型: C站上Ahri的模型</p><p>prompt:(8k, RAW photo:1.2),best quality, ultra high res, ahri, ahri_(league_of_legends), 1girl, absurdres, animal_ears, black_hair, breasts, detached_sleeves, distr, highres, facial_mark, fox_ears, fox_tail, hand_up, league_of_legends, long_hair, long_sleeves, magic, multiple_tails, orange_eyes, parted_lips, solo, standing, tail, full body <lora:Ahri:0.6> <lora:KoreanDollLikeness:0.3><br>Negative prompt: (painting by bad-artist-anime:0.9), (painting by bad-artist:0.9), watermark, text, error, blurry, jpeg artifacts, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, artist name, (worst quality, low quality:1.4), bad anatomy</p></blockquote><h1 id="controlNet"><a href="#controlNet" class="headerlink" title="controlNet"></a>controlNet</h1><p>controlNet也是用来引导图片生成的，好处是在文本生成图片的基础上，加上另一张图做引导，可以让生成的图在一些底层特征上看起来相似。可以采用的特征分为以下几种:</p><ul><li>canny: 边缘检测</li><li>depth: 深度检测</li><li>hed:边缘检测</li><li>mlsd: 线段检测</li><li>normal_map: 建模识别</li><li>openpose: 姿势骨骼提取</li><li>openpose_hand:姿势骨骼+手部</li><li>scribble: 提取黑白稿</li><li>fake_scribble: 涂鸦风格提取</li><li>segmentation: 分割</li></ul><p>给大家推荐一个整理了各种画家风格的网站，可以供diffusion model生成类似风格的图像：<a href="https://lib.kalos.art/topic?model=1&amp;topic=0">https://lib.kalos.art/topic?model=1&amp;topic=0</a></p><p>引导图(昨天去商场夹的公仔):<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/AI%E7%BB%98%E7%94%BB%E2%80%94%E2%80%94webui/5.png" alt=""></p><p>生成图<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/AI%E7%BB%98%E7%94%BB%E2%80%94%E2%80%94webui/6.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/AI%E7%BB%98%E7%94%BB%E2%80%94%E2%80%94webui/7.png" alt=""></p><blockquote><p>prompt: ((4k,masterpiece,best quality)), cat, animal painting by Jasmine Becket-Griffith, detailed，multicoloured</p></blockquote><p>再来张中国风</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/AI%E7%BB%98%E7%94%BB%E2%80%94%E2%80%94webui/8.png" alt=""></p><blockquote><p>prompt: cat, animal painting by  Bada Shanren, beautiful details, shuimobysim, traditional chinese ink painting</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="diffusion model" scheme="https://oysz2016.github.io/tags/diffusion-model/"/>
    
    <category term="text to image" scheme="https://oysz2016.github.io/tags/text-to-image/"/>
    
  </entry>
  
  <entry>
    <title>半监督学习汇总——检测</title>
    <link href="https://oysz2016.github.io/post/bcf25936.html"/>
    <id>https://oysz2016.github.io/post/bcf25936.html</id>
    <published>2023-02-18T02:30:32.663Z</published>
    <updated>2023-02-18T03:17:47.797Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span><br>上一篇文章中介绍了<a href="https://oysz2016.github.io/post/18365ad2.html#more">半监督学习——分类</a>的一些文章，今天来看一下半监督学习应用在检测领域的一些思路。</p><h1 id="STAC"><a href="#STAC" class="headerlink" title="STAC"></a>STAC</h1><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2005.04757">https://arxiv.org/abs/2005.04757</a></p><p><strong>代码链接:</strong><a href="https://github.com/google-research/ssl_detection/">https://github.com/google-research/ssl_detection/</a></p><p>之前的半监督学习(Semi-supervised learning, SSL)基本应用在分类领域，而STAC是在目标检测领域应用半监督学习的论文, 该细分领域为半监督目标检测(Semi-supervised Object Detection, SS-OD)。<strong>其思想和分类中使用的一致性学习比较相似，也分为teacher网络和student网络</strong>，思想如下图所示</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/1.png" alt=""></p><p>流程如下:</p><ul><li>用有标签数据训练一个teacher模型</li><li>用训练好的teacher模型在无标签的数据上生成伪标签(包含bbox，label)</li><li>对无标签图片应用强数据增广，在几何变换的增广时，bbox也要做相应的增广</li><li>训练时计算无标签的损失和有监督的损失</li></ul><p>下图是应用的一些增广的可视化<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/2.png" alt=""></p><p>下图对比了有监督的模型，增加RandAugment和使用STAC性能的变化<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/3.png" alt=""></p><h1 id="soft-teacher"><a href="#soft-teacher" class="headerlink" title="soft teacher"></a>soft teacher</h1><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2106.09018">https://arxiv.org/abs/2106.09018</a></p><p><strong>代码链接:</strong><a href="https://github.com/microsoft/SoftTeacher">https://github.com/microsoft/SoftTeacher</a></p><p>soft teacher可以看作是对STAC的改进，STAC的方法简单有效，但是teacher模型和student模型是分阶段训练的。由于student模型会依赖于teacher模型在无标签数据上生成的伪标签，若teacher模型生成了错误且置信度高的伪标签，会造成误差的传递，这也是分阶段训练方式的通病。因此，soft teacher认为端到端训练能取得更好的效果。</p><p>soft teacher的网络结构如下图所示:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/4.png" alt=""></p><p>teacher模型和student模型采用完全相同的结构，其流程为:</p><ul><li>对于无标签数据，弱数据增广后使用teacher模型，nms后得到检测结果</li><li>soft teacher改进的原因主要是考虑到错误的伪标签对student网络学习非常不利，因此使用了”soft”, 具体而言是使用了可靠性度量作为伪标签的权重，而不是所有的伪标签同等重要。可靠性度量采用的是检测的分数</li><li>对于分类和边框回归两个任务，分别使用了两种度量方式保证生成的伪标签的可靠性<ul><li>对于分类分支，使用分类置信度作为判断标准，设置较高的阈值能提升分类标签的可靠性</li><li>对于检测分支, 采用的是box jittering的方式，具体而言，会在候选框周围生成一定偏移量的新bbox，并将新bbox通过teacher模型修正bbox的结果。将该过程重复n次，计算n次修正之后bbox的方差，公式如下。方差越小代表bbox的可信度越高，figuer3(c)的图可以反映方差和bbox预测效果的关系。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/5.png" alt=""></li></ul></li></ul><p>如下图(b)所示, 候选框的定位精度和分类置信度没有很强的相关性，这点在很多检测论文中都有提及并且有优化，例如IOU Net, yolox中的Decoupled head<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/6.png" alt=""></p><p>从实验结果可以看出，相比STAC有较大提升<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/7.png" alt=""></p><h1 id="Dense-teacher"><a href="#Dense-teacher" class="headerlink" title="Dense teacher"></a>Dense teacher</h1><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2106.09018">https://arxiv.org/abs/2106.09018</a></p><p><strong>代码链接:</strong><a href="https://github.com/microsoft/SoftTeacher">https://github.com/microsoft/SoftTeacher</a><br>之前基于伪标签分类具有以下问题:</p><ul><li>Thresholding: 阈值用来区分teacher模型在无标签数据上生成的分类和bbox结果应该如何取舍。作为预定义的数值，阈值非常难选取。设置的较大，会导致召回难以提升；设置的较小，精度会难以提升。</li><li>NMS: NMS的问题和threshold相似，都是预定义的值。设置的较大，会过滤掉一些稍微密集的正样本，较小会有些假正例被保留下来</li><li>Label Assign: </li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/8.png" alt=""></p><p>说了之前伪标签这么多的缺点，接下来看看dense teacher是怎么做的。<br>在dense teacher中提出了密集伪标签(dense pseudo-label, DPL),区别于之前伪标签和有标注数据形式一致。伪标签类似FCOS的中间产物，具体而言将feature map通过cls head输出logits, 再通过sigmoid生成dense label。生成的sigmoid数值是连续的，因此需要使用Quality Focal Loss(Generalized Focal Loss)作为损失函数监督student模型训练。<strong>需要注意的是dense label并不是feautre输出的是每个点的分类和回归结果，类似FCOS的训练方式</strong>。dense label如下图中绿色区域</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/9.png" alt=""></p><p>DPL中有丰富的信息，但由于没有阈值的过滤，也保留了很多低分的信息。直观上看，这些低分信息对模型帮助很小，这部分区域可以看作难区分的负样本，文中也证明了保留该信息作为监督信号会损害student模型的学习。因此根据teacher模型的特征丰富度得分(Feature Richness Score, FRS),划分learning region和suppressing region。具体的，通过FRS选取top k%的像素作为学习区域，其他区域被抑制为0.</p><p>该方法相比之前方法也有一定提升，但由于伪标签的生成和FCOS相似，不太适用于rcnn等两阶段基于anchor的方法</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A3%80%E6%B5%8B/10.png" alt=""></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>由于目标检测包含了分类和定位两个任务，因此目标检测场景下的半监督学习和分类场景还是非常相似的。整体思路是使用伪标签/一致性的方法对无标签数据监督。STAC将分类中半监督的方法借鉴过来，在检测领域的半监督取得了很好的效果；而soft teacher进一步思考伪标签的合理性，避免teacher模型误差的传播，区别于分类模型，在额外的定位任务上思考如何评估定位的准确性；Dense teacher则思考半监督学习和检测任务中的阈值对模型整体效果的影响，整体思路也借鉴了anchor free的FCOS模型的思想，进一步提升了半监督带来的收益。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="目标检测" scheme="https://oysz2016.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    <category term="半监督" scheme="https://oysz2016.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3/"/>
    
  </entry>
  
  <entry>
    <title>半监督学习汇总——分类</title>
    <link href="https://oysz2016.github.io/post/18365ad2.html"/>
    <id>https://oysz2016.github.io/post/18365ad2.html</id>
    <published>2023-02-04T10:56:45.104Z</published>
    <updated>2023-02-18T03:17:16.009Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>半监督学习是机器学习中很常用的一种算法，之前有了解些大概，但没系统性的看过该领域的文章。趁着有空总结了下该领域的文章，由于文章比较多，整理完会分成两部分:</p><ul><li><strong>半监督学习——分类</strong></li><li><strong>半监督学习——检测</strong><br>这篇文章是分类，对半监督学习感兴趣的朋友可以期待下。</li></ul><h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><p><strong>监督算法:</strong> 监督算法的训练数据是有标签的，目的是让模型能正确预测数据的标签</p><p><strong>无监督算法:</strong> 训练数据没有标签，根据数据自身的特性设计模型进行分类</p><p><strong>半监督算法:</strong> 半监督算法是介于无监督和监督算法之间的一种算法类型。其特点是有少量有标签的数据，以及大量的无标签数据，可以得到只用有标签数据训练更好的结果</p><h1 id="π-model"><a href="#π-model" class="headerlink" title="π model"></a>π model</h1><p>π model网络结构如下图所示。因为是半监督学习，既有有标签数据，也有无标签数据，π model将网络分为了两部分:</p><ul><li>第一部分为图中使用交叉熵损失。该部分为有label的数据$x_i$，$z_i$为模型预测结果，用于和$y_i$比对，使用corss entropy</li><li>第二部分使用MSE损失。该部分为无label的数据$x_i$，$x_i$会经过数据增广和模型中的drop out，这两部分都是为了引入一定的数据差异。同一个输入经过两次增广和dropout之后得到特征$z_i$和$\widetilde{z_i}$，使用MSE loss，目的是使增广前后的特征尽可能的相似。<br>两个损失会经过weighted sum配置合适的权重，得到最终的loss</li></ul><p>下面是π model的伪代码</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/2.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/3.png" alt=""></p><h1 id="Temporal-ensembling"><a href="#Temporal-ensembling" class="headerlink" title="Temporal ensembling"></a>Temporal ensembling</h1><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/1610.02242">https://arxiv.org/abs/1610.02242</a></p><p><strong>代码链接:</strong> <a href="https://github.com/ferretj/temporal-ensembling">https://github.com/ferretj/temporal-ensembling</a><br>Temporal ensembling的方法和π model非常相似，核心思想都是想利用好有监督的数据和无监督的数据，改进的原因是在π model中使用了两次模型增广和推理，耗时比较长，在Temporal ensembling中使用了<strong>Temporal</strong>的模型，具体而言是使用了之前epoch得到的特征结果去做对比，模型结构如下图:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/4.png" alt=""></p><p>$\widetilde{z_i}$为之前epoch模型保存的特征，$z_i$为当前模型的推理结果，并且会保存给下一个epoch使用</p><p>伪代码如下:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/5.png" alt=""><br>注意, 在更新特征z时，不光用到了当前的特征，也会考虑到之前所有的特征，具体可以看伪代码最后几行</p><p>在大量半监督数据上做了消融实验，验证了Temporal ensembling的有效性</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/6.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/7.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/8.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/9.png" alt=""></p><h1 id="Mean-teacher"><a href="#Mean-teacher" class="headerlink" title="Mean teacher"></a>Mean teacher</h1><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/1703.01780">https://arxiv.org/abs/1703.01780</a></p><p><strong>代码链接:</strong> <a href="https://github.com/CuriousAI/mean-teacher">https://github.com/CuriousAI/mean-teacher</a></p><p>mean teacher的思想和Temporal ensembling也非常相似。改进的原因是在Temporal ensembling中每个epoch才会更新一次对比的特征，在大型的数据集上训练时，用于对比的特征更新的非常慢，时间成本也很高，因此在mean teacher中使用更新模型的方式取代原来更新特征的方式。思想如下图所示</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/10.png" alt=""></p><p>student model即当前模型，teacher model为更新的模型，会参考当前模型和之前模型的权重，得到一个新的模型。更新的方式用公式</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/11.png" alt=""></p><p>目的也是使student mode和teacher model预测出的特征一致</p><p>mean teache和之前的工作π model以及Temporal ensembling进行了对比</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/12.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/13.png" alt=""></p><h1 id="MixMatch"><a href="#MixMatch" class="headerlink" title="MixMatch"></a>MixMatch</h1><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/1905.02249">https://arxiv.org/abs/1905.02249</a></p><p><strong>代码链接:</strong> <a href="https://github.com/YU1ut/MixMatch-pytorch">https://github.com/YU1ut/MixMatch-pytorch</a></p><p>mixmatch是最小化熵的方法，和前面介绍的三篇一致性正则化法有些差异。最小化熵方法的思想基于机器学习中的一个共识。即分类器的边界边际分布的高密度区域(说人话就是不能从类中心去划分边界)。因此强迫分类器对未标记数据做出低熵预测。实现方法是在损失函数中增加一项，最小化$p_{model}(y|x)$对应的熵。</p><p>mixmatch的思想在下图中伪代码展示的很清楚</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/14.png" alt=""></p><p>一个batch中有B份有标签的数据X和B份无标签的数据U</p><ul><li>将有标签的数据增广一次</li><li>将无标签的数据增广K次</li><li>对无标签的数据增广后的结果模型推理后分类，对K个结果取平均，得到模型预测的标签</li><li>使用temperature sharpening算法，对上一步得到的标签后处理。sharpening算法公式如下:</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/15.png" alt=""></p><ul><li>增强后带标签的数据组成一个batch，即$\widetilde{X}$</li><li>增强后无标签的数据，和预测出的标签组成K个batch，即$\widetilde{U}$</li><li>将$\widetilde{X}$和$\widetilde{U}$混合，得到新的数据集W</li><li>应用mixup将增广后有标签的数据$\widetilde{X}$和新的数据W混合得到$X^{‘}$</li><li>应用mixup将增广后无标签的数据$\widetilde{U}$和新的数据W混合得到$U^{‘}$</li></ul><p>得到两个新的数据集后就可以按mixup混合的思路进行训练</p><p>sharpening的大致流程如下图所示</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/16.png" alt=""></p><p>接下来是实验结果部分:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/17.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/18.png" alt=""></p><h1 id="ReMixMatch"><a href="#ReMixMatch" class="headerlink" title="ReMixMatch"></a>ReMixMatch</h1><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/1911.09785">https://arxiv.org/abs/1911.09785</a></p><p><strong>代码链接:</strong><a href="https://github.com/google-research/remixmatch">https://github.com/google-research/remixmatch</a></p><p>从名字上可以看出ReMixMatch是MixMatch的改进版本。MixMatch中比较重要的思想是猜测无标签数据的标签，使用最小化熵的方法做训练。<br>ReMixMatch的改进主要包含两部分: Distribution Alignment和Augmentation Anchor。伪代码如下:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/19.png" alt=""></p><ul><li><strong>Distribution Alignment</strong>。考虑到猜测无标签数据的label有可能存在噪声，因此考虑到使用有标签数据的标签分布，对无标签的猜测进行对齐。对应代码中的第7行。如下图所示，$q_b$对应Label guess。$\widetilde{p}(y)$是一个运行平均版本的无标签猜测，$p(y)$是有标签数据的标签分布。对齐之后的标签猜测如下公式<script type="math/tex; mode=display">q_b=Normalize(q_b*\frac{p(y)}{\widetilde{p}(y)})</script></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/20.png" alt=""></p><ul><li><strong>Augmentation Anchor</strong>。考虑到简单数据增广得到的预测结果会比复杂增广得到的预测结果要更加可信。因此对于一张图片，首先会进行弱增广，再进行多次强增广。弱增广和强增广共同使用一个标签进行mixup和模型训练</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/21.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/22.png" alt=""></p><p>接下来是实验结果部分:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/23.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/24.png" alt=""></p><h1 id="FixMatch"><a href="#FixMatch" class="headerlink" title="FixMatch"></a>FixMatch</h1><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/2001.07685">https://arxiv.org/abs/2001.07685</a></p><p><strong>代码链接:</strong><a href="https://github.com/google-research/fixmatch">https://github.com/google-research/fixmatch</a></p><p>在FixMatch中没有采用MixMatch系列中有标签数据和无标签数据猜测标签后互相mixup作为训练样本的方法，本身思想和mean teacher及之前方法更相似。<strong>FixMatch中的Fix主要强调的是混合两种数据增广方式，分别是弱增广和强增广</strong><br>弱数据增广的方式:</p><ul><li>平移</li><li>反转</li><li>平移&amp;反转<br>强数据增广:</li><li>cutout</li><li>random augment</li><li>control theory augment<br>其中若数据增广和强数据增广的前两种方式都比较常见，这里详细说下control theory augment数据增广方法:</li><li>该数据增广方法中有18个候选集，例如旋转，色彩变换等</li><li>初始化transform的权重$W=[w_1,w_2,…,w_18]$</li><li>随机选择其中两个增广方法i和j，增广的权重分别为$w_i/(w_i+w_j)$, $w_j/(w_i+w_j)$</li><li>将两个增广图像混合，更新transform的权重$W$, 第一个公式可以看作MAE损失，第二个公式是用动量的方式更新transform的权重。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/30.png" alt=""></li></ul><p>因此可以将control theory augment看作是可以学习的rand augment, 能学习出好的增广方式，使得分类更准确。为什么不用auto augment呢？因为auto augment需要标签作为学习的条件，而半监督中大部分数据都没有标签</p><p>对于无标签的数据，会先经过弱数据增广获取伪标签，只有某一类的置信度大于一定的阈值才会执行为标签的生成（下图中红色部分），生成的伪标签用于监督强增广的输出值。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/25.png" alt=""></p><p>接下来是实验结果部分:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/26.png" alt=""></p><h1 id="Noisy-Student"><a href="#Noisy-Student" class="headerlink" title="Noisy Student"></a>Noisy Student</h1><p><strong>论文链接:</strong><a href="https://arxiv.org/abs/1911.04252">https://arxiv.org/abs/1911.04252</a></p><p><strong>代码链接:</strong><a href="https://github.com/google-research/noisystudent">https://github.com/google-research/noisystudent</a></p><p>Noisy Student和之前的一致性正则化法也非常相似。区别在于强调了在student模型中加入噪声。在这篇文章中有teacher model和student model的概念。teacher是使用有标签数据训练的模型，student是使用teacher模型预测了无标签数据的标签后，用无标签数据训练的模型。</p><p>而Noisy Student强调的是在student模型中加入噪声，teacher模型和student模型可以用不同的模型训练，也可以使用相同的模型。要思考加入噪声的原因，可以假设teacher和student相同结构联合训练的场景，则student模型不加入噪声，则预测结果和通过teacher模型生成的伪标签会完全一致，student模型就失去了更新的动力。所以加入噪声是很有必要的。</p><p>Noisy Student的思想如下图所示:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/27.png" alt=""></p><p>关于student模型使用数据增广和dropout的消融实验如下：</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/28.png" alt=""></p><p>该文章列举了一些消融实验得到的经验，基本都很符合直觉:</p><ul><li>使用性能好的teacher模型能得到更好的结果</li><li>大量的无标签数据是必要的</li><li>在某些场景下，soft伪标签比hard伪标签效果要好</li><li>大的学生模型很重要</li><li>数据均衡对小模型很重要</li><li>有标签数据和无标签数据联合训练效果更好</li><li>无标签数据:有标签数据的比值越大，该方法越有效</li><li>从头开始训练student有时比用teacher初始化student效果要好</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过阅读上面的7篇文章，半监督学习大概可以分成3种训练方式，分别是<strong>一致性正则(Consistency Regularization Model)</strong>, <strong>伪标签模型(Proxy Label Model)</strong>, <strong>一致性正则&amp;伪标签模型</strong></p><ul><li><strong>一致性正则:</strong> 图片增广前后，模型的预测结果应该相同。在该种方法中一般会加入数据增广和dropout，引入随机性，损失函数会使用MSE Loss。例如π model，Temporal ensembling，Mean teacher都是该类型的方法</li><li><strong>伪标签模型:</strong> 先在有标签的数据上做训练，然后预测无标签数据的伪标签；模型的训练包括有标签的训练和伪标签的训练。例如MixMatch、ReMixmatch都是该类型的方法</li></ul><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="半监督" scheme="https://oysz2016.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3/"/>
    
    <category term="分类" scheme="https://oysz2016.github.io/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>决策树汇总——ID3、C4.5、CART</title>
    <link href="https://oysz2016.github.io/post/5f5a1d32.html"/>
    <id>https://oysz2016.github.io/post/5f5a1d32.html</id>
    <published>2023-01-15T06:58:46.653Z</published>
    <updated>2023-01-15T07:48:24.536Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span><br>决策树是机器学习中的经典算法之一，十大机器学习算法中就包含决策树算法(Decision Tree). 顾名思义，决策树是基于树结构进行决策的，根据算法不同会使用多叉树或者二叉树。其实人类在日常生活中，遇到问题时也会使用决策树进行判断。例如西瓜书里有列举挑选西瓜的例子。</p><p>一颗决策树一般包含一个根结点，若干个内部节点和若干叶节点，根结点是判断的开始，内部节点是一些子判断流程，而叶节点对应于决策的结果。决策树作为机器学习算法的一种，其目的是产生一颗泛化能力强的树，能处理训练集中没有见到的情形，并能正确决策。</p><h2 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h2><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>信息熵是度量样本集合纯度最常用的一种指标，令D中第k类样本的比例为$p_k$, 则D的信息熵为<br>$Ent(D)=-\sum_{k=1}^{|y|} p_klog_2{p_k}$</p><p>信息增益=信息熵-条件熵<br>$Gain(D,a)=Ent(D)-\sum_{v=1}^{V} \frac{D^v}{D}Ent(D^v)$</p><p>以西瓜书中的列子<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%86%B3%E7%AD%96%E6%A0%91%E6%B1%87%E6%80%BB/2.png" alt=""></p><p>17个样本中正例有8个，负例有9个，则根结点的信息熵为:<br>$Ent(D)=-\sum_{k=1}^{2} p_klog_2{p_k}=-(\frac{8}{17}log_2 \frac{8}{17}+\frac{9}{17}log_2 \frac{9}{17})=0.998$</p><p>然后计算相应属性的条件熵，以色泽属性为例，可以分为 $D^1$(色泽=青绿)有6个样本，正反各3个，$D^2$(色泽=乌黑)有6个样本，正4，饭2，$D^3$(色泽=浅白)，有5个样本，正1反4.则3个分支节点的信息熵为：<br>$Ent(D^1)=-(\frac{3}{6}log_2 \frac{3}{6}+\frac{3}{6}log_2 \frac{3}{6})=1$<br>$Ent(D^2)=-(\frac{4}{6}log_2 \frac{4}{6}+\frac{2}{6}log_2 \frac{2}{6})=0.918$<br>$Ent(D^2)=-(\frac{1}{5}log_2 \frac{1}{5}+\frac{4}{5}log_2 \frac{4}{5})=0.722$</p><p>则色泽的信息增益为:</p><p>$Gain(D,a)=Ent(D)-\sum_{v=1}^{V} \frac{D^v}{D}Ent(D^v)=0.998-(\frac{6}{17}<em>1+\frac{6}{17}</em>0.918+\frac{5}{17}*0.722)=0.109$</p><p>也可以得到其他属性的信息增益，在根节点的位置信息增益最大的属性为纹理Gain=0.381.<br>确定第一个分支后，后面的内部节点和叶节点叶通过类似方式递归判断，则生成的决策树为:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%86%B3%E7%AD%96%E6%A0%91%E6%B1%87%E6%80%BB/3.png" alt=""></p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>ID3没有剪纸的策略，容易过拟合</li><li>没有考虑缺失值</li><li>只能处理离散分布的特征</li></ul><h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><p>C4.5在ID3的基础上做了一些改进:</p><ul><li>引入后剪枝缓解过拟合</li><li>可以处理属性中有连续值的情形，具体做法是对于某属性在区间$[a^i, a^{i+1})$，若取任意值产生的划分结果相同，则取该区间的中位点作为划分点</li><li>对于缺失值，C4.5的做法是用没有缺失的样本子集计算信息增益</li></ul><h3 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h3><p>剪枝的目的是防止决策树过拟合，在构建决策树时，为了能正确分类训练样本。决策树的结点划分过程会不断重复，可能导致分的过细，在训练集拟合的很好，但测试集的效果变得很差。所以需要剪枝帮助模型获得泛化性。<br>剪枝有<strong>预剪枝</strong>和<strong>后剪枝</strong>两种，引用下西瓜书里关于两种剪枝方式的差异</p><blockquote><p><strong>预剪枝</strong>是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;<strong>后剪枝</strong>则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点.</p></blockquote><p>判断上述两种剪枝方式是否能带来性能提升的方式是采用验证集，验证剪枝前后的效果。</p><p>预剪枝和后剪枝的例子见下面两图:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%86%B3%E7%AD%96%E6%A0%91%E6%B1%87%E6%80%BB/4.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/%E5%86%B3%E7%AD%96%E6%A0%91%E6%B1%87%E6%80%BB/5.png" alt=""></p><h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ul><li>C4.5使用的是多叉树，二叉树效率更高</li><li>只能用于分类</li><li>使用的熵模型有大量耗时的对数运算，连续值还有排序运算</li><li>在构造树的过程中，对数值属性值需要按照其大小排序，从中选择一个分割点。当训练集打到内存无法容纳时，会无法运行</li></ul><h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><h3 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h3><p>CART使用基尼指数作为划分属性的度量<br>$Gini(D)=\sum_{k=1}^{|y|}\sum_{k^{‘} \neq k} p_k p_{k^{‘}}=1- \sum_{k=1}^{|y|}p_k^2$</p><p>$Gini(D)$反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此$Gini(D)$越小，则数据集D的纯度越高</p><p>将其写成和信息增益等价的形式，则基尼指数为</p><p>$ Gini_index(D,a)=\sum_{v=1}^{V} \frac{D^v}{D} Gini(D^v) $</p><p>在是否要划分决策树的判断时，CART会选择使得划分后基尼指数最小的属性作为最优划分属性。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://oysz2016.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="决策树" scheme="https://oysz2016.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>因果图的思想优化长尾问题</title>
    <link href="https://oysz2016.github.io/post/3a13345e.html"/>
    <id>https://oysz2016.github.io/post/3a13345e.html</id>
    <published>2023-01-01T04:00:43.755Z</published>
    <updated>2023-01-01T04:21:49.320Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2009.12991">https://arxiv.org/abs/2009.12991</a><br><strong>代码链接:</strong> <a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch">https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch</a><br>这篇文章和之前有介绍过的一篇场景图生成的文章<a href="https://arxiv.org/abs/2002.11949">Unbiased Scene Graph Generation from Biased Training</a>思想比较类似, 作者也是同一个团队。贴一个<a href="https://oysz2016.github.io/post/b4822109.html">传送门</a>，方便感兴趣的同学浏览。<br>之所以说思想类似，是因为这两篇文章不仅都用来解决长尾问题，而且都用到了<strong>因果图</strong>的的思想。在看这篇论文的时候，有些内容读起来还是很难理解的，查阅了一些<strong>统计学</strong>和<strong>因果关系</strong>的相关概念才觉得清晰了些。这些理论也非常有意思，看完后我觉得在阅读这篇文章前还是很有必要学习的。在写这篇blog的时候，我也尽量把需要用到的背景知识整理出来，方便理解。</p><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><h3 id="因果关系"><a href="#因果关系" class="headerlink" title="因果关系"></a>因果关系</h3><p>关于因果关系，在<a href="https://zhuanlan.zhihu.com/p/111306353">https://zhuanlan.zhihu.com/p/111306353</a>中有详细的背景知识。为了方便阅读和加深自己的理解。我在这里精简的引用下，更详细的内容可以去阅读大佬的知乎文章。</p><blockquote><p>《The Book of Why》中将因果理论的探索类比成一个向上的阶梯，包含三个层级：Seeing，Doing和Imaging。<br><strong>第一层级: Association 关联</strong>，对应的是大多数机器学习算法和动物。该层级强调基于被动的观察（passive observation）来预测，通过观察来寻找规律，并非真正的因果。<strong>其本质就是条件概率P(Y|X)，在观察到X的条件下Y发生的概率，也是传统机器学习里被广泛应用的。</strong><br><strong>第二层级：Intervention干预</strong>, 干预指的是消除因果关系中的混杂影响，推导出真正的因果关系，如随机对照试验。<br><strong>第三层级：Counterfactual 反事实</strong>，counterfactual和干预intervention区分的关键在于“hindsight”(事后来看)，即反事实强调在对结果已知观测的基础上再对反事实的问题进行解答。</p><h3 id="因果推断中的变量"><a href="#因果推断中的变量" class="headerlink" title="因果推断中的变量"></a>因果推断中的变量</h3><p>因果推断中有一些专业术语用来表示在因果中不同变量的角色，主要分为<a href="https://zh.wikipedia.org/wiki/%E5%B9%B2%E6%93%BE%E5%9B%A0%E7%B4%A0">混淆变量(confounder)</a>，<a href="https://zh.wikipedia.org/wiki/%E4%B8%AD%E4%BB%8B%E8%AE%8A%E9%A0%85">中介变项(mediator)</a>，<a href="https://zh.wikipedia.org/wiki/%E5%B0%8D%E6%92%9E%E5%9B%A0%E5%AD%90">对撞因子(collider)</a><br><strong>混淆变量</strong>比较通俗的例子是老年人因为退休了会更有时间晨炼，但老年人却比年轻人更容易得癌症，如果不控制年龄的分布，就会得到晨炼的人容易得癌症。这里的年龄就是混淆变量，需要被控制<br><strong>中介效应</strong>指的是从一个变量到另一个变量，中间会有些其他的变量带来影响。比如吃药能带来疾病的好转，可能是药本身起了作用，也可能是心理安慰。比较极端的例子是是不是就会出现用面粉做假药的新闻，对于患者而言，药本身可能没起到作用，但可能由于吃了“药”，觉得自己马上会好，比较积极的心态带来了身体的好转。<br><strong>对撞因子</strong>指的是同时被两个以上变量影响的因素，而这些影响对撞因子的变量之间不见得有因果关系。例如在NBA球员中，会发现身高比较高的人得分率并没有很高，这是因为身高矮的人能进NBA必然是用其他优势弥补了劣势。<strong>身高</strong>和<strong>得分率</strong>之间并没有明显的因果关系，而他们都决定能不能<strong>进NBA</strong>。仔细思考就会发现对撞因子的例子很容易造成幸存者偏差。</p><h3 id="Propensity-Score"><a href="#Propensity-Score" class="headerlink" title="Propensity Score"></a>Propensity Score</h3><p><strong>倾向评分匹配(Propensity Score Matching)</strong> 是一种统计学的方法，指的是在<strong>观察研究中</strong>，由于种种原因，<strong>数据偏差（bias）</strong> 和 <strong>混杂变量（confounding variable）</strong> 较多，<strong>倾向评分匹配的方法正是为了减少这些偏差和混杂变量的影响，以便对实验组和对照组进行更合理的比较。</strong><br>为了能较好说明这个方法的价值，可以用理科里学的随机对照实验(Randomized Controlled Trial data)做对比。随机对照试验在样本量足够的情况下是很科学的评判变量对结果影响的实验方法，但很多时候是不符合科研伦理的，比如要研究吸烟是否有害健康，如果招收大量人员，然后随机分配到吸烟组和不吸烟组，这种实验设计不太容易实现，而且也存在危害测试人员健康的可能。而这个研究课题其实很容易通过<strong>观察的研究数据</strong>进行实验，面对观察的研究数据，如果不加调整，很容易获得错误的结论，比如拿吸烟组健康状况最好的一些人和不吸烟组健康状况最不好的一些人作对比，得出吸烟对于健康并无负面影响的结论。从统计学角度分析原因，这是因为观察研究并未采用随机分组的方法，无法基于大数定理的作用，在实验组和对照组之间削弱混杂变量的影响，很容易产生系统性的偏差。<strong>倾向评分匹配</strong>就是用来解决这个问题，消除组别之间的干扰因素。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>Re-Balanced Training:</strong> re-blance的方法主要有两种，分别是re-sampling和re-weighting。<br><strong>Hard Example Mining:</strong> 不关注于每个类别样本数量的先验分布，而是关注于难样本用于缓解长尾问题，代表方法是focal loss<br><strong>Transfer Learning/Two-Stage Approach:</strong> 作者总结的这类工作的特点是将头部类的knowledge转移到尾部类，用以改善长尾问题。其中比较有代表性的是<a href="https://arxiv.org/abs/1910.09217">Decoupling</a>算法，和受Decoupling启发的<a href="https://arxiv.org/abs/1912.02413">BBN</a>算法。BBN在之前的<a href="https://oysz2016.github.io/post/b4822109.html">关于长尾问题的文章</a>中也有过总结。<br><strong>Causal Inference:</strong> 因果图推理作者主要列举了一些这方面的著作，当然也提到了自己在场景图生成中的文章</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>作者将视角聚焦于梯度优化器中常用的track——<strong>Momentum</strong>。在讲解作者是怎么做的之前，让我们回顾下Momentum。Momentum的思想是累积一个历史的梯度信息用来加速优化器，好处主要有以下两点:</p><ul><li>每次梯度更新的时候，不仅考虑了当前梯度的方向，同时也考虑了之前更新的方向，在梯度优化时，不会抖动的那么随意</li><li>Momentum相当于给梯度优化的方向施加了一个惯性，参数优化时容易突破局部最优解，更可能找到全局最优解.</li></ul></blockquote><p><strong>上面是一些比较定性的分析，关于Momentum为什么能work?</strong><a href="https://distill.pub/2017/momentum/">https://distill.pub/2017/momentum/</a>做了非常好的可视化，可以直观的感受到Momentum为梯度优化带来的改变<br>下面的两张图分别是没有Momentum和使用合理参数设置Momentum对模型优化带来的差异，可以看到Momentum能提高网络训练的稳定性，并且同样的迭代次数更容易收敛到全局最优。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/1.png" alt=""><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/2.png" alt=""><br>回归正题，对于长尾分布的数据集，正是由于Momentum会受之前梯度信息的影响，Momentum所产生的惯性，会带来马太效应，即模型的优化方向会倾向于让模型对头部类的效果更好。<br>作者将Momentum对网络的作用用因果图抽象了出来，如figure 1(a)所示<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/3.png" alt=""><br>上图中，<strong>X代表backbone提取到的特征</strong>，<strong>M代表优化器中的动量</strong>，<strong>Y代表预测结果</strong>，<strong>D代表动量所产生的惯性，由于是长尾数据集，这里的D特指对头部类优化的惯性，而在balanced的数据集中，D对每个类别的贡献是一样的</strong>。图中的箭头表示彼此的影响，例如，X-&gt;Y以为着，Y的得出收到X的影响。从因果图中的关系可以看出<strong>节点M和节点D分别代表混淆变量(confounder)和中介效应(mediator)</strong>。<br><strong>M-&gt;X</strong>代表的是特征图X的是在动量M的影响下训练的,figure 1(b)中可视化了动量M对不同类别的影响，可以发现头部类在动量中占比较大。<br>知道了混淆变量和中介效应之后，需要做的就是<strong>消除这些变量对模型带来的偏见</strong>。和<a href="https://arxiv.org/abs/2002.11949">Unbiased Scene Graph Generation from Biased Training</a>中类似，接下来需要构建TDE(Total Direct Effect)用于消除偏见，公式如下:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/10_4.png" alt=""><br>和公式对应的因果图如Figure 3<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/4.png" alt=""><br>在计算TDE时需要抹除掉混淆变量M对X的影响，但没有办法得到M的分布。在<a href="http://bayes.cs.ucla.edu/jsm-july2012-pdf.pdf">Causal inference in statistics: A primer</a>的书中有提到<strong>Inverse Probability Weighting</strong>的公式，这个公式给出了一种思路，即没有办法得到M的分布时，可以看M和X有没有一一对应关系。在这个方法里，M和X确实是有对应关系的。所以可以将对X的采样看成是对M的近似<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/5.png" alt=""><br>这里将weights和features的通道/维度划分成k组，可以认为是做了k倍的细粒度采样。这样的好处是通过multi-head多重采样能更好的近似。<br>M能够做近似之后，还需要考虑 <strong>倾向评分(Propensity Score)</strong> 的影响，在这个问题中需要对所有类别做归一化的统一分布，也就是考虑每个类别的模长。下面就是得到的Propensity Score的公式, 其中第一项是类别感知的, 其中第一项是class-specific，第二项是class-agnostic。需要第二项的原因是因为从Figure 1(b)中可以看出x也具有bias。</p><script type="math/tex; mode=display">g(i, x^k; w^k_i)=||x^k|| \cdot ||w_i^k|| + \gamma||x^k||</script><p>则公式2中的第一项$P (Y = i|do(X = x))$可以表示为<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/6.png" alt=""><br>公式2中的第二项和第一项的区别在于使用了空数据$x_0$替代x。而其他项保持不变，这一部分是构建反事实的因果图，相当于让网络仅通过M和D得到Y，而x没起到作用。可以把这部分看作是偏差。<br>最终的TDE如下式：<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/7.png" alt=""></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>和其他方法的对比如下:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/8.png" alt=""><br>关于本文方法和其他方法的差异，可以用下图表示。</p><ul><li>在baseline的数据集上有问题是由于训练数据是长尾的，而测试数据是balanced的，存在<strong>分布不匹配</strong>。</li><li><strong>One-stage Re-balancing</strong>的方法本质是<strong>改变了训练数据的分布，这种方式会带来错误的模型建模</strong>；</li><li><strong>Two-stage Re-balancing</strong>的方法是第一阶段先通过原始的数据对模型建模，第二阶段再优化分类器，<strong>对分类器边界做调整</strong>，所以能work</li><li>而本文的方法是在测试的时候将分布做了移动<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Long-Tailed%20Classification%20by%20Keeping%20the%20Good%20and%20Removing%20the%20Bad%20Momentum%20Causal%20Effect/9.png" alt=""><br><strong>优势:</strong></li><li><strong>不需要复杂的stage训练方式</strong></li><li><strong>可适用于多个任务，如图片分类，检测之类</strong></li><li><strong>不需要依赖数据的分布, 感觉这个优势对于online的训练比较有意义, 因为其他的训练方式其实都可以获取到数据的数据分布。</strong><h2 id="笔者总结"><a href="#笔者总结" class="headerlink" title="笔者总结"></a>笔者总结</h2>要使用作者提供的 <a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch">https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch</a> 代码中的<strong>CausalNormClassifier</strong>总结起来有几个要点, 为方便理解，下面要点中的超链接会索引到具体的代码:</li><li>训练的时候需要用<a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch/blob/9726439a702614e99a02e2ba321ec4e56491239e/classification/models/CausalNormClassifier.py#L54">multi-head normalized classifier</a></li><li>训练时需要记录<a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch/blob/9726439a702614e99a02e2ba321ec4e56491239e/classification/run_networks.py#L215">移动的平均特征</a></li><li>测试的时候需要用<strong>counterfactual TDE inference</strong>，即<a href="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch/blob/9726439a702614e99a02e2ba321ec4e56491239e/classification/models/CausalNormClassifier.py#L43">去除具有头部类倾向的部分</a><br>让我们回顾上面的图7，在作者<a href="https://zhuanlan.zhihu.com/p/259569655">博客</a>的评论部分，有非常简洁的总结, 在这里引用下：<blockquote><ol><li>decouple两阶段都是在train过程中，一阶段长尾分布下训练representation + classifier；二阶段直接通过暴力resample来调整classifier。</li><li>de-confound也可以看做两阶段，一阶段在train过程中，通过重采样和normalized的措施来训练representation + classifier；二阶段放在了test过程中，用一阶段中统计的bias来缓解测试中的class bias，得到TDE。<br>这两篇文章都很巧妙的使用了因果图。虽然很多trick可解释性确实不太强，不过细细思考起来，一些算法流程对整体算法的影响还是比较make sence。私以为在如果算法框架中有些trick能带来收益的同时，也会让带来一些问题负面的影响，就很适合用因果图的思想消除负面影响，不过这确实很考验对于问题的抽象能力和对因果图的理解<blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></li></ol></blockquote></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="长尾优化" scheme="https://oysz2016.github.io/tags/%E9%95%BF%E5%B0%BE%E4%BC%98%E5%8C%96/"/>
    
    <category term="因果图" scheme="https://oysz2016.github.io/tags/%E5%9B%A0%E6%9E%9C%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>扩散模型汇总——从DDPM到DALLE2</title>
    <link href="https://oysz2016.github.io/post/d10097db.html"/>
    <id>https://oysz2016.github.io/post/d10097db.html</id>
    <published>2022-12-17T01:25:30.740Z</published>
    <updated>2022-12-25T09:20:29.682Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><p>扩散模型最早是在2015年在<a href="https://arxiv.org/abs/1503.03585">Deep unsupervised learning using nonequilibrium thermodynamics</a>中提出，<strong>其目的是消除对训练图像连续应用的高斯噪声</strong>，可以将其视为一系列<strong>去噪自编码器</strong>。它使用了一种被称为“潜在扩散模型”（latent diffusion model; LDM）的变体。训练自动编码器将图像转换为低维潜在空间。随后在2020年提出的DDPM将扩散模型的思想用于图像生成。</p><p><strong>生成模型</strong>: 给定来自感兴趣分布的观察样本x，生成模型的目标是学习对其真实数据分布$p(x)$进行建模<br><strong>隐变量(latent Variable)</strong>: 对于许多模态，我们可以将我们观察到的数据视为由相关的看不见潜在变量生成的，我们可以用随机变量 z 表示。为什么能用看不见的潜在变量表示，感性的理解可以参考柏拉图洞穴的寓言。在这个寓言中，一群人一生都被锁在一个山洞里，只能看到投射在他们面前的墙上的二维阴影，这是由看不见的三维物体在火前经过而产生的。对于这样的人来说，他们所观察到的一切，实际上都是由他们永远看不到的更高维度的抽象概念决定的。</p><p>生成模型发展到如今有下图中几种流派，从下面的算法结构图可以看出Diffusion model相比其他方法<strong>有比较大的不同:</strong></p><ul><li><strong>Diffusion model的算法过程中的latent Variable维度都是相同的</strong></li><li><strong>存在一个前向和反向的过程</strong></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/1.png" alt=""></p><p>从以上的不同出发，可以推测出Diffusion model包含两个过程：分别是<strong>前向过程</strong>和<strong>反向过程</strong><br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/2.png" alt=""></p><h3 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h3><p>前向过程在论文中也称为<strong>扩散过程(diffusion process)</strong>，<strong>是向数据随机添加噪声，直至原始图像整个变成随机噪声的过程</strong>，这个过程记为$x_o~q(x_o)$<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/3.png" alt=""></p><h3 id="反向过程"><a href="#反向过程" class="headerlink" title="反向过程"></a>反向过程</h3><p>反向过程是<strong>前向过程的反转</strong>，反向过程的<strong>目的是将随机噪声的分布，逐渐去噪生成真实的样本。</strong>反向过程实际上也是生成数据的过程。将该过程的表达式为:<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/4.png" alt=""></p><h2 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a><br><strong>代码链接:</strong> <a href="https://github.com/hojonathanho/diffusion">https://github.com/hojonathanho/diffusion</a></p><p>在DPM中原始的扩散模型，在反向过程中是用$x_{t-1}$ 预测 $x_{t}$，而<strong>DDPM预测的是从t时刻到t-1时刻添加的噪声</strong>，只要减去添加的噪声，同样也能得到$x_{t}$时刻的特征。</p><p>由于在从噪声恢复到目标图像的过程中，特征维度是一致的，在DDPM中采用的是U-Net的结构，在T步的反向过程中，U-Net模型是参数共享的，为了能告知U-Net模型现在是反向传播的第几步，在每一步反向传播时会增加一个<strong>time embedding</strong>，其实现和transformer中的position embedding相似</p><p>在反向过程中预测的噪声都是符合正态分布的，也就是只用拟合噪声的均值和方差就可以预测出噪声，在DDPM中将方差固定为常数，只预测均值</p><p>DDPM的原理到这里基本就介绍完了。关于论文中扩散模型正向和反向过程都是符合高斯分布的，可以做比较详尽的推理和证明，对正向和反向推理过程感兴趣的同学可以看看论文或者其他blog中的推导</p><h2 id="improved-DDPM"><a href="#improved-DDPM" class="headerlink" title="improved DDPM"></a>improved DDPM</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2102.09672">https://arxiv.org/abs/2102.09672</a><br><strong>代码链接:</strong> <a href="https://github.com/openai/improved-diffusion">https://github.com/openai/improved-diffusion</a><br>从论文的名字可以看出主要是对DDPM做的改进，<strong>主要介绍下和DDPM的差异</strong></p><ul><li>将DDPM中用常数指代的方差，用模型学习了</li><li>将添加噪声的schedule改了，从线性的改成了余弦的<br>作者发现DDPM中线性的噪声schedule在高分辨率的图像生成中表现较好，但对于分辨率比较低的，例如64<em>64和32</em>32的图像任务中表现的不那么好。特别是在扩散过程的后期，最后的几步噪声过大，对样本质量的贡献不大。从Figure 3可以看出，cosine schedule的方法在每一步添加的噪声后相比之前图片都有一些差异，而linear schedule方法，在后期几步差异已经不大了。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/5.png" alt=""><br>我也用coco的数据集试了下，确实cosine比linear要合理些。<br>下图展示的是前向过程，迭代不同次数的结果</li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/7.png" alt=""></p><p>下面以mnist数据集为例，展示反向过程，迭代不同次数的结果</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/7_2.png" alt=""></p><h2 id="Diffusion-models-beat-GAN-on-image-Synthesis"><a href="#Diffusion-models-beat-GAN-on-image-Synthesis" class="headerlink" title="Diffusion models beat GAN on image Synthesis"></a>Diffusion models beat GAN on image Synthesis</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2105.05233">https://arxiv.org/abs/2105.05233</a><br><strong>代码链接:</strong> <a href="https://github.com/openai/guided-diffusion">https://github.com/openai/guided-diffusion</a></p><p><strong>和之前方法的差异</strong></p><ul><li><strong>从GAN的实验中得到启发，对扩散模型进行了大量的消融实验，找到了更好的架构更深更宽的模型</strong></li><li><strong>用了classifier guider diffusion</strong></li></ul><h3 id="网络结构消融实验"><a href="#网络结构消融实验" class="headerlink" title="网络结构消融实验"></a>网络结构消融实验</h3><p>文中使用的基础模型是U-Net加一个单头全局注意力模块，以FID为评价指标，在ImageNet128<em>128上进行消融实验。<br>作者从模型的<strong>宽度(channels)</strong>、<strong>深度(depth)</strong>、<strong>注意力头的数量(heads)</strong>、<strong>注意力的分辨率(attention resolutions)</strong>、<em>*使用BigGAN的上/下采样激活(BigGAN-up/downsample)</em></em>、调整残差连接的权重(rescale-resblock)等方面进行了消融实验</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/9.png" alt=""></p><p>从上表中可以看到，加宽和加深网络都能带来明显的提升，增加注意力头的数量、使用多分辨率组合的注意力模块比只使用单头单一分辨率更有助于提升模型表现，BigGAN的上下采样也能提升模型表现。<strong>唯独修改残差连接的权重没有带来提升。</strong></p><p>虽然增加深度能带来模型性能的提升，但也会增加训练时间，并且需要更长时间才能拟合到一个一般结构模型能达到的效果。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/10.png" alt=""></p><p>另外通过Table 1的实验，作者使用了Channels为128，2个残差块，高分辨率的attention和使用BigGan中的上下采样激活。进一步探究注意力头的数量和每个注意力头通道数间的关系<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/11.png" alt=""></p><p>通过上表的结果进一步证明，<strong>注意力头的数量能提升性能，但每个注意力头的通道数并不是越多越好</strong></p><h3 id="Classifier-Guidance"><a href="#Classifier-Guidance" class="headerlink" title="Classifier Guidance"></a>Classifier Guidance</h3><p>作者受目前GAN方法里通常会使用的类别信息辅助图像生成的原理启发，开发了一个<strong>将类别信息引入扩散模型中的方法Classifier Guidance Diffusion</strong>，这个方法通俗的说是会训练一个图片分类器，在扩散模型的生成过程中的中间的latend code会<strong>通过分类器计算得到一个梯度，该梯度会指导扩散模型的迭代过程</strong>。其实这一操作也比较make sense，有一个分类器的存在能更好的告诉U-Net的模型在反向过程生成新图片的时候，当前图片有多像需要生成的物体。<strong>有点类似GAN中存在一个判别器的意思</strong>。</p><p>在论文中提到使用Classifier Guidance的技术<strong>能更好的生成逼真的图像</strong>，同时能加速图像生成的速度。论文中也提到，通过使用Classifier Guidance的track会牺牲掉一部分的多样性，换取图片的真实性</p><h2 id="Classifier-Free-Diffusion-Guidance"><a href="#Classifier-Free-Diffusion-Guidance" class="headerlink" title="Classifier-Free Diffusion Guidance"></a>Classifier-Free Diffusion Guidance</h2><ul><li>论文链接: <a href="https://arxiv.org/abs/2207.12598">https://arxiv.org/abs/2207.12598</a></li></ul><p>这篇论文的主要贡献是优化了Openai在《Diffusion models beat GAN on image Synthesis》中提出的Classifier Guidance，<strong>在Classifier Guidance中提出用另外一个模型做引导，需要用预训练的模型或者额外训练一个模型。不仅成本比较高而且训练的过程是不可控的。</strong></p><p>而这篇论文的方法研究的是没有分类器，也可以用生成模型自己做引导，所以起名叫“Classifier-Free Diffusion Guidance”。具体来说在该方法中联合训练了conditional和unconditional的扩散模型，并且结合了两个模型的score estimates，以实现样本质量和多样性之间的均衡。下图的Algorithms1和Algorithms2详细描述了Classifier-Free的做法。</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/12.png" alt=""></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/13.png" alt=""></p><p>最终模型的输出为下面的公式:</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/14.png" alt=""><br>最终的输出为有条件生成的输出减去无条件生成的输出，看到这里个人感觉和之前介绍的两篇关于因果分析的论文<a href="https://oysz2016.github.io/post/b4822109.html">《Unbiased Scene Graph Generation from Biased Training》</a>和<a href="https://oysz2016.github.io/post/3a13345e.html">《Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect》</a>思路十分相似，<strong>可以将无条件生成的输出看作是偏差，用正常训练的网络减去有偏差的网络能得到想要的输出</strong>。回到这篇论文的思路，有条件生成的可以看作是用了和图片匹配的文本对c，而无条件生成将其中的文本对c置为了空集。其中w是超参数，用来条件有条件和无条件生成两者的比例，实验部分有该参数对性能的详细对比，以及和其他方法的对比</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/15.png" alt=""></p><p><strong>从下面的结果图可以看出Classifier-Free Guidance相比不用non-guided的方法多样性会有些损失，但图像的真实性和色彩饱和度是要更好的</strong></p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/16.png" alt=""></p><p>值得一提的是，虽然Classifier-Free Guidance的方法没有引入新的模型，但方法本身仍然是”<strong>昂贵的</strong>“，因为训练的时候需要生成两个输出。在扩散模型本身就很慢的情况下，会进一步增加耗时</p><h2 id="GLIDE"><a href="#GLIDE" class="headerlink" title="GLIDE"></a>GLIDE</h2><p><strong>论文链接:</strong> <a href="https://arxiv.org/abs/2112.10741">https://arxiv.org/abs/2112.10741</a><br><strong>代码链接:</strong> <a href="https://github.com/openai/glide-text2im">https://github.com/openai/glide-text2im</a><br>在前面扩散模型的一系列进展之后，尤其是当guidance技术之后证明扩散模型也能生成高质量的图像后。Openai开始探索文本条件下的图像生成，并在这篇论文里对比了两种不同的guidance策略，分别是通过<strong>CLIP引导</strong>和<strong>classifier-free的引导</strong>。验证了classifier-free的方式生成的图片更真实，与提示的文本有更好的相关性。并且使用classifier-free的引导的GLIDE模型在35亿参数的情况下优于120亿参数的DALL-E模型</p><p>该方法沿袭了Openai一贯的做法，什么模块效果好就用什么，然后进一步增加模型的参数量和数据量。具体而言:</p><ul><li>使用了更大的模型，其中模型的结构和《Classifier-Free Diffusion Guidance》方法中的模型结构一样，不过增大了通道数，参数量达到了35亿</li><li>更多的数据，和Dalle相同的图像-文本对</li><li>更充分的训练，2048的batch size，迭代了250万次</li></ul><p><strong>GLIDE最大的贡献是开始用文本作为条件引导图像的生成</strong>，下图是其训练过程，和之前工作差异主要有以下几点：</p><ul><li><strong>分词后将文本送入transformer（bert），生成文本的embedding</strong></li><li><strong>文本embedding中最后一个token的特征作为扩散模型中classifier-free guidance中的条件c</strong></li></ul><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/17.gif" alt=""></p><p>GLIDE的效果确实十分惊艳，图片非常真实而且有很多细节。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/18.png" alt=""></p><h2 id="DALLE2"><a href="#DALLE2" class="headerlink" title="DALLE2"></a>DALLE2</h2><p><strong>论文链接:</strong> <a href="https://cdn.openai.com/papers/dall-e-2.pdf">https://cdn.openai.com/papers/dall-e-2.pdf</a><br><strong>代码链接:</strong> <a href="https://github.com/lucidrains/DALLE2-pytorch">https://github.com/lucidrains/DALLE2-pytorch</a><br>如果说前面所提到的方法将扩散模型优化到比同期gan模型指标还要好，让研究人员看到了扩散模型在生成领域的前景，那么Dalle2则将扩散模型引入了公众视野。</p><p>在GLIDE取得成功之后，Openai又进一步在GLIDE上加了一些track，成为了Dalle2。dalle2的结构如下图所示：</p><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/19.png" alt=""></p><p>上面的图中将有一条分割线，分割线的上半部分代表CLIP模型，下半部分代表DALLE2。<br>为了更好的理解DALLE2，回顾一下CLIP模型，CLIP模型是有一个图像-文本对，文本通过一个text encoder得到文本特征，图像通过image encoder得到图像特征。他们两者就是一对正样本，而该文本跟其他的图像就构成负样本。<br>在Dalle2中CLIP模型没有经过进一步的训练，主要用处是用来根据文本生成文本特征，然后prior根据文本特征生成对应的图像特征，<strong>这一步很有意思，在论文中作者认为显式将图像特征建模出来，再用图像特征生成图像，会比直接通过文本特征生成图像效果要好。</strong></p><p>方法:<br>dalle2使用的数据和CLIP，Dalle，GLIDE一样，都是图像文本对(x，y)。x代表图像，y代表图像对应的文本，$z_i$代表CLIP模型输出的图像特征，$z_t$代表CLIP模型输出的文本特征。则Dalle2的网络结构由两部分组成:</p><ul><li>prior: 根据文本y生成图像特征$z_i$</li><li>decoder: 使用prior生成的$z_i$(对应的文本y，y可有可无)，生成图像x</li></ul><script type="math/tex; mode=display">P(x|y)=P(x，z_i|y)=P(x|z_i，y)P(z_i|y)</script><p>作者用公式证明了可以通过两阶段方式生成图像的原因。$P(X|Y)$代表要用文本生成图像，可以等价于$P(x，z_i|y)$，因为可以认为x和图像特征$z_i$是一一对应的，可以根据链式法则等价于$P(z_i|y)$。</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder部分的模型结构和GLIDE基本一致，使用了CLIP模型作为guidance，也使用了classifier-free guidance，并且classifier-free中的guidance有两种，一种是CLIP模型，另外一种是文本。<br>使用了级联式的生成，即生成的图片先从64<em>64到256</em>256，再到1024*1024</p><h3 id="Prior"><a href="#Prior" class="headerlink" title="Prior"></a>Prior</h3><p>prior模型的任务是从文本特征生成图像特征，这里有两种常用的方法</p><ul><li>Auto-regressive</li><li>扩散模型</li></ul><p>但是自回归的模型训练效率比较低，所以DALLE2的方法中使用的是扩散模型。在prior模型里也是用到了classifier-free guidance。在模型实现上使用的是transformer的decoder，模型的输入非常多，包含文本、CLIP的text embedding、扩散模型中常见的time step的embedding，还有加过噪声之后CLIP的image embedding；输出则预测没有噪声的CLIP的image embedding。和之前扩散模型不一样的地方在于没有使用DDPM中预测噪声的方式，而是直接还原每一步的图像</p><p>最后再贴一贴惊艳的效果<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94%E4%BB%8EDDPM%E5%88%B0DALLE2/20.png" alt=""></p><h2 id="笔者总结"><a href="#笔者总结" class="headerlink" title="笔者总结"></a>笔者总结</h2><p>上面总结了各种各样的扩散模型，总体来说扩散模型的可解释性比GAN要好很多，也有很多数学公式可以证明。发展到如今，扩散模型慢慢的接替了GAN在生成领域头把交椅的地位。并且随着Dalle2的提出确实带来了无穷的想象力。在人工智能没有普及的年代，就有讨论随着人工智能的发展有哪些职业会被取代，但是基本上大家都觉得创造性的工作，是无法被取代的，因为创造性的工作没有固定的模式，大多数需要灵感。但目前来看扩散模型已经具有了一定的创造性，相信对大部分的认知都是有一定的冲击，也说明扩散模型确实是一个很有趣的研究方向。</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="扩散模型" scheme="https://oysz2016.github.io/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="生成模型" scheme="https://oysz2016.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>EDU。</title>
    <link href="https://oysz2016.github.io/post/2c662fdb.html"/>
    <id>https://oysz2016.github.io/post/2c662fdb.html</id>
    <published>2022-12-07T02:04:37.899Z</published>
    <updated>2022-12-04T02:22:04.833Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="053a8f692f6e720d6e9784b4a87b6ed7ebbc5a60433d3a26cc95a253d9d1ee8a">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">Here&#39;s something encrypted, password is required to continue reading.</summary>
    
    
    
    <category term="随笔" scheme="https://oysz2016.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="https://oysz2016.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>《VQ-VAE》论文笔记</title>
    <link href="https://oysz2016.github.io/post/d1dddfb8.html"/>
    <id>https://oysz2016.github.io/post/d1dddfb8.html</id>
    <published>2022-11-27T02:12:48.415Z</published>
    <updated>2022-12-05T01:10:19.851Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>论文链接: <a href="https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf</a></p><p>论文代码: <a href="https://github.com/ritheshkumar95/pytorch-vqvae">https://github.com/ritheshkumar95/pytorch-vqvae</a></p><p>VQ-VAE(Vector Quantised- Variational AutoEncoder)和VAE(TODO)一样也是生成模型。虽然文中作者给自己的模型取名为VQ-VAE，但实际上和VAE的关系不太太，其模型其实是基于自编码器AE</p><h1 id="Pixel-CNN"><a href="#Pixel-CNN" class="headerlink" title="Pixel CNN"></a>Pixel CNN</h1><p>既然思想是基于自编码器AE，那就得追溯到自编码生成模型的代表作Pixel CNN了，Pixcel CNN是Google Deep Mind在2016年提出的。其思想是通过前面的一些数值，得到当前数值的分布，其预测方式为:<br>$p(x)=\prod_{i=1}^{n^{2}}p(x_i|x_1,…,x_{i-1})$</p><p>每个像素是一个256分类的问题，且每个像素的分类，依赖于之前像素的信息。以cifa10数据集为例，图像大小为32<em>32</em>3，可以将图片看成序列，则长度为32<em>32</em>3=3072。生成cifa图片，需要对3072的序列按seq2seq的方式推理。<br>基于Pixel CNN的思想，由于模型用到的是CNN，为了实现在推理当前像素时能有效的遮住还未看到的信息。其提出的方式是<strong>Gate Convolutions Layers</strong>层，其思想如下图所示：<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VQ-VAE/WX20221012-200838%402x.png" alt=""><br>简言之就是对卷积做mask处理，生成一个n*n的卷积，按照从上到下，从左到右的顺序，将卷积中心及之后的特征值置为0，而其他位置置为1，保证卷积操作只能看到该像素之前的像素</p><p>Pixel CNN等自回归模型的缺点:</p><ul><li>模型耗时较长，对于分辨率稍微高点的图像，自回归模型需要逐像素推理才能还原出图像</li><li>图像的像素时很冗余的，这一思想在最近的很多论文中都有论证，如<a href="https://arxiv.org/abs/2111.06377">MAE</a>。虽然图像中每个像素时离散的，但事实上连续的像素时相近的，有时RGB值差个别数值，并不影响图像的生成，而转变成像素分类问题，只有非对即错的结果</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>VQ-VAE的论文确实写的比较难懂，而苏神的blog则要清晰非常多，关于算法原理这块推荐大家可以读读苏神的blog</p><p>针对自回归模型存在的缺点，VQ-VAE的思想是先对图像降维，再对编码向量用Pixel CNN的方式建模。这种i想具有如下的坑:</p><ul><li>因为Pixel CNN建模使用的离散的序列，就意味着VQ-VAE降维度的时候需要转换为离散的序列。其实自编码器就是很常用的降维方法之一，然而其生成的编码向量都是连续的</li><li>降维后的特征和原始特征存在差异，求梯度的时候不能用原始特征和gt比对，因为优化目标已经变成了降维后的特征。也不能直接用降维后的特征，因为VQ-VAE中的降维实际上是映射到编码表，这个过程是不能求梯度的</li></ul><h2 id="离散化"><a href="#离散化" class="headerlink" title="离散化"></a>离散化</h2><p>在VQ-VAE中，一张图片x会先经过encoder，得到连续的变量z</p><script type="math/tex; mode=display">z=encoder(x)</script><p>这里的z是一个大小为d的向量，VQ-VAE还维护一个Embedding层，我们也可以称为编码表，记为</p><script type="math/tex; mode=display">E=(e_1, e_2,..., e_k)</script><p>其中每一个$e_i$都是大小为d的向量，接着，VQ-VAE通过最邻近搜索，将z映射为这K个向量之一：</p><script type="math/tex; mode=display">z=e_k, k=argmin||e_j||_2</script><p>将z映射到编码表后的特征记为$z_q$, 则$z_q$才是编码后的结果，会将$z_q$传给decoder做生成，这样以来就将连续的特征z转变为了降维后的离散特征$z_q$<br><strong>上面的流程实际上是简化的</strong>，如果只编码一个向量，重构时容易出现失真，而且泛化性一般，因此实际编码时直接用多层卷积将x编码为m×m个大小为d的向量，也就是说，z的总大小为m×m×d，它依然保留着位置结构，然后每个向量都用前述方法映射为编码表中的一个，就得到一个同样大小的$z_q$，然后再用它来重构。这样一来，$z_q$也等价于一个m×m的整数矩阵，这就实现了离散型编码。</p><h2 id="前向和反向传播"><a href="#前向和反向传播" class="headerlink" title="前向和反向传播"></a>前向和反向传播</h2><p>如果是普通的自编码器，直接用下述loss训练即可:</p><script type="math/tex; mode=display">||x-decoder(z)||^2_2</script><p>但是$z_q$并不是原来的z, 可就算换成$z_q$也不能计算梯度，换言之，我们的目标其实是$‖x−decoder(z_q)‖_2^2$最小，但是却不好优化，而$||x-decoder(z)||^2_2$容易优化，但却不是我们的优化目标。那怎么办呢？当然，一个很粗暴的方法是两个都用：</p><script type="math/tex; mode=display">||x-decoder(z)||^2_2+‖x−decoder(z_q)‖_2^2</script><p>但这样并不好，因为decoder(z)并不是优化目标，会带来额外的约束</p><p>VQ-VAE中用了一个巧妙且直接的方法，称为Straight-Through Estimator，你也可以称之为“直通估计”。最早源于论文<a href="https://arxiv.org/abs/1308.3432">Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation</a></p><p>Straight-Through Estimator的思想很简单，就是前向传播的时候可以用想要的变量(哪怕不可导)，而反向传播的时候用你设计的梯度去替代。根据这个思想，我们设计的目标函数是：</p><script type="math/tex; mode=display">||x-decoder(z+sg[z_q-z])||^2_2</script><p>其中sg是stop gradient的意思，就是不要它的梯度。这样一来，前向传播计算（求loss）的时候，就直接等价于$decoder(z+z_q−z)=decoder(z_q)$.然后反向传播（求梯度）的时候，由于$z_q−z$不提供梯度，所以它也等价于decoder(z)，这个就允许我们对encoder进行优化了。</p><h2 id="维护编码表"><a href="#维护编码表" class="headerlink" title="维护编码表"></a>维护编码表</h2><p>上面我们提到离散化是通过编码表映射完成，期望是映射后的$z_q$和$z$相近，不然仍然会导致生成的图像失真严重，因为离散化其实是在做量化，而量化的目的是减少计算量的同时，尽量不损失精度。由于编码表E是相对自由的，而z要尽力保证重构效果，所以我们应当尽量“让$z_q$去靠近$z$”而不是“让z去靠近$z_q$”。而因为$‖z_q−z‖^2_2$的梯度等于对zq的梯度加上对z的梯度，所以我们将它等价地分解为:</p><script type="math/tex; mode=display">||sg[z]-z_q||^2_2+||z-sg[z_q]||^2_2</script><p>第一项相等于固定$z$，让$z_q$靠近$z$，第二项则反过来固定$z_q$，让$z$靠近$z_q$。注意这个“等价”是对于反向传播（求梯度）来说的，对于前向传播（求loss）它是原来的两倍。根据我们刚才的讨论，我们希望“让$z_q$去靠近$z$”多于“让$z$去靠近$z_q$”，所以可以调一下最终的loss比例：</p><script type="math/tex; mode=display">||x-decoder(z+sg[z_q-z])||^2_2+\beta||sg[z]-z_q||^2_2+\gamma||z-sg[z_q]||^2_2</script><p>其中$\gamma&lt;\beta$，在原论文中使用的是$\gamma=0.25\beta$</p><h2 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h2><p>经过上面的离散化之后，将图片编码为$m*m$的整数矩阵。该矩阵也一定程度保留了原始图片的信息，可以使用自回归模型如Pixel CNN，对编码矩阵拟合。</p><p>通过Pixel CNN得到编码分布后，可以随机生成一个新的编码矩阵，然后通过编码表E映射为浮点数$z_q$,最后经过decoder得到一张图片.</p><p>一般来说，得到的<script type="math/tex">m*m</script> 比原来的<script type="math/tex">n*n*3</script>要小的多，因此计算也更快速</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt="enter description here"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生成网络" scheme="https://oysz2016.github.io/tags/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>《VAE》论文笔记</title>
    <link href="https://oysz2016.github.io/post/63606c55.html"/>
    <id>https://oysz2016.github.io/post/63606c55.html</id>
    <published>2022-11-20T02:12:26.984Z</published>
    <updated>2022-12-05T01:10:15.363Z</updated>
    
    <content type="html"><![CDATA[<p><p class="description"></p><br><span id="more"></span></p><p>论文链接: <a href="https://arxiv.org/pdf/1312.6114.pdf">https://arxiv.org/pdf/1312.6114.pdf</a></p><p>论文代码: <a href="https://github.com/devnag/pytorch-generative-adversarial-networks">https://github.com/devnag/pytorch-generative-adversarial-networks</a></p><h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><h2 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h2><p>auto-encoder是一种无监督的算法，自编码器是一个输入和学习目标相同的神经网络，其结构分为编码器和解码器两部分。其思想是输入x经过encoder生成hidden layer特征z，再将z经过decoder重新预测生成$x^{‘}$<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VAE/v2-ec10f5dbd7197120cb87eaee5dc04e9a_b.png" alt=""></p><h2 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h2><p>VAE和GAN一样都属于生成模型，即从训练数据建模真实数据的分布，再反过来用学到的模型生成新的数据。这一类模型是假设有一个数据集$X=\{x^{(i)}\}_{i=1}^N$, 理想情况下是用$x_i$拟合函数p(x), 通过p(x)能得到$X$之外的数据，但这是理想情况，GAN和VAE采用了不同的方式达到生成模型的效果，之前GAN的论文笔记中有梳理过，GAN是一个对抗网络，通过判别器来判断生成器产生数据的效果。而VAE的方式在下面会详细介绍</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>VAE的思想是对于一个真实样本$x_k$, 假设存在后验分布$P(Z|x_k)$和$x_k$是一一对应的，$P(Z|x_k)$是一个正态分布图分布，能知道该正态分布的均值和方差，就能将其还原回X，在VAE中$P(Z|x_k)$的均值和方差是通过模型计算得到。<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VAE/WX20221010-203445@2x.png" alt=""><br>为了使模型具有生成能力，VAE希望$p(Z_x)$趋向于正态分布<br><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/VAE/WX20221010-204114@2x.png" alt=""></p><p>在VAE中实际上也有两个encoder，分别用来计算均值和方差，在encoder计算的损失中会加入KL Loss，实际上是给encoder部分增加正则项，希望encoder的均值为0.VAE的损失函数会倾向于在训练初期，会降低噪声(KL loss增加)，使得拟合更容易，而在decoder训练的不错时，会增加噪声(KL loss减少)，使得拟合更困难</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>实际上VAE中也有对抗的过程，因为decoder的部分希望没有噪声，而KL loss希望有高斯噪声，两者在训练的时候实际上是对立的。VAE和GAN两种方式实际上各有优缺点，GAN在训练时不稳定，而VAE生成的图像相对GAN会模糊些</p><blockquote class="blockquote-center"><p><strong><font size="5">欢迎关注我的公众号!</font></strong></p></blockquote><p><img data-src="https://myblog-1258449291.cos.ap-beijing.myqcloud.com/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p class=&quot;description&quot;&gt;&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="https://oysz2016.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://oysz2016.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生成网络" scheme="https://oysz2016.github.io/tags/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
