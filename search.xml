<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>如何阅读一本书</title>
      <link href="/2018/04/30/%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E4%B8%80%E6%9C%AC%E4%B9%A6/"/>
      <url>/2018/04/30/%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E4%B8%80%E6%9C%AC%E4%B9%A6/</url>
      <content type="html"><![CDATA[<p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/封面.png?imageMogr2/thumbnail/!60p" alt="enter description here"><a id="more"></a></p><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>&emsp;&emsp;趁着五一的闲暇，在家里读完了《如何阅读一本书》。这本书的初版是在1940年出版，距今有70多年。这本书很偏学术风，逻辑性很强，前两章引出了阅读的四个层次，往后的章节都是对前两章的补充。每一章节的结束基本都有对整章内容的总结。<br>&emsp;&emsp;作者在开篇提到，收音机、电视取代了以往由书本所提供的部分功能，就像照片取代了图画或艺术设计的部分功能一样。但不可否认，书籍的作用是不可取代的。相比较于这版书出版的年代，90后的一代可以称为随着互联网成长起来的一代，因此拥有而且能适应更多的资讯获取方式，各种公众号以及自媒体会将整理好的信息推送给用户。利用碎片时间获取信息很有必要，也是一个良性的发展。然而很多有用的知识，是需要潜下心来系统的去看书，才能获取的。10多年的教育使大多数人都具备了阅读的能力，并且对如何去阅读一本书有了自己的习惯。我常常觉得我的习惯不太好，为了能在有限的时间，用更优质的方法去选择和阅读一本值得读的书。因此在五一读的第一本书便是用来纠正自己阅读习惯的。下面分享一下这本书的读书笔记。</p><h2 id="阅读的活力与艺术"><a href="#阅读的活力与艺术" class="headerlink" title="阅读的活力与艺术"></a>阅读的活力与艺术</h2><p>&emsp;&emsp;阅读是一件说简单也简单，说复杂也复杂的活动。单从阅读能为我们带来什么这个角度来说，无非有这几点：理解力、娱乐和资讯。在阅读之前带着好的目标能有事半功倍的效果，</p><h3 id="主动的阅读：如何做一个自我要求的阅读者"><a href="#主动的阅读：如何做一个自我要求的阅读者" class="headerlink" title="主动的阅读：如何做一个自我要求的阅读者"></a>主动的阅读：如何做一个自我要求的阅读者</h3><ul><li>阅读是一件可以主动的事</li><li>阅读越主动，效果越好<br>&emsp;&emsp;以棒球赛为例，作者=投球手；读者=捕手。区别在于，读书不止有读的好与读的差两种状态。而更靠近读的好还是读得差，取决于阅读时多么主动，以及投入多少心思和阅读技巧。<br>阅读的时候，让自己昏昏入睡比保持清醒要容易的多。在阅读时处于哪种状态，主要取决于一个人的阅读目标。作者指出，许多人都能清楚的区分想要从书中获益还是取乐，但最后仍然无法按目标阅读。原因在于不知道怎样做一个自我要求的阅读者。在阅读一本书时想打瞌睡，并不是不想努力，而是不知道如何努力。要学会主动阅读<br><strong>主动阅读的基础</strong>：阅读时提出问题，并尝试自己解答</li><li>这本书再谈论什么</li><li>哪些部分详细描述了</li><li>这本书的观点对吗</li><li>这本书的信息跟你有什么关系</li></ul><p>&emsp;&emsp;<strong>如何让一本书真正属于自己</strong>：学会做笔记，阅读一本书像是与作者对话。而笔记能表达自己与作者之间相异或相同的观点。做笔记的方法：</p><ul><li>画底线</li><li>标记重点符号</li><li>在空白处编号，理清作者的思路</li><li>圈出关键字或句子</li><li>在书的空白处做笔记</li></ul><p>&emsp;&emsp;<strong>培养阅读习惯</strong>:习惯是第二天性，让阅读变得自然。任何原创性的东西都有规则和技巧可循，如绘画和雕塑。<br>&emsp;&emsp;<strong>从许多规则中养成一个习惯</strong>：任何技能在不熟练的时候，都显得笨手笨脚，熟悉了后能将复杂的步骤连贯起来，变得优美且和谐。</p><h3 id="阅读的目标：为了获得资讯而读，以及求得理解而读"><a href="#阅读的目标：为了获得资讯而读，以及求得理解而读" class="headerlink" title="阅读的目标：为了获得资讯而读，以及求得理解而读"></a>阅读的目标：为了获得资讯而读，以及求得理解而读</h3><p>&emsp;&emsp;作者鼓励如果想要从一本书获取到有用的东西，而不是打发时间，最好是为了提升理解力和资讯。事实上，在获取到理解力与资讯的同时，就具有了消遣的效果。<br>&emsp;&emsp;<strong>阅读这个词可以区分为两种含义：</strong></p><ul><li>阅读与自己理解力相近的读物，如报纸：能增加资讯，却不能增加理解力</li><li>阅读的书籍或听的演讲，对方水平远高于自己:理解更多的事情（<strong>知乎上关于知识的定义</strong>）</li></ul><h3 id="阅读就是学习"><a href="#阅读就是学习" class="headerlink" title="阅读就是学习"></a>阅读就是学习</h3><p>&emsp;&emsp;法国文艺复兴时期的人文主义思想家蒙田说：初学者的无知在于未学，而学者的无知在学后。后者被英国诗人亚历山大.蒲伯称为书呆子，读的广却读不通。要避免都的多就是读的好的观点，要学会选择值得阅读的书籍，在读书时思考，还要运用感觉和想象力。</p><h3 id="老师的出席与缺席"><a href="#老师的出席与缺席" class="headerlink" title="老师的出席与缺席"></a>老师的出席与缺席</h3><p>&emsp;&emsp;阅读一本书只能靠自己，更像是跟着一位缺席的老师学习，因此阅读书籍时更需要主动，也要通过各种方式运用书籍和作者交流。</p><h2 id="阅读的层次"><a href="#阅读的层次" class="headerlink" title="阅读的层次"></a>阅读的层次</h2><p>&emsp;&emsp;阅读的层次是递进的，而且的向下包含的。四种阅读层次：</p><ul><li>初级阅读（elementary reading）:能认字，这是基础，能明白每个字的意思，才能明白句子背后的含义。</li><li>检视阅读（inspectional reading）略读，在很短时间读完一本书</li><li>分析阅读（analytical reading），全盘完整的阅读，寻求理解</li><li>主题阅读(syntopical reading)，在阅读时，比较很多相关的书<br>对于中国的国情，每一个受过义务教育的人都能精通初级阅读。因此主要看作者分享的后续阅读方法。</li></ul><h2 id="检视阅读"><a href="#检视阅读" class="headerlink" title="检视阅读"></a>检视阅读</h2><p>&emsp;&emsp;精通了初级阅读才能熟练的检视阅读，检视阅读一共有两种：</p><ul><li>有系统的粗读：目标是快速发现一本书值不值得多花时间阅读。略读是以最小的时间代价了解作者为什么写这样一本书以及这本书是否对自己有用。关于略读的一些建议：1.先看书名页，看序 2.研究目录页 3.如果有索引，也要检阅下 4.出版者的介绍 5.挑几个跟主题相关的篇章看看 6.翻看几页</li><li>粗浅的阅读：并不是说草草的读完了事，而是遇到不懂的地方，记录下来，先略过。集中精神读完弄得懂的地方。享受读书的快乐，洞察全书的意义。稍后再专心研究不懂的地方。</li></ul><p>&emsp;&emsp;阅读的速度很重要，不同的书籍用不同的速度阅读。<strong>训练阅读速度的方法</strong>：眼睛一次只能读一个字或句子，但大脑能在一瞥之间掌握一个句子或段落。要学会跟着大脑的快速运转看书，而不是眼部的慢动作。训练自己快速阅读的方式：用手指着读，手移动的速度稍比自己的阅读速度快，强迫自己以更快的速度阅读。</p><h2 id="分析阅读"><a href="#分析阅读" class="headerlink" title="分析阅读"></a>分析阅读</h2><p><strong>书籍分类的重要性</strong>：在阅读前要知道自己读的是那一类书<br><strong>书名与内容的关系</strong>：阅读书名可以让读者在阅读之前，获得一些基本的资讯</p><p>&emsp;&emsp;分析阅读的规则：</p><ul><li><p>分析阅读的第一阶段：找出一本书在谈些什么</p><ul><li>依照书的种类与主题分析</li><li>使用一个单一的句子或最多几句话叙述整本书的内容。</li><li>将书中重要篇章列举出来，说明是如何按照顺序则称一个整体的架构<br>后两条规则可以帮助写作，因为写作和阅读是一体两面的事情，一个作品应该有整体感，逻辑清晰，前后连贯。</li><li>找出作者要问的问题，或作者想要解决的问题</li></ul></li><li><p>分析阅读的第二阶段：诠释一本书的内容规则</p><ul><li>诠释作者使用的关键字，与作者达成共识</li><li>从最重要的句子中抓出作者的重要主旨</li><li>找出作者的论述 ，重新架构这些论述的前因后果，以明白作者的主张</li><li>确定作者已经解决了哪些问题，还有哪些是未解决的</li></ul></li><li><p>分析阅读的第三阶段：评价一本书的规则</p><ul><li>除非已经能诠释书的架构，否则不要轻易批评</li><li>证明作者知识的不足</li><li>证明作者错误的地方</li><li>证明作者的不合逻辑</li><li>证明作者的分析与理由是不完整的</li></ul></li></ul><h2 id="主题阅读：阅读的最终目标"><a href="#主题阅读：阅读的最终目标" class="headerlink" title="主题阅读：阅读的最终目标"></a>主题阅读：阅读的最终目标</h2><p>&emsp;&emsp;主题阅读的准备阶段：</p><ul><li>针对要研究的主题，参考图书馆目录以及书中的索引。找到与主题相关的书籍</li><li>浏览整理出的书单，确定哪些与要研究的主题相关。明确各个书与主题的相关程度。</li></ul><p>&emsp;&emsp;在讨论某个主题的书时，所涉及到的往往不止是一本书。主题阅读的五个步骤：</p><ul><li>找到相关章节，关心的重点应该在具体的内容，而不是整本书</li><li>分析阅读中说与作者达成共识。但面对不同的作者描述同样的观点，会有不同的字眼。需要找出他们之间的共识。</li><li>建立一个主旨，列出一连串的问题，从书中找答案</li><li>理清主要及次要的议题。将作者针对不同各个问题的不同意见整理在各个议题旁。</li><li>将问题与议题按顺序排列，以凸显主题。</li></ul><h2 id="阅读与心智的成长："><a href="#阅读与心智的成长：" class="headerlink" title="阅读与心智的成长："></a>阅读与心智的成长：</h2><p>&emsp;&emsp;好书能带来心智的成长，提升阅读能力，能教会你了解这个世界以及自己。不仅是读的更好，还更懂得生命；变得有智慧与知识，对人类生命中永恒的真理有更深刻的认识。在读书的时候不光要会阅读，也要能分辨哪些书能给自己带来成长。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>不要被书中的难点吓到，90%的书知道个大概就好，想要了解的地方再深究；</li><li>阅读时要系统的总结，梳理文章的内容</li><li>学会做笔记，包括画重点，记录章节，页数以及自己针对某段话的思考</li><li>选择优质的书籍，不要将时间浪费在不好的书上</li><li>阅读时有以下重点：<ul><li>带着问题和目的主动的阅读</li><li>整理文章的脉络</li><li>与“书本”交流，时常问自己这本书这一章是解决什么问题，哪些观点对或者不对，值得借鉴和应该避免的错误</li><li>将书本引入自己的知识结构中</li></ul></li></ul>]]></content>
      
      <categories>
          
          <category> 读书笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Caffe版Faster R-CNN可视化——网络模型,图像特征,Loss图,PR曲线</title>
      <link href="/2018/04/27/Caffe%E7%89%88Faster%20R-CNN%E5%8F%AF%E8%A7%86%E5%8C%96%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B,%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81,Loss%E5%9B%BE,PR%E6%9B%B2%E7%BA%BF/"/>
      <url>/2018/04/27/Caffe%E7%89%88Faster%20R-CNN%E5%8F%AF%E8%A7%86%E5%8C%96%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B,%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81,Loss%E5%9B%BE,PR%E6%9B%B2%E7%BA%BF/</url>
      <content type="html"><![CDATA[<h2 id="可视化网络模型"><a href="#可视化网络模型" class="headerlink" title="可视化网络模型"></a>可视化网络模型</h2><p>&emsp;&emsp;Caffe目前有两种常用的可视化模型方式：</p><ul><li>使用Netscope在线可视化</li><li>Caffe代码包内置的draw_net.py文件可以可视化网络模型<a id="more"></a><h3 id="Netscope"><a href="#Netscope" class="headerlink" title="Netscope"></a>Netscope</h3>&emsp;&emsp;Netscope能可视化神经网络体系结构（或技术上说，Netscope能可视化任何有向无环图）。目前Netscope能可视化Caffe的prototxt 文件。网址为：<br><a href="http://ethereon.github.io/netscope/#/editor" target="_blank" rel="noopener">http://ethereon.github.io/netscope/#/editor</a><br>&emsp;&emsp;Netscope的使用非常简单，只需要将prototxt的文件复制到Netscope的编辑框，再按快捷键Shift+Enter即可得到网络模型的可视化结构。Netscope的优点是显示的网络模型简洁，而且将鼠标放在右侧可视化的网络模型的任意模块上，会显示该模块的具体参数。图1以Faster R-CNN中ZF模型的train.prototxt文件为例</li></ul><p><img src="https://user-gold-cdn.xitu.io/2018/4/27/163055f07a98ecf7?w=1913&amp;h=891&amp;f=png&amp;s=117701" alt="图1 Netscope可视化ZF网络模型"></p><div align="center">图1 Netscope可视化ZF网络模</div><h3 id="draw-net-py"><a href="#draw-net-py" class="headerlink" title="draw_net.py"></a>draw_net.py</h3><p>&emsp;&emsp;draw_net.py同样是将prototxt绘制成网络模型，在绘制之前，需要安装两个依赖库：</p><blockquote><p>1、安装ＧraphViz<br>　　# sudo apt-get install GraphViz<br>　　注意，这里用的是apt-get来安装，而不是pip.<br>2 、安装pydot<br>　　# sudo pip install pydot<br>　　用的是pip来安装，而不是apt-get</p></blockquote><p>&emsp;&emsp;安装完毕后，即可调用draw_net.py绘制网络模型，如绘制caffe自带的LeNet网络模型：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo python python/draw_net<span class="selector-class">.py</span> examples/mnist/lenet_train_test<span class="selector-class">.prototxt</span> netImage/lenet<span class="selector-class">.png</span> --rankdir=TB</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;其中有三个参数，各自的含义为：</p><blockquote><p>第一个参数：网络模型的prototxt文件<br>第二个参数：保存的图片路径及名字<br>第二个参数：–rankdir=x , x 有四种选项，分别是LR, RL, TB, BT 。用来表示网络的方向，分别是从左到右，从右到左，从上到小，从下到上。默认为ＬＲ。</p></blockquote><p>&emsp;&emsp;可视化结果如下图所示：</p><p><img src="https://user-gold-cdn.xitu.io/2018/4/27/163055f0793044ea?w=654&amp;h=1669&amp;f=png&amp;s=95256" alt="图2 draw_net.py可视化LeNet网络模型 "></p><div align="center">图2 draw_net.py可视化LeNet网络模型</div><h2 id="可视化图像特征"><a href="#可视化图像特征" class="headerlink" title="可视化图像特征"></a>可视化图像特征</h2><p>&emsp;&emsp;关于图像的可视化，我也使用过两种两种方式：</p><ul><li>修改demo.py代码输出中间层结果</li><li>使用可视化工具deep-visualization-toolbox</li></ul><h3 id="修改demo-py"><a href="#修改demo-py" class="headerlink" title="修改demo.py"></a>修改demo.py</h3><p>&emsp;&emsp;该部分是参考薛开宇的《caffe学习笔记》中的<figure class="highlight plain"><figcaption><span>VOC为例，修改demo.py文件后，代码如下：</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">``` stylus</span><br><span class="line">#!/usr/bin/env python</span><br><span class="line">#-*-coding:utf-8-*-</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import matplotlib</span><br><span class="line">matplotlib.use(&apos;Agg&apos;)</span><br><span class="line">import _init_paths</span><br><span class="line">from fast_rcnn.config import cfg</span><br><span class="line">from fast_rcnn.test import im_detect</span><br><span class="line">from fast_rcnn.nms_wrapper import nms</span><br><span class="line">from utils.timer import Timer</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import scipy.io as sio</span><br><span class="line">import caffe, os, sys, cv2</span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">CLASSES = (&apos;__background__&apos;,</span><br><span class="line">           &apos;aeroplane&apos;, &apos;bicycle&apos;, &apos;bird&apos;, &apos;boat&apos;,</span><br><span class="line">           &apos;bottle&apos;, &apos;bus&apos;, &apos;car&apos;, &apos;cat&apos;, &apos;chair&apos;,</span><br><span class="line">           &apos;cow&apos;, &apos;diningtable&apos;, &apos;dog&apos;, &apos;horse&apos;,</span><br><span class="line">           &apos;motorbike&apos;, &apos;person&apos;, &apos;pottedplant&apos;,</span><br><span class="line">           &apos;sheep&apos;, &apos;sofa&apos;, &apos;train&apos;, &apos;tvmonitor&apos;)</span><br><span class="line"></span><br><span class="line">NETS = &#123;&apos;vgg16&apos;: (&apos;VGG16&apos;,</span><br><span class="line">                  &apos;VGG16_faster_rcnn_final.caffemodel&apos;),</span><br><span class="line">        &apos;zf&apos;: (&apos;ZF&apos;,</span><br><span class="line">                  &apos;zf_faster_rcnn_iter_2000.caffemodel&apos;)&#125;</span><br><span class="line"></span><br><span class="line">def vis_detections(im, class_name, dets, thresh=0.5):</span><br><span class="line">    &quot;&quot;&quot;Draw detected bounding boxes.&quot;&quot;&quot;</span><br><span class="line">    inds = np.where(dets[:, -1] &gt;= thresh)[0]</span><br><span class="line">    if len(inds) == 0:</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">    im = im[:, :, (2, 1, 0)]</span><br><span class="line">    fig, ax = plt.subplots(figsize=(12, 12))</span><br><span class="line">    ax.imshow(im, aspect=&apos;equal&apos;)</span><br><span class="line">    for i in inds:</span><br><span class="line">        bbox = dets[i, :4]</span><br><span class="line">        score = dets[i, -1]</span><br><span class="line"></span><br><span class="line">        ax.add_patch(</span><br><span class="line">            plt.Rectangle((bbox[0], bbox[1]),</span><br><span class="line">                          bbox[2] - bbox[0],</span><br><span class="line">                          bbox[3] - bbox[1], fill=False,</span><br><span class="line">                          edgecolor=&apos;red&apos;, linewidth=3.5)</span><br><span class="line">            )</span><br><span class="line">        ax.text(bbox[0], bbox[1] - 2,</span><br><span class="line">                &apos;&#123;:s&#125; &#123;:.3f&#125;&apos;.format(class_name, score),</span><br><span class="line">                bbox=dict(facecolor=&apos;blue&apos;, alpha=0.5),</span><br><span class="line">                fontsize=14, color=&apos;white&apos;)</span><br><span class="line"></span><br><span class="line">    ax.set_title((&apos;&#123;&#125; detections with &apos;</span><br><span class="line">                  &apos;p(&#123;&#125; | box) &gt;= &#123;:.1f&#125;&apos;).format(class_name, class_name,</span><br><span class="line">                                                  thresh),</span><br><span class="line">                  fontsize=14)</span><br><span class="line">    plt.axis(&apos;off&apos;)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.draw()</span><br><span class="line"></span><br><span class="line">def demo(net, image_name):</span><br><span class="line">    &quot;&quot;&quot;Detect object classes in an image using pre-computed object proposals.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # Load the demo image</span><br><span class="line">    im_file = os.path.join(cfg.DATA_DIR, &apos;demo&apos;, image_name)</span><br><span class="line">    im = cv2.imread(im_file)</span><br><span class="line"></span><br><span class="line">    # Detect all object classes and regress object bounds</span><br><span class="line">    timer = Timer()</span><br><span class="line">    timer.tic()</span><br><span class="line">    scores, boxes = im_detect(net, im)</span><br><span class="line">    timer.toc()</span><br><span class="line">    print (&apos;Detection took &#123;:.3f&#125;s for &apos;</span><br><span class="line">           &apos;&#123;:d&#125; object proposals&apos;).format(timer.total_time, boxes.shape[0])</span><br><span class="line"></span><br><span class="line">    # Visualize detections for each class</span><br><span class="line">    CONF_THRESH = 0.8</span><br><span class="line">    NMS_THRESH = 0.3</span><br><span class="line">    for cls_ind, cls in enumerate(CLASSES[1:]):</span><br><span class="line">        cls_ind += 1 # because we skipped background</span><br><span class="line">        cls_boxes = boxes[:, 4*cls_ind:4*(cls_ind + 1)]</span><br><span class="line">        cls_scores = scores[:, cls_ind]</span><br><span class="line">        dets = np.hstack((cls_boxes,</span><br><span class="line">                          cls_scores[:, np.newaxis])).astype(np.float32)</span><br><span class="line">        keep = nms(dets, NMS_THRESH)</span><br><span class="line">        dets = dets[keep, :]</span><br><span class="line">        vis_detections(im, cls, dets, thresh=CONF_THRESH)</span><br><span class="line"></span><br><span class="line">def parse_args():</span><br><span class="line">    &quot;&quot;&quot;Parse input arguments.&quot;&quot;&quot;</span><br><span class="line">    parser = argparse.ArgumentParser(description=&apos;Faster R-CNN demo&apos;)</span><br><span class="line">    parser.add_argument(&apos;--gpu&apos;, dest=&apos;gpu_id&apos;, help=&apos;GPU device id to use [0]&apos;,</span><br><span class="line">                        default=0, type=int)</span><br><span class="line">    parser.add_argument(&apos;--cpu&apos;, dest=&apos;cpu_mode&apos;,</span><br><span class="line">                        help=&apos;Use CPU mode (overrides --gpu)&apos;,</span><br><span class="line">                        action=&apos;store_true&apos;)</span><br><span class="line">    parser.add_argument(&apos;--net&apos;, dest=&apos;demo_net&apos;, help=&apos;Network to use [zf]&apos;,</span><br><span class="line">                        choices=NETS.keys(), default=&apos;zf&apos;)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    return args</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line"></span><br><span class="line">    cfg.TEST.HAS_RPN = True  # Use RPN for proposals</span><br><span class="line"></span><br><span class="line">    args = parse_args()</span><br><span class="line">    prototxt = os.path.join(cfg.MODELS_DIR, NETS[args.demo_net][0],</span><br><span class="line">                            &apos;faster_rcnn_alt_opt&apos;, &apos;faster_rcnn_test.pt&apos;)</span><br><span class="line">    caffemodel = os.path.join(cfg.DATA_DIR, &apos;faster_rcnn_models&apos;,</span><br><span class="line">                              NETS[args.demo_net][1])</span><br><span class="line"></span><br><span class="line">    if not os.path.isfile(caffemodel):</span><br><span class="line">        raise IOError((&apos;&#123;:s&#125; not found.\nDid you run ./data/script/&apos;</span><br><span class="line">                       &apos;fetch_faster_rcnn_models.sh?&apos;).format(caffemodel))</span><br><span class="line"></span><br><span class="line">    if args.cpu_mode:</span><br><span class="line">        caffe.set_mode_cpu()</span><br><span class="line">    else:</span><br><span class="line">        caffe.set_mode_gpu()</span><br><span class="line">        caffe.set_device(args.gpu_id)</span><br><span class="line">        cfg.GPU_ID = args.gpu_id</span><br><span class="line">    net = caffe.Net(prototxt, caffemodel, caffe.TEST)</span><br><span class="line">#指定caffe路径，以下是我的caffe路径 </span><br><span class="line">    caffe_root=&apos;/home/ouyang/GitRepository/py-faster-rcnn/caffe-fast-rcnn/&apos;</span><br><span class="line">    # import sys</span><br><span class="line">    sys.path.insert(0, caffe_root+&apos;python&apos;)</span><br><span class="line">    # import caffe</span><br><span class="line"></span><br><span class="line">    # #显示的图表大小为 10,图形的插值是以最近为原则,图像颜色是灰色</span><br><span class="line">    plt.rcParams[&apos;figure.figsize&apos;] = (10, 10)</span><br><span class="line">    plt.rcParams[&apos;image.interpolation&apos;] = &apos;nearest&apos;</span><br><span class="line">    plt.rcParams[&apos;image.cmap&apos;] = &apos;gray&apos;</span><br><span class="line">    image_file = caffe_root+&apos;examples/images/vehicle_0000015.jpg&apos;  </span><br><span class="line">    # 载入模型</span><br><span class="line">    npload = caffe_root+ &apos;python/caffe/imagenet/ilsvrc_2012_mean.npy&apos;  </span><br><span class="line">    </span><br><span class="line">    transformer = caffe.io.Transformer(&#123;&apos;data&apos;: net.blobs[&apos;data&apos;].data.shape&#125;)</span><br><span class="line">    transformer.set_transpose(&apos;data&apos;, (2,0,1))</span><br><span class="line">    transformer.set_mean(&apos;data&apos;, np.load(npload).mean(1).mean(1))</span><br><span class="line">    # 参考模型的灰度为0~255，而不是0~1</span><br><span class="line">    transformer.set_raw_scale(&apos;data&apos;, 255) </span><br><span class="line">    # 由于参考模型色彩是BGR,需要将其转换为RGB</span><br><span class="line">    transformer.set_channel_swap(&apos;data&apos;, (2,1,0))</span><br><span class="line">    im=caffe.io.load_image(image_file)</span><br><span class="line">    net.blobs[&apos;data&apos;].reshape(1,3,224,224)</span><br><span class="line">    net.blobs[&apos;data&apos;].data[...] = transformer.preprocess(&apos;data&apos;,im)</span><br><span class="line">    # 显示出各层的参数和形状，第一个是批次，第二个是feature map数目，第三和第四是每个神经元中图片的长和宽</span><br><span class="line">    print [(k,v.data.shape) for k,v in net.blobs.items()]</span><br><span class="line">    #输出网络参数</span><br><span class="line">    print [(k,v[0].data.shape) for k,v in net.params.items()]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">    def show_image(im):</span><br><span class="line">        if im.ndim==3:</span><br><span class="line">            m=im[:,:,::-1]</span><br><span class="line">        plt.imshow(im)</span><br><span class="line">        #显示图片的方法</span><br><span class="line">        plt.axis(&apos;off&apos;) # 不显示坐标轴</span><br><span class="line">        plt.show()    </span><br><span class="line"></span><br><span class="line">    # 每个可视化的都是在一个由一个个网格组成</span><br><span class="line">    def vis_square(data,padsize=1,padval=0):</span><br><span class="line">        data-=data.min()</span><br><span class="line">        data/=data.max()</span><br><span class="line">        </span><br><span class="line">        # force the number of filters to be square</span><br><span class="line">        n=int(np.ceil(np.sqrt(data.shape[0])))</span><br><span class="line">        padding=((0,n**2-data.shape[0]),(0,padsize),(0,padsize))+((0,0),)*(data.ndim-3)</span><br><span class="line">        data=np.pad(data,padding,mode=&apos;constant&apos;,constant_values=(padval,padval))</span><br><span class="line">        # 对图像使用滤波器</span><br><span class="line">        </span><br><span class="line">        data=data.reshape((n,n)+data.shape[1:]).transpose((0,2,1,3)+tuple(range( 4,data.ndim+1)))</span><br><span class="line">        data=data.reshape((n*data.shape[1],n*data.shape[3])+data.shape[4:])   </span><br><span class="line">        </span><br><span class="line">        #show_image(data)</span><br><span class="line">        plt.imshow(data)</span><br><span class="line">        plt.show()</span><br><span class="line">        # 设置图片的保存路径，此处是我的路径</span><br><span class="line">        plt.savefig(&quot;./tools/Vehicle_2000/fc6.jpg&quot;)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    out = net.forward()</span><br><span class="line">    image=net.blobs[&apos;data&apos;].data[4].copy()</span><br><span class="line">    image-=image.min()</span><br><span class="line">    image/=image.max()</span><br><span class="line">    # 显示原始图像</span><br><span class="line">    show_image(image.transpose(1,2,0))</span><br><span class="line">    #网络提取conv1的卷积核</span><br><span class="line">    filters = net.params[&apos;conv1&apos;][0].data</span><br><span class="line">    vis_square(filters.transpose(0, 2, 3, 1))</span><br><span class="line">    #过滤后的输出,96 张 featuremap</span><br><span class="line">    feat =net.blobs[&apos;conv1&apos;].data[0,:96]</span><br><span class="line">    vis_square(feat,padval=1)</span><br><span class="line">    #第二个卷积层,显示全部的96个滤波器,每一个滤波器为一行。</span><br><span class="line">    filters = net.params[&apos;conv2&apos;][0].data</span><br><span class="line">    vis_square(filters[:96].reshape(96**2, 5, 5))</span><br><span class="line">    # #第二层输出 256 张 featuremap</span><br><span class="line">    feat = net.blobs[&apos;conv2&apos;].data[0]</span><br><span class="line">    vis_square(feat, padval=1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    filters = net.params[&apos;conv3&apos;][0].data</span><br><span class="line">    vis_square(filters[:256].reshape(256**2, 3, 3))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 第三个卷积层:全部 384 个 feature map</span><br><span class="line">    feat = net.blobs[&apos;conv3&apos;].data[0]</span><br><span class="line">    vis_square(feat, padval=0.5)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    #第四个卷积层,我们只显示前面 48 个滤波器,每一个滤波器为一行。</span><br><span class="line">    filters = net.params[&apos;conv4&apos;][0].data</span><br><span class="line">    vis_square(filters[:384].reshape(384**2, 3, 3))</span><br><span class="line"></span><br><span class="line">    # 第四个卷积层:全部 384 个 feature map</span><br><span class="line">    feat = net.blobs[&apos;conv4&apos;].data[0]</span><br><span class="line">    vis_square(feat, padval=0.5)</span><br><span class="line">    # 第五个卷积层:全部 256 个 feature map</span><br><span class="line">    filters = net.params[&apos;conv5&apos;][0].data</span><br><span class="line">    vis_square(filters[:384].reshape(384**2, 3, 3))</span><br><span class="line"></span><br><span class="line">    feat = net.blobs[&apos;conv5&apos;].data[0]</span><br><span class="line">    vis_square(feat, padval=0.5)</span><br><span class="line">    #第五个 pooling 层</span><br><span class="line">    feat = net.blobs[&apos;fc6&apos;].data[0]</span><br><span class="line">    vis_square(feat, padval=1)</span><br><span class="line">    第六层输出后的直方分布</span><br><span class="line">    feat=net.blobs[&apos;fc6&apos;].data[0]</span><br><span class="line">    plt.subplot(2,1,1)</span><br><span class="line">    plt.plot(feat.flat)</span><br><span class="line">    plt.subplot(2,1,2)</span><br><span class="line">    _=plt.hist(feat.flat[feat.flat&gt;0],bins=100)</span><br><span class="line">    # #显示图片的方法</span><br><span class="line">    #plt.axis(&apos;off&apos;) # 不显示坐标轴</span><br><span class="line">    plt.show()  </span><br><span class="line">    plt.savefig(&quot;fc6_zhifangtu.jpg&quot;) </span><br><span class="line">    # 第七层输出后的直方分布</span><br><span class="line">    feat=net.blobs[&apos;fc7&apos;].data[0]</span><br><span class="line">    plt.subplot(2,1,1)</span><br><span class="line">    plt.plot(feat.flat)</span><br><span class="line">    plt.subplot(2,1,2)</span><br><span class="line">    _=plt.hist(feat.flat[feat.flat&gt;0],bins=100)</span><br><span class="line">    plt.show()</span><br><span class="line">    plt.savefig(&quot;fc7_zhifangtu.jpg&quot;) </span><br><span class="line">    #看标签</span><br><span class="line">    #执行测试  </span><br><span class="line">    image_labels_filename=caffe_root+&apos;data/ilsvrc12/synset_words.txt&apos;</span><br><span class="line">    #try:</span><br><span class="line">    labels=np.loadtxt(image_labels_filename,str,delimiter=&apos;\t&apos;)</span><br><span class="line">    top_k=net.blobs[&apos;prob&apos;].data[0].flatten().argsort()[-1:-6:-1]</span><br><span class="line">    #print labels[top_k]</span><br><span class="line">    for i in np.arange(top_k.size):</span><br><span class="line">        print top_k[i], labels[top_k[i]]</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;下面贴几张检测结果</p><p><img src="https://user-gold-cdn.xitu.io/2018/4/27/163055f07a3eac7e?w=466&amp;h=357&amp;f=jpeg&amp;s=27803" alt="图3 原始检测图片"></p><div align="center">图3 原始检测图片</div><p><img src="https://user-gold-cdn.xitu.io/2018/4/27/163055f07a4a0941?w=1000&amp;h=1000&amp;f=jpeg&amp;s=168164" alt="图4 conv1参数可视化"></p><div align="center">图4 conv1参数可视化</div><p><img src="https://user-gold-cdn.xitu.io/2018/4/27/163055f0795032db?w=1000&amp;h=1000&amp;f=jpeg&amp;s=314989" alt="图5 conv1特征可视化"></p><div align="center">图5 conv1特征可视化</div><h3 id="deep-visualization-toolbox"><a href="#deep-visualization-toolbox" class="headerlink" title="deep-visualization-toolbox"></a>deep-visualization-toolbox</h3><p>&emsp;&emsp;deep-visualization-toolbox是Jason Yosinsk出版在Computer Science上的一篇论文的源代码，改论文主要讲述的是卷积神经网络的可视化，感兴趣的朋友可以看看这篇论文（<a href="https://arxiv.org/pdf/1506.06579.pdf" target="_blank" rel="noopener">论文地址</a>）。B站上有个讲怎么使用该工具的视频，这里附上链接<a href="https://www.bilibili.com/video/av7405645/" target="_blank" rel="noopener">https://www.bilibili.com/video/av7405645/</a>。<br>&emsp;&emsp;该工具的源码在github：<a href="https://github.com/yosinski/deep-visualization-toolbox" target="_blank" rel="noopener">https://github.com/yosinski/deep-visualization-toolbox</a>。该github下有完整的安装配置步骤，还是以图2中的马为例，贴几张检测结果图。</p><p><img src="https://user-gold-cdn.xitu.io/2018/4/27/163055f07aab31b9?w=1920&amp;h=1080&amp;f=png&amp;s=1096956" alt="图6 ToolBox conv1特征可视化"></p><div align="center">图6 ToolBox conv1特征可视化</div><p><img src="https://user-gold-cdn.xitu.io/2018/4/27/163055f0a7aac1ec?w=1920&amp;h=1080&amp;f=png&amp;s=1019414" alt="图7 ToolBox conv2特征可视化"></p><div align="center">图7 ToolBox conv2特征可视化</div><p>&emsp;&emsp;从检测效果上看，还是挺简洁的。图片左侧的一列图片左上角是输入图片，中间部分是图片经过网络前向传播得到的特征图可视化，左下角是其特征可视化。</p><h2 id="Loss可视化"><a href="#Loss可视化" class="headerlink" title="Loss可视化"></a>Loss可视化</h2><p>&emsp;&emsp;网络训练过程中Loss值的可视化可以帮助分析该网络模型的参数是否合适。在使用Faster R-CNN网络训练模型时，训练完成后的日志文件中保存了网络训练各个阶段的loss值，如图8所示。只用写简单的python程序，读取日志文件中的迭代次数，以及需要的损失值，再画图即可完成Loss的可视化。</p><p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/日志图片.png-chuli" alt="图8 模型的训练日志"></p><div align="center">图8 模型的训练日志</div><br>&emsp;&emsp;在下面贴出Loss可视化的代码：<br><br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python  </span><br><span class="line">import os  </span><br><span class="line">import sys  </span><br><span class="line">import numpy as np  </span><br><span class="line">import matplotlib<span class="selector-class">.pyplot</span> as plt  </span><br><span class="line">import math  </span><br><span class="line">import re  </span><br><span class="line">import pylab  </span><br><span class="line">from pylab import <span class="selector-tag">figure</span>, show, <span class="selector-tag">legend</span>  </span><br><span class="line">from mpl_toolkits<span class="selector-class">.axes_grid1</span> import host_subplot  </span><br><span class="line">  </span><br><span class="line"># 日志文件名</span><br><span class="line">fp = open(<span class="string">'faster_rcnn_end2end_ZF_.txt.2018-04-13_19-46-23'</span>, <span class="string">'r'</span>,encoding=<span class="string">'UTF-8'</span>) </span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">train_iterations = []  </span><br><span class="line">train_loss = []  </span><br><span class="line">test_iterations = []  </span><br><span class="line"><span class="selector-id">#test_accuracy</span> = []  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> ln <span class="keyword">in</span> fp:  </span><br><span class="line">  # get train_iterations and train_loss  </span><br><span class="line">  <span class="keyword">if</span> <span class="string">'] Iteration '</span> <span class="keyword">in</span> ln and <span class="string">'loss = '</span> <span class="keyword">in</span> ln:  </span><br><span class="line">    arr = re.findall(r<span class="string">'ion \b\d+\b,'</span>,ln)  </span><br><span class="line">    train_iterations.append(int(arr[<span class="number">0</span>].strip(<span class="string">','</span>)[<span class="number">4</span>:]))  </span><br><span class="line">    train_loss.append(float(ln.strip().split(' = ')[-1]))  </span><br><span class="line">      </span><br><span class="line">fp.close()  </span><br><span class="line">  </span><br><span class="line">host = host_subplot(<span class="number">111</span>)  </span><br><span class="line">plt.subplots_adjust(<span class="attribute">right</span>=<span class="number">0.8</span>) # ajust the right boundary of the plot window  </span><br><span class="line"><span class="selector-id">#par1</span> = host.twinx()  </span><br><span class="line"># set labels  </span><br><span class="line">host.set_xlabel(<span class="string">"iterations"</span>)  </span><br><span class="line">host.set_ylabel(<span class="string">"RPN loss"</span>)  </span><br><span class="line"><span class="selector-id">#par1</span>.set_ylabel(<span class="string">"validation accuracy"</span>)  </span><br><span class="line">  </span><br><span class="line"># plot curves  </span><br><span class="line">p1, = host.plot(train_iterations, train_loss, label=<span class="string">"train RPN loss"</span>)  </span><br><span class="line">.  </span><br><span class="line">host.legend(loc=<span class="number">1</span>)  </span><br><span class="line">  </span><br><span class="line"># set label color  </span><br><span class="line">host<span class="selector-class">.axis</span>[<span class="string">"left"</span>]<span class="selector-class">.label</span><span class="selector-class">.set_color</span>(p1.get_color())  </span><br><span class="line">host.set_xlim([-<span class="number">1000</span>, <span class="number">60000</span>])  </span><br><span class="line">host.set_ylim([<span class="number">0</span>., <span class="number">3.5</span>])  </span><br><span class="line">  </span><br><span class="line">plt.draw()  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><br>&emsp;&emsp;可视化效果如下图所示<br><br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/loss可视化.png-chuli" alt="图9 Loss可视化"><br><div align="center">图9 Loss可视化</div><h2 id="画PR图"><a href="#画PR图" class="headerlink" title="画PR图"></a>画PR图</h2><p>&emsp;&emsp;Faster R-CNN训练网络在输出网络模型的同级文件夹里有每一类检测目标每张图片的准确率和召回率，可以绘制准确率召回率(Precision-recall, PR)曲线，PR曲线的面积即准确率的值。<br>&emsp;&emsp;该文件存储在==output\faster_rcnn_end2end\voc_2007_test\zf_faster_rcnn_iter==下的.pkl文件下，需要将其转换为.txt文件。代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#-*-coding:utf-<span class="number">8</span>-*-</span><br><span class="line">import cPickle as pickle</span><br><span class="line">import numpy as np </span><br><span class="line">np.set_printoptions(threshold=np.NaN) </span><br><span class="line">fr = open(<span class="string">'./aeroplane_pr.pkl'</span>)    #open的参数是pkl文件的路径</span><br><span class="line">inf = pickle.load(fr)       #读取pkl文件的内容</span><br><span class="line">print inf</span><br><span class="line">fo = open(<span class="string">"aeroplane_pr.txt"</span>, <span class="string">"wb"</span>)</span><br><span class="line">fo.write(str(inf))</span><br><span class="line">fo.close()</span><br><span class="line">fr.close()                       #关闭文件</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;执行完这个程序后，会将.pkl文件转换为.txt文件保存。.txt文件能直观看到每张图片的检测准确率与召回率。用与画loss图相似的方法，即可完成PR曲线的绘制。效果图如图10所示。</p><p><img src="https://user-gold-cdn.xitu.io/2018/4/27/163055f0c90ea778?w=800&amp;h=596&amp;f=png&amp;s=20463" alt="图10 PR曲线"></p><div align="center">图10 PR曲线</div><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 薛开宇，caffe学习笔记<br>[2] Yosinski J, Clune J, Nguyen A, et al. Understanding Neural Networks Through Deep Visualization[J]. Computer Science, 2015.</p>]]></content>
      
      <categories>
          
          <category> 深度学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 卷积神经网络 </tag>
            
            <tag> Faster R-CNN </tag>
            
            <tag> 可视化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>卷积神经网络模型解读汇总——LeNet5，AlexNet、ZFNet、VGG16、GoogLeNet和ResNet</title>
      <link href="/2018/04/26/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%A7%A3%E8%AF%BB%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94LeNet5%EF%BC%8CAlexNet%E3%80%81ZFNet%E3%80%81VGG16%E3%80%81GoogLeNet%E5%92%8CResNet/"/>
      <url>/2018/04/26/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%A7%A3%E8%AF%BB%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94LeNet5%EF%BC%8CAlexNet%E3%80%81ZFNet%E3%80%81VGG16%E3%80%81GoogLeNet%E5%92%8CResNet/</url>
      <content type="html"><![CDATA[<p>&emsp;&emsp;在我的<a href="https://oysz2016.github.io/">个人博客</a>上一篇<a href="https://oysz2016.github.io/2018/04/25/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/">博文</a>中分析了卷积神经网络的结构与相关算法，知道了这些基本原理之后。这篇博文主要介绍在卷积神经网络的发展历程中一些经典的网络模型。</p><h2 id="LeNet5"><a href="#LeNet5" class="headerlink" title="LeNet5"></a>LeNet5</h2><p>&emsp;&emsp;LeCun等将BP算法应用到多层神经网络中，提出LeNet-5模型[1]（<a href="http://yann.lecun.com/exdb/lenet/index.html" target="_blank" rel="noopener">效果和paper见此处</a>），并将其用于手写数字识别，卷积神经网络才算正式提出。LeNet-5的网络模型如图1所示。网络模型具体参数如图2所示。<br><a id="more"></a><br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/LeNet5网络模型.png-chuli" alt="enter description here"></p><p><div align="center">图1 LeNet-5网络模型</div></p><p><div align="center">表1 LeNet-5具体参数</div><br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/LeNet5具体参数.png-chuli" alt="enter description here"><br><strong>输入</strong>：32*32的手写字体图片，这些手写字体包含0~9数字，也就是相当于10个类别的图片;<br><strong>输出</strong>：分类结果，0~9之间。<br>&emsp;&emsp;从输入输出可以知道，改网络解决的是一个十分类的问题，分类器使用的Softamx回归。</p><ul><li><strong>C1</strong>：卷积核参数如表所示。卷积的后的尺寸计算公式为：<br>$$outHeight=(inHeight+2pad-filterHeight)/strides[1]+1 $$<br>$$outWidth=(inWidth+2pad-filterHidth)/strides[2] +1 $$<br>&emsp;&emsp;因此，经过C1卷积层后，每个特征图大小为32-5+1=28，这一层输出的神经元个数为28<em>28</em>6=784。而这一层卷积操作的参数个数为5<em>5</em>1<em>6+6=156，其中参数个数与神经元个数无关，只与卷积核大小（此处为5</em>5），卷积核数量（此处为6，上一层图像默认深度为1）；</li><li><strong>S2</strong>：输入为28<em>28</em>6，该网络使用最大池化进行下采样，池化大小为2<em>2，经过池化操作后输出神经元个数为14</em>14*6；</li><li><strong>C3</strong>：经过C3层后，输出为10<em>10</em>16，参数个数为5<em>5</em>6*16+16=2416个参数；</li><li><strong>S4</strong>：输入为10<em>10</em>16，参数与S2层一致，池化后输出神经元个数为5<em>5</em>16；</li><li><strong>C5</strong>：经过C5层后，输出为1<em>1</em>120，参数个数为5<em>5</em>16<em>120+120=48120个参数。（这一层的卷积大小为5</em>5，图像的输入大小也为5*5，可等效为全连接层）；</li><li><strong>F6</strong>：输出为1<em>1</em>84，参数个数为1<em>1</em>120*84+84=10164<br>参数总量：60856</li></ul><p>&emsp;&emsp;从表1的具体参数可以看出，LeNet的网络结构十分简单且单一，卷积层C1、C3和C5层除了输出维数外采用的是相同的参数，池化层S2和S4采用的也是相同的参数</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>&emsp;&emsp;2012年Krizhevsky使用卷积神经网络在ILSVRC 2012图像分类大赛上夺冠，提出了AlexNet模型[2]（<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">论文地址</a>）。这篇文章凭借着诸多创新的方法，促使了之后的神经网络研究浪潮。AlexNet网络的提出对于卷积神经网络具有里程碑式的意义，相比较于LeNet5的改进有以下几点</p><ol><li>数据增强</li></ol><ul><li>水平翻转</li><li>随机裁剪、平移变换</li><li>颜色光照变换</li></ul><ol start="2"><li>Dropout： Dropout方法和数据增强一样，都是防止过拟合的。简单的说，dropout能按照一定的概率将神经元从网络中丢弃。一个很形象的解释如图2所示，左图为dropout前，右图为dropout后。dropout能在一定程度上防止网络过拟合，并且能加快网络的训练速度。<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/dropout.png-chuli" alt="enter description here"><div align="center">图2 Dropout示意图</div></li><li>ReLU激活函数：ReLu具有一些优良特性，在为网络引入非线性的同时，也能引入稀疏性。稀疏性可以选择性激活和分布式激活神经元，能学习到相对稀疏的特征，起到自动化解离的效果。此外，ReLu的导数曲线在输入大于0时，函数的导数为1，这种特性能保证在输入大于0时梯度不衰减，从而避免或抑制网络训练时的梯度消失现象，网络模型的收敛速度会相对稳定[10]。</li><li>Local Response Normalization：Local Response Normalization要硬翻译的话是局部响应归一化，简称LRN，实际就是利用临近的数据做归一化。这个策略贡献了1.2%的Top-5错误率。</li><li>Overlapping Pooling：Overlapping的意思是有重叠，即Pooling的步长比Pooling Kernel的对应边要小。这个策略贡献了0.3%的Top-5错误率。</li><li>多GPU并行：这个太重要了，入坑了后发现深度学习真是“炼丹”的学科。得益于计算机硬件的发展，在我自己训练时，Gpu大概能比Cpu快一个数量级以上。能极大的加快网络训练。<br>AlextNet的网络结构如图3所示，具体参数如表2所示。<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/AlexNet网络模型.png-chuli" alt="enter description here"><br><div align="center">图3 AlexNet网络模型</div><br><div align="center">表2 AlexNet具体参数</div><br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/AlexNet具体参数2.png-chuli" alt="enter description here"><br><strong>输入</strong>：224<em>224</em>3（RGB图像），图像会经过预处理变为227<em>227</em>3;<br><strong>输出</strong>：使用的是ImageNet数据集，该数据集有1000个类，因此输出的类别也是1000个。<br>&emsp;&emsp;从输入输出可以知道，改网络解决的是一个十分类的问题，分类器使用的Softamx回归。<ul><li><strong>conv1</strong>：输出为55<em>55</em>96，参数个数为11<em>11</em>3*96+96=34944</li><li><strong>pool1</strong>：输出为27<em>27</em>96；</li><li><strong>conv2</strong>：输出为27<em>27</em>256，参数个数为5<em>5</em>96*256+256=614656</li><li><strong>pool2</strong>：输出为13<em>13</em>256；</li><li><strong>conv3</strong>：输出为13<em>13</em>384，参数个数为3<em>3</em>256*384+384=885120</li><li><strong>conv4</strong>：输出为13<em>13</em>384，参数个数为3<em>3</em>384*384+384=1327488</li><li><strong>conv5</strong>：输出为13<em>13</em>256，参数个数为3<em>3</em>384*256+256=884992</li><li><strong>pool3</strong>：输出为6<em>6</em>256；</li><li><strong>fc6</strong>：输出为1<em>1</em>4096，参数个数为1<em>1</em>256*4096+4096=1052672</li><li><strong>fc7</strong>：输出为1<em>1</em>4096，参数个数为1<em>1</em>4096*4096+4096=16781312<br>参数总量：21581184</li></ul></li></ol><p>&emsp;&emsp;通过对比LeNet-5和AlexNet的网络结构可以看出，AlexNet具有更深的网络结构，更多的参数。</p><h2 id="ZFNet"><a href="#ZFNet" class="headerlink" title="ZFNet"></a>ZFNet</h2><p>&emsp;&emsp;ZFNet[3]（<a href="https://arxiv.org/pdf/1311.2901.pdf" target="_blank" rel="noopener">论文地址</a>）是由纽约大学的Matthew Zeiler和Rob Fergus所设计，该网络在AlexNet上进行了微小的改进，但这篇文章主要贡献在于在一定程度上解释了卷积神经网络为什么有效，以及如何提高网络的性能。该网络的贡献在于：</p><ul><li>使用了反卷积网络，可视化了特征图。通过特征图证明了浅层网络学习到了图像的边缘、颜色和纹理特征，高层网络学习到了图像的抽象特征；</li><li>根据特征可视化，提出AlexNet第一个卷积层卷积核太大，导致提取到的特征模糊；</li><li>通过几组遮挡实验，对比分析找出了图像的关键部位；</li><li>论证了更深的网络模型，具有更好的性能。</li></ul><p>&emsp;&emsp;ZFNet的网络模型如图4所示，具体参数如表3所示。<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/ZFNet网络模型.jpg-chuli" alt="enter description here"> <div align="center">图4 ZFNet网络模型</div><br>   <div align="center">表3 ZFNet具体参数</div><br> <img src="http://p7jiixmp8.bkt.clouddn.com/Blog/ZFNet具体参数.png-chuli" alt="enter description here"><br>&emsp;&emsp;ZFNet的网络模型与AlexNet十分相似，这里就不列举每一层的输入输出了。</p><h2 id="VGG16"><a href="#VGG16" class="headerlink" title="VGG16"></a>VGG16</h2><p>&emsp;&emsp;VGGNet[4]是由牛津大学计算机视觉组和Google DeepMind项目的研究员共同研发的卷积神经网络模型，包含VGG16和VGG19两种模型，其网络模型如图5所示，<a href="http://ethereon.github.io/netscope/#/gist/dc5003de6943ea5a6b8b" target="_blank" rel="noopener">也可以点击此处链接</a>查看网络模型。<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/Vgg16网络结构2.jpg-chuli" alt="enter description here"><div align="center">图5 VGG16网络模型</div><br>&emsp;&emsp;从网络模型可以看出，VGG16相比AlexNet类的模型具有较深的深度，通过反复堆叠3<em>3的卷积层和2</em>2的池化层，VGG16构建了较深层次的网络结构，整个网络的卷积核使用了一致的3<em>3的尺寸，最大池化层尺寸也一致为2</em>2。与AlexNet主要有以下不同：</p><ul><li>Vgg16有16层网络，AlexNet只有8层；</li><li>在训练和测试时使用了多尺度做数据增强。</li></ul><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p>&emsp;&emsp;GoogLeNet[5]（<a href="https://arxiv.org/pdf/1409.4842.pdf" target="_blank" rel="noopener">论文地址</a>）进一步增加了网络模型的深度和宽度，但是单纯的在VGG16的基础上增加网络的宽度深度会带来以下缺陷：</p><ul><li>过多的参数容易引起过拟合；</li><li>层数的加深，容易引起梯度消失现象。</li></ul><p>&emsp;&emsp;GoogLeNet的提出受到论文Network in Network（NIN）的启发，NIN有两个贡献：</p><ul><li>提出多层感知卷积层：使用卷积层后加上多层感知机，增强网络提取特征的能力。普通的卷积层和多层感知卷积层的结构图如图6所示，Mlpconv相当于在一般的卷积层后加了一个1*1的卷积层；<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/多层感知机.png-chuli" alt="enter description here"><div align="center">图6 普通卷积层和多层感知卷积层结构图</div></li><li>提出了全局平均池化替代全连接层，从上文计算的LeNet5，AlexNet网络各层的参数数量发现，全连接层具有大量的参数。使用全局平均池化替代全连接层，能很大程度减少参数空间，便于加深网络，还能防止过拟合。</li></ul><p>&emsp;&emsp;GoogLeNet根据Mlpconv的思想提出了Inception结构，该结构有两个版本，图7是Inception的naive版。该结构巧妙的将1<em>1、3</em>3和5*5三种卷积核和最大池化层结合起来作为一层结构。<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/Inception_naive.png-chuli" alt="enter description here"></p><p><div align="center">图7 Inception结构的naive版</div><br>&emsp;&emsp;然而Inception的naive版中5<em>5的卷积核会带来很大的计算量，因此采用了与NIN类似的结构，在原始的卷积层之后加上了1</em>1卷积层，最终版本的Inception如图8所示。</p><p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/Inception.png-chuli" alt="图8 降维后的Inception模块"></p><p><div align="center">图8 降维后的Inception模块</div><br>&emsp;&emsp;GoogLeNet的模型结构如图9所示，详细参数如表4所示。<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/GoogLeNet网络模型.jpg-chuli" alt="enter description here"></p><p><div align="center">图9 GoogLeNet模型结构</div></p><p><div align="center">表4 GoogLeNet具体参数</div><br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/GoogleNet具体参数.png-chuli" alt="enter description here"></p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>&emsp;&emsp;卷积神经网络模型的发展历程一次次证明加深网络的深度和宽度能得到更好的效果，但是后来的研究发现，网络层次较深的网络模型的效果反而会不如较浅层的网络，称为“退化”现象，如图10所示。<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/退化现象.jpg-chuli" alt="enter description here"></p><p><div align="center">图10 退化现象</div><br>&emsp;&emsp;退化现象产生的原因在于当模型的结构变得复杂时，随机梯度下降的优化变得更加困难，导致网络模型的效果反而不如浅层网络。针对这个问题，MSRA何凯明团队提出了Residual Networks<a href="[论文地址](https://arxiv.org/pdf/1512.03385.pdf">6</a>)。该网络具有Residual结构如图11所示。</p><p> <img src="http://p7jiixmp8.bkt.clouddn.com/Blog/Residual结构.png-chuli" alt="enter description here"></p><p><div align="center">图11 Residual 结构</div><br>&emsp;&emsp;ResNet的基本思想是引入了能够跳过一层或多层的“shortcut connection”，即增加一个identity mapping（恒等映射），将原始所需要学的函数H(x)转换成F(x)+x，而作者认为这两种表达的效果相同，但是优化的难度却并不相同，作者假设F(x)的优化 会比H(x)简单的多。这一想法也是源于图像处理中的残差向量编码，通过一个reformulation，将一个问题分解成多个尺度直接的残差问题，能够很好的起到优化训练的效果。<br>&emsp;&emsp;这个Residual block通过shortcut connection实现，通过shortcut将这个block的输入和输出进行一个element-wise的加叠，这个简单的加法并不会给网络增加额外的参数和计算量，同时却可以大大增加模型的训练速度、提高训练效果，并且当模型的层数加深时，这个简单的结构能够很好的解决退化问题。<br>&emsp;&emsp;首先构建了一个18层和一个34层的plain网络，即将所有层进行简单的铺叠，然后构建了一个18层和一个34层的residual网络，仅仅是在plain上插入了shortcut，而且这两个网络的参数量、计算量相同，并且和之前有很好效果的VGG-19相比，计算量要小很多。（36亿FLOPs VS 196亿FLOPs，FLOPs即每秒浮点运算次数。）这也是作者反复强调的地方，也是这个模型最大的优势所在。<br>&emsp;&emsp;模型构建好后进行实验，在plain上观测到明显的退化现象，而且ResNet上不仅没有退化，34层网络的效果反而比18层的更好，而且不仅如此，ResNet的收敛速度比plain的要快得多。<br>对于shortcut的方式，作者提出了三个策略：</p><ul><li>使用恒等映射，如果residual block的输入输出维度不一致，对增加的维度用0来填充；</li><li>在block输入输出维度一致时使用恒等映射，不一致时使用线性投影以保证维度一致；</li><li>对于所有的block均使用线性投影。<br>ResNet论文的最后探讨了阻碍网络更深的瓶颈问题，如图12所示，论文中用三个1x1,3x3,1x1的卷积层代替前面说的两个3x3卷积层，第一个1x1用来降低维度，第三个1x1用来增加维度，这样可以保证中间的3x3卷积层拥有比较小的输入输出维度。<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/更深的residual block.png-chuli" alt="enter description here"><br><div align="center">图12 更深的residual block</div><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>[1] Lecun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.<br>[2] Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems. Curran Associates Inc. 2012:1097-1105.<br>[3] Zeiler M D, Fergus R. Visualizing and Understanding Convolutional Networks[J]. 2013, 8689:818-833.<br>[4] Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.<br>[5] Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2015:1-9.<br>[6] He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]// Computer Vision and Pattern Recognition. IEEE, 2016:770-778.</li></ul>]]></content>
      
      <categories>
          
          <category> 深度学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 卷积神经网络 </tag>
            
            <tag> LeNet5 </tag>
            
            <tag> AlexNet </tag>
            
            <tag> ZFNet </tag>
            
            <tag> VGG16 </tag>
            
            <tag> GoogLeNet </tag>
            
            <tag> ResNet </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>卷积神经网络的结构与相关算法</title>
      <link href="/2018/04/25/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/"/>
      <url>/2018/04/25/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>&emsp;&emsp;早在上世纪50年代，美国神经生物学家David Hubel通过研究猫和猴子的瞳孔区域与大脑皮层神经元的对应关系就发现视觉系统的信息处理方式是分级的。这一发现，促成了神经网络在图像处理上的发展。<br>&emsp;&emsp;神经网络的发展史可以分为三个阶段，第一个阶段是Frank Rosenblatt提出的感知机模型[1]，感知机模型的逻辑简单有效，但不能处理异或等非线性问题。第二个阶段是Rumelhart等提出的反向传播算法[2]，该算法使用梯度更新权值，使多层神经网络的训练成为可能。第三个阶段得益于计算机硬件的发展和大数据时代的到来，促进了深度神经网络的发展。<a id="more"></a><br>在上一篇博客<a href="https://oysz2016.github.io/2018/04/24/%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">什么是深度学习</a>中介绍了深度学习的三种网络模型，其中卷积神经网络在图像处理上有着诸多突破性的进展。由于我对卷积神经网络较为熟悉，下面将根据神经网络的三个发展阶段阶段分析讨论卷积神经网络的发展史。</p><h2 id="传统神经网络"><a href="#传统神经网络" class="headerlink" title="传统神经网络"></a>传统神经网络</h2><h3 id="感知机与多层网络"><a href="#感知机与多层网络" class="headerlink" title="感知机与多层网络"></a>感知机与多层网络</h3><p>&emsp;&emsp;感知机（Perceptron）由两层神经元组成，是一种二分类的线性分类模型，也是最简单的单层前馈神经网络（Feedforward Neural Network）模型。感知机的提出受到生物神经元的启发，神经元在处理突触传递而来的电信号后，若产生的刺激大于一定的阈值，则神经元被激活，感知机也具有类似的结构。假设输入空间是χ⊆R^n，输出空间是y={+1,-1}，x和y分别属于两个空间，则由感知机表示的由输入空间到输出空间的函数为：f(x)=sign(w∙x+b)<br>&emsp;&emsp;其中w和b是感知机模型的参数，w∈R^n称为权重(Weight)，b∈R^n称为偏置(Bias)，w∙x表示两个向量间的内积。<br>&emsp;&emsp;Minsky和Papert已证明若决策区域类型是线性可分的，则感知机一定会学习到收敛的参数权重w和偏置b，否则感知机会发生震荡[3]（fluctuation）。因此，感知机在线性可分数据中表现良好，如果设定足够的迭代次数，能很好的处理近似线性可分的数据。但如果对非线性可分的数据，如异或问题，单层感知机不能有效的解决。由于不能用一条直线划分样本空间，有学者想到用多条直线来划分样本，多层感知机就是这样一个模型。多层感知机结构如图1所示，相比较于单层感知机，多层感知机增加了隐藏层的层数。</p><p><center><img src="http://p7jiixmp8.bkt.clouddn.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%BB%93%E6%9E%84%E5%9B%BE.jpg-chuli?imageMogr2/thumbnail/!70p" alt="多层感知机结构图"></center></p><p><div align="center">图1 多层感知机结构图</div><br>&emsp;&emsp;随着隐藏层数的增加，感知机的分类能力如表1所示。</p><p><div align="center">表1 感知机分类能力比较</div></p><p><center><img src="http://p7jiixmp8.bkt.clouddn.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%88%86%E7%B1%BB%E8%83%BD%E5%8A%9B%E6%AF%94%E8%BE%832.png-chuli" alt="enter image description here"></center><br>&emsp;&emsp;由表可知，在异或问题中，无隐层的感知机不能解决异或问题，引入了隐层后，异或问题得到解决，而随着层数越多，对于异或问题的拟合会越来越好。这说明，在感知机中随着隐藏层层数的增多，决策区域可以拟合任意的区域，因此理论上多层感知机可以解决任何线性或非线性的分类问题。但是，Minsky和Papert提出隐藏层的权重和偏置参数无法训练，这是由于隐藏层不存在期望的输出，无法通过单层感知机的训练方式训练多层感知机[4]。</p><h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p>&emsp;&emsp;如何训练多层感知机的难点在很长一段时间没有得到解决，要训练多层网络，需要更有效的学习算法。反向传播（BackPropagation，BP）算法[1]是训练多层网络的常用方法，该方法用链式法则对网络中所有权重和偏置计算损失函数的梯度，将梯度反馈给随机梯度下降或其它最优化算法，用来更新权值以最小化损失函数。在网络中，正向传播和反向传播的过程如图2所示。在这个例子中输入图片经过网络正向传播后得到的分类是狗，与实际类别的人脸不符，此时会将误差逐层反向传播，修正各个层的权重和偏置参数后，再进行正向传播，反复迭代，直至网络的参数能正确的分类输入的图片。</p><p><center><img src="http://p7jiixmp8.bkt.clouddn.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/BP%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B.jpg-chuli" alt="enter image description here"></center></p><p><div align="center">图2 BP网络训练过程</div><br>&emsp;&emsp;反向传播算法的主要步骤如下：</p><ul><li>随机初始化多层网络的权重和偏置参数，将训练数据送入多层网络的输入层，经过隐藏层和输出层，得到输出结果。完成网络的前向传播过程；</li><li>计算输出层实际值和输出值间的偏差，根据反向传播算法中的链式法则，得到每个隐藏层的误差，根据每层的误差调整各层的参数。完成网络的反向传播过程；</li><li>不断迭代前两步中的正向传播和反向传播过程，直至网络收敛。</li></ul><p>&emsp;&emsp;由于还不熟悉markdown的公式编辑，这里省去反向传播的推导过程，感兴趣的朋友可以阅读周志华教授的《机器学习》<code>第五章神经网络</code>里面有详尽的推导过程。</p><h2 id="卷积神经网络的基本思想"><a href="#卷积神经网络的基本思想" class="headerlink" title="卷积神经网络的基本思想"></a>卷积神经网络的基本思想</h2><p>&emsp;&emsp;在BP神经网络中，每一层都是全连接的，参数数量随着网络宽度和深度增加会指数级增长。多层网络结合BP算法对输入数据虽然有强大的表示能力，但巨大的参数一方面限制了每层能够容纳的最大神经元数量，另一方面也限制了神经网络的深度。受到动物视觉皮层中感受野的启发，效仿这种结构的卷积神经网络具有局部连接和参数共享的特点，可以有效的减少网络的相关参数数量，优化网络的训练速度。</p><h3 id="局部连接"><a href="#局部连接" class="headerlink" title="局部连接"></a>局部连接</h3><p>&emsp;&emsp;Hubel和Wiesel在二十世纪五十年代和六十年代的研究表明，猫和猴子的视觉皮层中的神经元只响应特定的某些区域的刺激。将这种视觉刺激影响单个神经元反应的区域称为感受野（receptive field），相邻神经元细胞具有相同或相似的感受野[5]。正是由于发现了感受野等功能在猫的视觉神经中枢中的作用，催生了日本学者福岛邦彦提出带卷积和下采样层的多层卷积神经网络[6-8]。<br>&emsp;&emsp;当我们在处理一副图像时，其输入往往是高维的。传统的神经网络将下一层神经元连接到上一层所有神经元。这种方式随着网络层数的增加，参数数量会爆炸式增加，在实际运用中，会无法训练网络。卷积神经网络中采取的做法是将每个神经元连接到上一层的部分神经元。这种连接的空间范围是一个超参数，称为神经元的感受野，感受野实际上是神经元映射到输入图像矩阵空间的大小。<br>&emsp;&emsp;局部连接的实现方式是引入卷积层，通过卷积层对应局部的图像，每一层的神经元组合在一起对应图像的全局信息。如图3所示，在网络的第m层，每个神经元感受野大小为3，能连接到上一层的3个神经元。m+1层与m层类似。随着层数增加，神经元相对于输入层的感受野会越来越大。每个神经元不会响应感受野以外神经元的变化。受启发于动物的视觉神经元只响应局部信息，这样的结构确保了卷积神经网络只响应上一层局部神经元的变化，起到过滤作用的同时，减少了网络参数。而且随着层数的增加，这种过滤作用会越来越全局。</p><p><center><img src="http://p7jiixmp8.bkt.clouddn.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E5%B1%80%E9%83%A8%E8%BF%9E%E6%8E%A5.png-chuli?imageMogr2/thumbnail/!50p" alt="局部连接"></center></p><p><div align="center">图3 局部连接</div></p><h3 id="权值共享"><a href="#权值共享" class="headerlink" title="权值共享"></a>权值共享</h3><p>&emsp;&emsp;卷积的优点除了局部连接外还有权值共享。如图4所示，假设第m-1层有5个神经元，m层有3个神经元，对第m-1层的特征进行卷积，得到第m层共有3个单元的输出特征图。虽然第m层每个神经元都与第m-1层中的3个神经元连接，但同一组卷积操作的权重参数相同。在这个例子中，通过权值共享，将9个参数较少到了3个。</p><p><center><img src="http://p7jiixmp8.bkt.clouddn.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E6%9D%83%E5%80%BC%E5%85%B1%E4%BA%AB.png-chuli?imageMogr2/thumbnail/!70p" alt="权值共享"></center></p><p><div align="center">图4 权值共享</div><br>&emsp;&emsp;卷积神经网络中权值共享的实现方式是让同一个卷积核去卷积整张图像，生成一整张特征图[9]。在卷积操作中，同一个卷积核内，所有神经元共享相同权值，权值共享的策略可以很大程度上降低网络需要计算的参数数量。通过权值共享，不仅大大增加了参数的训练效率，而且提取的特征在一定程度上具有位置不变性，加强了特征对输入图像的表达能力。</p><h2 id="卷积神经网络结构"><a href="#卷积神经网络结构" class="headerlink" title="卷积神经网络结构"></a>卷积神经网络结构</h2><p>&emsp;&emsp;卷积神经网络是一种层次模型（Hierarchical Model），其输入是RGB图像，视频，音频等数据。卷积神经网络通过一系列卷积（Convolution）操作，非线性激活函数（Non-linear Activation Function），池化（Pooling）操作层层堆叠，逐层从原始数据获取高层语义信息[10]。<br>如图5所示，在结构上，卷积神经网络分类器有四种类型的网络层：卷积层、池化层、全连接层和分类器。各层次之间的有如下约束：<br>（1）多个卷积（C）和池化（S）层，将上一层的输出图像与本层权重W做卷积得到各个C层，然后经过下采样得到S层。<br>（2）全连接层：全连接层的输入是最后一个卷积池化层的输出，其输出是一个N维的列向量，维度对应类别的个数。<br>（3）分类器：p_1，p_2，p_n的具体数值代表输入图像属于各类别的概率，分类器根据提取到的特征向量将检测目标划分到合适的类中。</p><p><center><img src="http://p7jiixmp8.bkt.clouddn.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%86%E7%B1%BB%E5%99%A8.png-chuli" alt="卷积神经网络分类器"></center></p><p><div align="center">图5 卷积神经网络分类器</div></p><h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p>&emsp;&emsp;图片有着固有的特性，这意味着，图像的一部分特征与其他部分相似，对一张图片学习到的一部分特征可以用于其他部分。卷积操作受启发于这种特性，具体操作如图6所示，输入图片大小为5×5，经过卷积核大小为3×3的卷积后，原来的输入空间映射到3×3的区域。再经过一次相同大小的卷积核后，图片大小变为了1×1。可见，卷积层逐层提取特征的方式，能从庞大的像素矩阵中，提取到对图像更有代表性的特征。卷积层最重要的是卷积核的设计，卷积核有几个参数:大小、步长、数目、边界填充。这些参数会对卷积的效果带来很大的影响。若卷积核设计的较大，如AlexNet[11]中使用的11×11和5×5的卷积核，其感受野很大，能覆盖图像更大的区域，对图像的“抽象”能力会较好，但较大的卷积核也会带来参数过多的负面影响。卷积核的步长指卷积每次滑动的距离，在一定程度上影响了特征提取的好坏。每一层网络的多个卷积核保证了提取到的特征是图像的多个方面，但卷积核的数量也不是越多越好，过多的卷积核会增加参数数量，计算复杂的同时容易过拟合。边界填充可用于卷积核与图像尺寸不匹配时，填充图像缺失区域。</p><p><center><img src="http://p7jiixmp8.bkt.clouddn.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E5%8D%B7%E7%A7%AF.png-chuli" alt="卷积示意图"></center></p><p><div align="center">图6 卷积示意图</div></p><h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p>&emsp;&emsp;卷积后的特征依然十分巨大，不仅带来计算性能的下降，也会产生过拟合。于是产生了对一块区域特征进行聚合统计的想法.例如，可以计算图像在某一块区域内的最大值或平均值代替这一块区域的特征，在降低特征维度的同时能使提取到的特征更具有代表性，还会使得处理过后的特征图谱拥有更大的感受野，这种用部分特征代替整体特征的操作称为池化[10]（Pooling）。常用的池化方法如下：最大值池化（Max Pooling）；均值池化（Mean Pooling）；随机池化（Random Pooling）。池化操作具有以下优良特性：<br>（1）平移不变性（Translation Invariant）。无论是哪种池化方式，提取的都是局部特征。池化操作会模糊特征的具体位置，图像发生了平移后，依然能产生相同的特征。<br>（2）特征降维（Feature Dimension Reduction），池化操作将一个局部区域的特征进一步抽象，池化中的一个元素对应输入数据中的一个区域，可以减少参数数量，降低维度。<br>&emsp;&emsp;池化操作的功能是减小特征空间的大小，以减小网络中的参数和计算量，从而避免过度拟合。如图7所示，224×224×64经过大小为2×2，步长为2的池化核，变成了112×112×64，使得特征图谱减少为原来的1/2。图7中池化方式是最大池化，即将一个区域内的最大值表示为这个区域的池化结果。</p><p><center><img src="http://p7jiixmp8.bkt.clouddn.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/%E6%B1%A0%E5%8C%96%E7%A4%BA%E6%84%8F%E5%9B%BE.png-chuli" alt="池化示意图"></center></p><p><div align="center">图7 池化示意图</div></p><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>&emsp;&emsp;前面讨论的卷积层，池化层等操作是将原始数据映射到特征空间，使得到的特征矩阵越来越抽象并对特征有良好的表达能力。Softmax分类器要求输入是列向量，需要全连接层将卷积和池化的输出映射到线性可分空间。<br>全连接层可以聚合卷积和池化操作得到的高阶特征，并且可以简化参数模型，一定程度的减少神经元数量和训练参数。为了能用反向传播算法训练网络，全连接层要求图片有固定的输入尺寸。因此早期网络中，需要对不同尺寸的图片进行裁剪或拉伸，这种操作会带来图片信息的失真和损失。在第三章讨论的感兴趣（Region of Interest）池化方法，可以很好的解决这一问题。<br>&emsp;&emsp;卷积层是由全连接层发展而来，全连接层可以用特殊的卷积层表示，对于前一层全连接的全连接层可以用卷积核大小为1×1的卷积层替代，而对于前一层是卷积的全连接层可以用对上一层所有输入全局卷积的卷积层替代。在全连接层中可以认为每个神经元的感受野是整个图像。全连接层隐藏层节点数越多，模型拟合能力越强，但参数冗余会带来过拟合的风险而且会降低效率。对于这个问题，一般的做法是采用正则化（Regularization）技术，如L1、L2范式。还有通过Dropout随机舍弃一些神经元，来减少权重连接，然后增强网络模型在缺失个体连接信息情况下的鲁棒性[10]。</p><h3 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h3><p>&emsp;&emsp;经过全连接层将特征映射到线性空间后，最后还需要将实例数据划分到合适的分类中。分类器有多种，常用的有支持向量机和Softmax回归，此处以Softmax为例子。Softmax函数用于将多个神经元的输出映射到(0,1)之间，转化为概率问题，从而处理多分类问题。如图8所示，Softmax层的输入分别是3、1和-3，在经过Softmax层后分别映射为0.85、0.12和0.03，三个值的累加和为1，其数值可以理解为概率，则属于y_1类的概率最大为0.85。这幅图是Softmax的通俗理解，具体推导过程可以参考<a href="http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92" target="_blank" rel="noopener">这篇文章</a>。</p><p><center><img src="http://p7jiixmp8.bkt.clouddn.com/image/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/softmax%E5%B1%82.png-chuli" alt="Softmax层"></center></p><p><div align="center">图8 Softmax层</div></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]    Rumelhart D E, Hinton G E, Williams R J. Learning representations by back-propagating errors [J]. Nature, 1986, 323(6088): 533-536.<br>[2]    Hinton G E, Osindero S, Teh Y W. A fast learning algorithm for deep belief nets [J]. Neural Computation, 2006, 18(7): 1527-1554.<br>[3]    Minsky M, Papert S. Perceptrons: An introduction to computational geometry [J]. 1969, 75(3): 3356-3362.<br>[4]    Minsky M L, Papert S A. Perceptrons (expanded edition) mit press [J]. 1988.<br>[5]    刘建立, 沈菁, 王蕾, 等. 织物纹理的简单视神经细胞感受野的选择特性 [J]. 计算机工程与应用, 2014, 50(1): 185-190.<br>[6]    Fukushima K. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position [J]. Biological Cybernetics, 1980, 36(4): 193-202.<br>[7]    Fukushima K. Neocognitron: A hierarchical neural network capable of visual pattern recognition [J]. Neural Networks, 1988, 1(2): 119-130.<br>[8]    Fukushima K, Miyake S. Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position [J]. Pattern Recognition, 1982, 15(6): 455-469.<br>[9]    曹婷. 一种基于改进卷积神经网络的目标识别方法研究 [D]. 湖南大学, 2016.<br>[10]    LeCun Y, Boser B, Denker J S, et al. Backpropagation applied to handwritten zip code recognition [J]. Neural computation, 1989, 1(4): 541-551.<br>[11]    Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks [C]. Proceedings of 26th Annual Conference on Neural Information Processing Systems, Nevada:NIPS, 2012: 1097-1105.</p>]]></content>
      
      <categories>
          
          <category> 深度学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 卷积神经网络 </tag>
            
            <tag> BP算法 </tag>
            
            <tag> 感知机 </tag>
            
            <tag> 卷积 </tag>
            
            <tag> 池化 </tag>
            
            <tag> 全连接 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>什么是深度学习</title>
      <link href="/2018/04/24/%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
      <url>/2018/04/24/%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</url>
      <content type="html"><![CDATA[<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p>&emsp;&emsp;比起深度学习，“机器学习”一词更耳熟能详。机器学习是人工智能的一个分支，它致力于研究如何通过计算的手段，利用经验来改善计算机系统自身的性能。通过从经验中获取知识，机器学习算法摒弃了人为向机器输入知识的操作，转而凭借算法自身来学到所需所有知识。对于传统机器学习算法而言，“经验”往往对应以“特征”形式存储的“数据”，传统机器学习算法所做的事情便是依靠这些数据产生“模型”。<a id="more"></a><br>&emsp;&emsp;特征的意义是找一个更好的空间去重构表达数据，把原始数据映射到高维空间，更便于划分不同类的数据。特征的选取是机器学习的核心，通常线性可分的数据用最简单的感知机即可划分，而现实应用中的数据往往是高维复杂的，传统的特征提取的方式可以归纳为以下几种：</p><ul><li><strong>依据经验人工挑选</strong>：如关于天气的数据集，如果是预测是否下雨，可以挑选与降雨密切相关的特征：季节、紫外线指数、温度、湿度、是否有云、风向和风速等属性。</li><li><strong>线性特征选择</strong>：假设特征之间相互独立，不存在交互，那么可以使用卡方检验、信息增益、互信息等方法逐个检验特征与结果之间的相关程度。更为简便的方法是使用LR等线性模型，先做一次预训练，根据特征对应的线性模型权值的绝对值大小来对特征的重要程度进行排序。</li><li><strong>非线性特征选择</strong>：如果属性之间不是相互独立，可以使用随机森林来进行特征选择，概括来说就是将想要检验重要性的特征在样本上进行permutation，然后观察OOB错误的上升程度，上升越大，说明这个特征越重要。</li></ul><p>&emsp;&emsp;以上介绍的都是传统的特征提取方式，而随着机器学习任务的复杂多变，现有的特征提取方法表现出了诸多弊端，针对一个数据集设计特征提取方法不仅费时费力，而且还十分敏感，换成其他的任务，表现往往不尽人意。得益于计算机硬件的发展和大数据时代的到来，计算机拥有了能处理大量数据的前提和能力，促进了深度学习的发展。</p><h2 id="深度学习的实质"><a href="#深度学习的实质" class="headerlink" title="深度学习的实质"></a>深度学习的实质</h2><p>&emsp;&emsp;深度学习以原始数据作为输入，经过算法层层的将数据抽象为自身任务所需要的最终特征表示。通过大量的数据逐层学习特征，免去了传统特征提取过程中人类先验知识的影响。通过数据自主的学习特征，以获取输入信息更本质的特征[1, 2]。<br>&emsp;&emsp;深度学习的实质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。因此，“深度模型”是手段，“特征学习”是目的。区别于传统的浅层学习，深度学习的不同在于：</p><ul><li><strong>强调了模型结构的深度</strong>，通常有5层、6层，甚至10多层的隐层节点；</li><li><strong>明确突出了特征学习的重要性</strong>，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。</li><li>与人工规则构造特征的方法相比，<strong>利用大数据来学习特征</strong>，更能够刻画数据的丰富内在信息。</li></ul><h2 id="深度学习的网络模型"><a href="#深度学习的网络模型" class="headerlink" title="深度学习的网络模型"></a>深度学习的网络模型</h2><p>&emsp;&emsp;相比较于传统的机器学习算法，深度学习除了模型学习，还有特征学习、特征抽象等任务模块的参与，借助多层任务模块完成最终学习任务，故称为“深度”学习。深度学习发展到如今已经有了多种结构的深度神经网络模型，如：</p><ul><li>由多个受限玻尔兹曼机组成的<strong>深度信念网络（Deep Belief Network，DBN）</strong>[3];</li><li>应用于自然语言处理的<strong>循环神经网络（Recurrent Neural Network，RNN）</strong>[4];</li><li>具有局部连接和权值共享等优点的<strong>卷积神经网络（Convolutional Neural Network，CNN）</strong>[5]。<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>[1] Bottou L, Chapelle O, Decoste D, et al. Scaling learning algorithms towards AI[J]. Large-scale kernel machines,2007,34(5): 321-359.<br>[2] Bengio Y, Delalleau O. On the expressive power of deep architectures [C]. Proceedings of International Conference on Algorithmic Learning Theory, Springer-Verlag, 2011: 18-36.<br>[3] Mikolov T, Karafiát M, Burget L, et al. Recurrent neural network based language model [C]. Proceedings of 11th Annual Conference of the International Speech Communication Association, Chiba: Interspeech, 2010: 1045-1048.<br>[4] LeCun Y, Boser B, Denker J S, et al. Backpropagation applied to handwritten zip code recognition [J]. Neural computation, 1989, 1(4): 541-551.</li></ul>]]></content>
      
      <categories>
          
          <category> 深度学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 特征提取 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>基于Github和Hexo的个人博客搭建</title>
      <link href="/2018/04/23/%E5%9F%BA%E4%BA%8EGithub%E5%92%8CHexo%E7%9A%84%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
      <url>/2018/04/23/%E5%9F%BA%E4%BA%8EGithub%E5%92%8CHexo%E7%9A%84%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
      <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>&emsp;&emsp;去年起看了很多大牛的博客，也萌生了搭建个人博客的想法，为什么搭建博客，总结下来有以下好处：</p><ul><li><strong>书写是为了更好的思考</strong></li><li><strong>激励自己持续学习</strong></li><li><strong>尝试持之以恒的去做一些事情</strong><a id="more"></a></li></ul><p>&emsp;&emsp;这次趁着大论文盲审结果还没出来和导师还没反馈小论文修改意见的间隙，花了一个周末的时间搭建好了自己的个人博客。在此将搭建过程作为自己的第一篇博客记录下来。</p><h2 id="Hexo简介"><a href="#Hexo简介" class="headerlink" title="Hexo简介"></a>Hexo简介</h2><p><img src="http://p7jiixmp8.bkt.clouddn.com/1524460444%281%29.jpg-chuli" alt="enter image description here"><br>&emsp;&emsp;Hexo 是一个基于 Node.js 的静态博客程序，可以方便的生成静态网页托管在github和Heroku上。Hexo有着丰富的主题，可以定制多种样式。<br>hexo特性：</p><ul><li>速度快：Hexo基于Node.js，支持多进程，几百篇文章也可以秒生成；</li><li>撰写工具丰富：支持GitHub Flavored Markdown和所有Octopress的插件；</li><li>扩展性强： Hexo支持EJS、Swig和Stylus。通过插件支持Haml、Jade和Less。</li></ul><p>&emsp;&emsp;使用hexo时，有以下常用命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo -g #安装</span><br><span class="line">npm install hexo -g #安装  </span><br><span class="line">npm update hexo -g #升级  </span><br><span class="line">hexo init #初始化</span><br><span class="line">hexo n &quot;我的博客&quot; == hexo new &quot;我的博客&quot; #新建文章</span><br><span class="line">hexo p == hexo publish</span><br><span class="line">hexo g == hexo generate#生成</span><br><span class="line">hexo s == hexo server #启动服务预览</span><br><span class="line">hexo d == hexo deploy#部署</span><br><span class="line">hexo new 创建文章</span><br></pre></td></tr></table></figure></p><h2 id="环境搭建-node-hexo-git"><a href="#环境搭建-node-hexo-git" class="headerlink" title="环境搭建(node,hexo,git)"></a>环境搭建(node,hexo,git)</h2><p>&emsp;&emsp;前面说过hexo是基于node.js，因此需要先安装<a href="https://nodejs.org/en/" target="_blank" rel="noopener">node.js</a>。git的配置这里就不再赘述，可以参考廖雪峰老师的<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/" target="_blank" rel="noopener">Git教程</a>。安装好Git后，需要将其与自己的GitHub账号关联上。安装好node.js后，仅需一步即可安装hexo的相关套件。在命令行输入:</p><pre><code>npm install hexo -g hexo-cli</code></pre><p>&emsp;&emsp;到这一步就安装好了所需的所用环境。</p><h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>&emsp;&emsp;在搭建自己的博客前，需要设置一个博客的根目录。使用命令行切换到该根目录下，输入：</p><pre><code>hexo init blog</code></pre><p>&emsp;&emsp;等待片刻，成功后会提示<code>INFO  Start blogging with Hexo!</code>初始化成功后，目录如下：</p><pre><code>.├── _config.yml├── package.json├── scaffolds├── source|   ├── _drafts|   └── _posts└── themes</code></pre><p>&emsp;&emsp;source的_posts目录下会自带一篇题为“Hello World”的示例文章，直接执行以下操作可以看到网站初步的模样:</p><pre><code>$ hexo generate# 启动本地服务器$ hexo server# 在浏览器输入 http://localhost:4000/就可以看见网页和模板了,若端口号被占用，可输入hexo server -p 4001改为其他端口号。</code></pre><p>&emsp;&emsp;访问<a href="http://localhost:4000/，界面如下：" target="_blank" rel="noopener">http://localhost:4000/，界面如下：</a><br><img src="http://p7jiixmp8.bkt.clouddn.com/%E5%8D%9A%E5%AE%A2%E5%88%9D%E5%A7%8B%E8%A7%86%E5%9B%BE.png-chuli" alt="enter image description here"></p><h2 id="部署及配置博客"><a href="#部署及配置博客" class="headerlink" title="部署及配置博客"></a>部署及配置博客</h2><h3 id="配置SSH"><a href="#配置SSH" class="headerlink" title="配置SSH"></a>配置SSH</h3><p>&emsp;&emsp;在上一步看到了网站的默认效果，此时需要将该博客部署到Github上，登陆Github，创建名为your_name.github.io(your_name替换成你的用户名)的仓库。重新打开CMD,输入：</p><pre><code>ssh-keygen -t rsa -C &quot;Github的注册邮箱地址&quot;</code></pre><p>&emsp;&emsp;一路Enter，得到信息：<code>Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub.</code>根据保存的路径找到id_rsa.pub文件，用编辑器打开，复制所有内容，然后<a href="https://github.com/login?return_to=https://github.com/settings/ssh" target="_blank" rel="noopener">Sign in to GitHub</a>，按以下步骤配置SSH：<br>&emsp;&emsp;New SSH key ——&gt;Title：blog ——&gt; Key：输入刚才复制的—— &gt;Add SSH key</p><h3 id="配置博客"><a href="#配置博客" class="headerlink" title="配置博客"></a>配置博客</h3><p>&emsp;&emsp;在blog目录下，用编辑器打开_config.yml，修改其中的配置信息。<br>&emsp;&emsp;修改网站中的相关信息 ：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">title: #标题</span><br><span class="line">subtitle: #副标题</span><br><span class="line">description: #站点描述</span><br><span class="line">author: #作者</span><br><span class="line">language: zh-Hans</span><br><span class="line">email:  #电子邮箱</span><br><span class="line">timezone: Asia/Shanghai</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;配置部署仓库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy: </span><br><span class="line">  type: git</span><br><span class="line">  repo: 刚刚github创库地址.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;特别提醒，在每个参数的：后都要加一个空格。以上操作完成后，执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo clean #清除缓存 网页正常情况下可以忽略此条命令</span><br><span class="line">hexo generate #生成</span><br><span class="line">hexo server #启动服务预览，非必要，可本地浏览网页</span><br><span class="line">hexo deploy #部署发布</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;得到提示信息<code>INFO  Deploy done: git</code>表示成功发布到Github上。然后在浏览器里输入your_name.github.io就可以访问刚刚配置好的博客了。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>&emsp;&emsp;到此为止，最基本的hexo+github搭建博客完结。hexo有许多优美简洁的主题，网上也有许多关于主题美化的教程，可以根据自己的喜好添加各种或实用或酷炫的功能。</p>]]></content>
      
      <categories>
          
          <category> Hexo博客搭建教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> Github </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
