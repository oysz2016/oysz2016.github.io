<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[《VQ-VAE》论文笔记]]></title>
    <url>%2Fpost%2Fd1dddfb8.html</url>
    <content type="text"><![CDATA[论文链接: https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf 论文代码: https://github.com/ritheshkumar95/pytorch-vqvae VQ-VAE(Vector Quantised- Variational AutoEncoder)和VAE(TODO)一样也是生成模型。虽然文中作者给自己的模型取名为VQ-VAE，但实际上和VAE的关系不太太，其模型其实是基于自编码器AE Pixel CNN既然思想是基于自编码器AE，那就得追溯到自编码生成模型的代表作Pixel CNN了，Pixcel CNN是Google Deep Mind在2016年提出的。其思想是通过前面的一些数值，得到当前数值的分布，其预测方式为:$p(x)=\prod_{i=1}^{n^{2}}p(x_i|x_1,…,x_{i-1})$ 每个像素是一个256分类的问题，且每个像素的分类，依赖于之前像素的信息。以cifa10数据集为例，图像大小为32323，可以将图片看成序列，则长度为32323=3072。生成cifa图片，需要对3072的序列按seq2seq的方式推理。基于Pixel CNN的思想，由于模型用到的是CNN，为了实现在推理当前像素时能有效的遮住还未看到的信息。其提出的方式是Gate Convolutions Layers层，其思想如下图所示：简言之就是对卷积做mask处理，生成一个n*n的卷积，按照从上到下，从左到右的顺序，将卷积中心及之后的特征值置为0，而其他位置置为1，保证卷积操作只能看到该像素之前的像素 Pixel CNN等自回归模型的缺点: 模型耗时较长，对于分辨率稍微高点的图像，自回归模型需要逐像素推理才能还原出图像 图像的像素时很冗余的，这一思想在最近的很多论文中都有论证，如MAE。虽然图像中每个像素时离散的，但事实上连续的像素时相近的，有时RGB值差个别数值，并不影响图像的生成，而转变成像素分类问题，只有非对即错的结果 MethodVQ-VAE的论文确实写的比较难懂，而苏神的blog则要清晰非常多，关于算法原理这块推荐大家可以读读苏神的blog 针对自回归模型存在的缺点，VQ-VAE的思想是先对图像降维，再对编码向量用Pixel CNN的方式建模。这种i想具有如下的坑: 因为Pixel CNN建模使用的离散的序列，就意味着VQ-VAE降维度的时候需要转换为离散的序列。其实自编码器就是很常用的降维方法之一，然而其生成的编码向量都是连续的 降维后的特征和原始特征存在差异，求梯度的时候不能用原始特征和gt比对，因为优化目标已经变成了降维后的特征。也不能直接用降维后的特征，因为VQ-VAE中的降维实际上是映射到编码表，这个过程是不能求梯度的 离散化在VQ-VAE中，一张图片x会先经过encoder，得到连续的变量z z=encoder(x)这里的z是一个大小为d的向量，VQ-VAE还维护一个Embedding层，我们也可以称为编码表，记为 E=(e_1, e_2,..., e_k)其中每一个$e_i$都是大小为d的向量，接着，VQ-VAE通过最邻近搜索，将z映射为这K个向量之一： z=e_k, k=argmin||e_j||_2将z映射到编码表后的特征记为$z_q$, 则$z_q$才是编码后的结果，会将$z_q$传给decoder做生成，这样以来就将连续的特征z转变为了降维后的离散特征$z_q$上面的流程实际上是简化的，如果只编码一个向量，重构时容易出现失真，而且泛化性一般，因此实际编码时直接用多层卷积将x编码为m×m个大小为d的向量，也就是说，z的总大小为m×m×d，它依然保留着位置结构，然后每个向量都用前述方法映射为编码表中的一个，就得到一个同样大小的$z_q$，然后再用它来重构。这样一来，$z_q$也等价于一个m×m的整数矩阵，这就实现了离散型编码。 前向和反向传播如果是普通的自编码器，直接用下述loss训练即可: ||x-decoder(z)||^2_2但是$z_q$并不是原来的z, 可就算换成$z_q$也不能计算梯度，换言之，我们的目标其实是$‖x−decoder(z_q)‖_2^2$最小，但是却不好优化，而$||x-decoder(z)||^2_2$容易优化，但却不是我们的优化目标。那怎么办呢？当然，一个很粗暴的方法是两个都用： ||x-decoder(z)||^2_2+‖x−decoder(z_q)‖_2^2但这样并不好，因为decoder(z)并不是优化目标，会带来额外的约束 VQ-VAE中用了一个巧妙且直接的方法，称为Straight-Through Estimator，你也可以称之为“直通估计”。最早源于论文Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation Straight-Through Estimator的思想很简单，就是前向传播的时候可以用想要的变量(哪怕不可导)，而反向传播的时候用你设计的梯度去替代。根据这个思想，我们设计的目标函数是： ||x-decoder(z+sg[z_q-z])||^2_2其中sg是stop gradient的意思，就是不要它的梯度。这样一来，前向传播计算（求loss）的时候，就直接等价于$decoder(z+z_q−z)=decoder(z_q)$.然后反向传播（求梯度）的时候，由于$z_q−z$不提供梯度，所以它也等价于decoder(z)，这个就允许我们对encoder进行优化了。 维护编码表上面我们提到离散化是通过编码表映射完成，期望是映射后的$z_q$和$z$相近，不然仍然会导致生成的图像失真严重，因为离散化其实是在做量化，而量化的目的是减少计算量的同时，尽量不损失精度。由于编码表E是相对自由的，而z要尽力保证重构效果，所以我们应当尽量“让$z_q$去靠近$z$”而不是“让z去靠近$z_q$”。而因为$‖z_q−z‖^2_2$的梯度等于对zq的梯度加上对z的梯度，所以我们将它等价地分解为: ||sg[z]-z_q||^2_2+||z-sg[z_q]||^2_2第一项相等于固定$z$，让$z_q$靠近$z$，第二项则反过来固定$z_q$，让$z$靠近$z_q$。注意这个“等价”是对于反向传播（求梯度）来说的，对于前向传播（求loss）它是原来的两倍。根据我们刚才的讨论，我们希望“让$z_q$去靠近$z$”多于“让$z$去靠近$z_q$”，所以可以调一下最终的loss比例： ||x-decoder(z+sg[z_q-z])||^2_2+\beta||sg[z]-z_q||^2_2+\gamma||z-sg[z_q]||^2_2其中$\gamma&lt;\beta$，在原论文中使用的是$\gamma=0.25\beta$ 生成经过上面的离散化之后，将图片编码为$m*m$的整数矩阵。该矩阵也一定程度保留了原始图片的信息，可以使用自回归模型如Pixel CNN，对编码矩阵拟合。 通过Pixel CNN得到编码分布后，可以随机生成一个新的编码矩阵，然后通过编码表E映射为浮点数$z_q$,最后经过decoder得到一张图片. 一般来说，得到的m*m 比原来的n*n*3要小的多，因此计算也更快速]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>生成网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《VAE》论文笔记]]></title>
    <url>%2Fpost%2F63606c55.html</url>
    <content type="text"><![CDATA[论文链接: https://arxiv.org/pdf/1312.6114.pdf 论文代码: https://github.com/devnag/pytorch-generative-adversarial-networks 背景知识Auto-encoderauto-encoder是一种无监督的算法，自编码器是一个输入和学习目标相同的神经网络，其结构分为编码器和解码器两部分。其思想是输入x经过encoder生成hidden layer特征z，再将z经过decoder重新预测生成$x^{‘}$ 生成模型VAE和GAN一样都属于生成模型，即从训练数据建模真实数据的分布，再反过来用学到的模型生成新的数据。这一类模型是假设有一个数据集$X=\{x^{(i)}\}_{i=1}^N$, 理想情况下是用$x_i$拟合函数p(x), 通过p(x)能得到$X$之外的数据，但这是理想情况，GAN和VAE采用了不同的方式达到生成模型的效果，之前GAN的论文笔记中有梳理过，GAN是一个对抗网络，通过判别器来判断生成器产生数据的效果。而VAE的方式在下面会详细介绍 MethodVAE的思想是对于一个真实样本$x_k$, 假设存在后验分布$P(Z|x_k)$和$x_k$是一一对应的，$P(Z|x_k)$是一个正态分布图分布，能知道该正态分布的均值和方差，就能将其还原回X，在VAE中$P(Z|x_k)$的均值和方差是通过模型计算得到。为了使模型具有生成能力，VAE希望$p(Z_x)$趋向于正态分布 在VAE中实际上也有两个encoder，分别用来计算均值和方差，在encoder计算的损失中会加入KL Loss，实际上是给encoder部分增加正则项，希望encoder的均值为0.VAE的损失函数会倾向于在训练初期，会降低噪声(KL loss增加)，使得拟合更容易，而在decoder训练的不错时，会增加噪声(KL loss减少)，使得拟合更困难 结论实际上VAE中也有对抗的过程，因为decoder的部分希望没有噪声，而KL loss希望有高斯噪声，两者在训练的时候实际上是对立的。VAE和GAN两种方式实际上各有优缺点，GAN在训练时不稳定，而VAE生成的图像相对GAN会模糊些]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>生成网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《NER&RE联合抽取》论文笔记]]></title>
    <url>%2Fpost%2Ff45dd794.html</url>
    <content type="text"><![CDATA[背景: NER任务: 预测文本中具有特定意义的实体，如人名，地名等 RE任务: 多分类任务，通过NER得到了实体之后，预测任意两个实体存在怎样的关系 实体关系抽取可以分为两类方法: pipeline models:可以任意组合不同的模型和数据，但关系模型使用实体模型的的预测结果作为其输入，会导致实体预测的错误传播到关系预测模型中 joint models:在一个模型中同时完成实体和关系抽取的任务,增强实体和关系的信息交互 关系抽取需要考虑的问题: SEO: (Single Entity Overlap)，一个实体与多个其他实体有关系 EPO(Entity Pair Overlap): 两个实体之间有多个关系 SOO(Subject Object Overlap)/HTO)(Head Tail Overlap): subject和object存在嵌套的情形 CsRel:A Novel Cascade Binary Tagging Framework for Relational Triple Extraction论文: https://arxiv.org/pdf/1909.03227.pdf代码: https://github.com/weizhepei/CasRelCasRel: 对关系三元组联合建模 通过下面的公式，可以将句子中含有(s,r,o)三元组的最大似然估计转换为先提取s后，在关系r的前提下，提取对应的o Cascade: 先抽取subject, 再抽取对应的关系和object。整个网络分成两步: 先抽取subject，再抽取对应的关系和object 对于每一个关系，都要做对应关系的object抽取，如果有N个关系，则有2N个序列 subject tagger 采用两个单独的二分类器分别检测subject实体的开始和结束，具体做法是经过bert后输出两个序列，start序列将实体的头token标记为1，end序列将实体的尾token标记为1。如果一个句子中检测出多个实体，则采用nearest的原则解码 relation-specific object taggers将subject的信息作为先验信息，带入到object和关系的抽取中。由于每个span的宽度不同，为了保证x和v的维度一致，需要将subject做max pooling TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking论文: https://arxiv.org/pdf/2010.13415.pdf代码: https://github.com/131250208/TPlinker-joint-extraction 之前工作存在的问题 曝光偏差(exposure bias): 在训练时，grouth truth token作为上下文，训练object和关系。而在预测时，用预测的subject作为输入，导致了训练和测试时的偏差 误差传播: 错误不可逆有的传播，如果subject未抽取到，则object和关系也将预测不出来 TPLinker通过三种类型的span矩阵抽取实体关系三元组，N是序列长度，R是关系的总数 EH-to-ET: 表示实体的头尾关系，1个N*N的矩阵。如两个实体：New York City:M(New, City) =1; De Blasio:M(De, Blasio) =1，在上图中紫色标记。 SH-to-OH: 表示subject和object的头部token间的关系，是R个N*N矩阵；如三元组(New York City, mayor,De Blasio):M(New, De)=1，在上图中位红色标记。 ST-to-OT: 表示subject和object的尾部token间的关系，是R个N*N的矩阵；如三元组(New York City, mayor,De Blasio):M(City, Blasio)=1，在上图中为蓝色标记。 共有2R+1个矩阵，为了防止稀疏计算，下三角矩阵不参与计算。实体标注不会产生下三角矩阵，但关系可能会存在，若关系矩阵存在于下三角，则将其转置到上三角，并由标记1转换为标记2 解码过程 解码EH-to-ET可以得到句子中所有的实体，用实体头token idx作为key，实体作为value，存入字典D中；得到三个实体{New York,New York City,De Blasio};则D={New:(New York,New York City),De:(De Blasio)} 对每种关系r，解码SH-to-OH得到token对并在D中关联其token idx的实体value；以关系mayor为例，解码SH-to-OH得到(De,New)，关联到D可以知道subject实体为(De Blasio), object实体为(New York, New York City) 解码ST-to-OT得到E=(City,Blasio), 关联上面的到的subject和object集合可以确认subject为(De Blasio),object为New York City PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction论文: https://arxiv.org/pdf/2106.09895.pdf代码: https://github.com/hy-struggle/PRGCCsRel缺点: 每个subject需要判断大量冗余的关系 每次只能处理一个subject，工程效率不太友好 TPLinker缺点: 构建了大量的关系矩阵，导致标签稀疏和收敛速度慢 同样存在关系冗余 将实体和关系抽取建模成三个子任务: 关系判断(Relation Judgement): 输入为句子的特征向量h, 输出是长度为r的向量。判断句子中可能存在的关系。 实体提取(Entity Extraction): 输入是句子的特征向量h和可能存在的关系$R_{pot}$。 对于每个候选关系，采用softmax进行两次三分类，第一次确定头实体的BIO标签，第二次确定尾实体的BIO标签。在抽取subject和object时，用到了关系的特征。将关系向量加到对应token向量上，经过全连接层、softmax得到分类概率。 主宾语对齐(Subject-object Alignment):输入是句子的特征向量。构建二维矩阵$M\in R^{n*n},M_{i,j}$存储的是subject实体首词为第i个token，object实体首词为第j个token的概率。由于通过实体提取获取了句子中在每个关系$r_{i}$相应的实体，则只要subject和object实体的首词能匹配上，则对应的实体也能匹配上 OneRel: Joint Entity and Relation Extraction with One Module in One Step论文: https://arxiv.org/pdf/2203.05412代码: https://github.com/ssnvxia/OneRelTPLinker存在的问题: 在构建实体和关系时，引入了1+2R个矩阵，即一个矩阵用来抽取关系，2R个矩阵抽取subject和object实体对头之间的关系，R个矩阵抽取是梯队尾之间的关系。存在较多冗余的信息，而且忽略了三元组中关系实体、头实体和尾实体相互的关系 提出单模块，单步解码的实体关系联合抽取方法 Multi-Module Multi-Step: 实体和关系分别建模，串行多步解码，会存在误差传递 Multi‐Module One‐Step: 实体和关系分别建模，单步解码，最后组装成三元组。存在冗余计算 One‐Module One‐Step: 用单个模块直接建模头实体，关系，尾实体使用token-pair的方式，用4个标记类型建模三元组 HB-TB：头实体的开始token与尾实体的开始token 进行连接。 HB-TE：头实体的开始token与尾实体的结束token 进行连接。 HE-TE：头实体的结束token与尾实体的结束token 进行连接。 -：不存在连接关系。 相比TPLinker构建的矩阵数量由1+2R个，减少到R个在(New York State, Contains, New York City)的三元组中, 关系M_r=contains的矩阵中。解码时通过通过HB-TE,HE-TE可以得到头实体New York State，HB-TB, HB-TE可以得到尾实体New York City UniRE: A Unified Label Space for Entity Relation Extraction论文: https://arxiv.org/abs/2107.04292代码: https://github.com/Receiling/UniRE 之前联合学习的方法仍然使用各自的标签空间，并没有将标签空间联合起来 将sequence labeling调整为关系抽取任务，将NER和RE两个任务放在同一个标签空间进行处理 引入词对关系表，将实体和关系通过该表完整的表示出来（PER：人名实体，GPE：地理位置实体，PER-SOC: 社会关系，ORG-AFF：机构附属关系， PHYS：位置临近关系 正向关系：表的上三角部分。人名实体 David Perkins 对地理位置实体 California 存在位置临近关系 PHYS ，那 David 对California，Perkins 对 California 都具有 PHYS 关系； 逆向关系：表的下三角部分。人名实体 doctors 对地理位置实体 village 存在隶属关系 ORG-AFF ，那 doctors 对 village具有 ORG-AFF 关系； 无向关系：和表对角线对称的。两个人名实体 David Perkins 和 wife 之间存在社会关系 PER-SOC ，这被分解成两个对称关系，David Perkins 对 wife 的正向关系和 wife 对 David Perkins 的逆向关系。实体也可以看做无向的关系，例如，David Perkins 是一个人名实体，那 David 对 David ，David 对Perkins，Perkins 对 David ，Perkins 对 Perkins 都具有标签为 PER 的关系。 模型的训练转换为预测任意两个单词之间的关系，采用双仿射变换 损失函数为交叉墒损失 对该建模的一些约束 对称性: 对称关系具有如下性质，(e_1,e_2,l)和(e_2,e_1,l)是等价的。因此满足该条件的两个关系关于对象线是对称的。实体和无向关系关于对角线是对称的。因此，这些标签对应的概率分数应该关于对角线对称。将标签分为y分为对称标签y_{sym}(实体和无向关系)和非对称标签y_{asym}。对于对称标签，具有如下约束: 蕴含性: 给定一个关系，则参与关系的必定是两个实体。相反，给定两个实体，他们两个之间不一定存在关系。则关系的概率分布应该不大于参与该关系的两个实体的概率分布 Max为hinge loss。最终的loss]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>NLP</tag>
        <tag>NER</tag>
        <tag>RE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[长尾优化汇总]]></title>
    <url>%2Fpost%2F8592ee55.html</url>
    <content type="text"><![CDATA[背景经典的深度学习数据集Mnist: 数据规模较小，10个类别ImageNet: 百万数据量，1000个类别两者的共同点：类别是均匀分布的 真实场景中的深度学习任务 类别不平衡是常态长尾问题长尾问题常见的表现及原因 ： 头部类的效果好，尾部类的效果差 模型是数据驱动的，头部类的数据多，尾部类的数据少 尾部类数量少的同时可能造成类别中样本差异较大，网络学习不充分ResamplingBBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition论文链接：https://arxiv.org/pdf/1912.02413.pdftwo stage finetuning: 第一阶段在原始不平衡的数据集上训练 第二阶段以一个很小的学习率使用resampling/reweighting的方法fintune根据two stage fintuning方法比只使用resampling/reweighting好的原因做了假设 reblance的方法有效的原因在于提升了分类器的性能 会损害网络学习到的特征为了验证上述假设，将网络拆解成了分类器和特征提取器两部分，分别实验: 在第一阶段使用交叉熵和resampling/reweighting训练整个网络 将第一阶段特征提取器的参数固定，使用交叉熵和resampling/reweighting训练分类器论文链接：https://arxiv.org/pdf/1912.02413.pdftwo stage finetuning: 第一阶段在原始不平衡的数据集上训练 第二阶段以一个很小的学习率使用resampling/reweighting的方法fintune根据two stage fintuning方法比只使用resampling/reweighting好的原因做了假设 reblance的方法有效的原因在于提升了分类器的性能 会损害网络学习到的特征为了验证上述假设，将网络拆解成了分类器和特征提取器两部分，分别实验: 在第一阶段使用交叉熵和resampling/reweighting训练整个网络 将第一阶段特征提取器的参数固定，使用交叉熵和resampling/reweighting训练分类器 设计了一个双分支的网络论文链接：https://arxiv.org/pdf/1912.02413.pdftwo stage finetuning: 第一阶段在原始不平衡的数据集上训练 第二阶段以一个很小的学习率使用resampling/reweighting的方法fintune根据two stage fintuning方法比只使用resampling/reweighting好的原因做了假设 reblance的方法有效的原因在于提升了分类器的性能 会损害网络学习到的特征为了验证上述假设，将网络拆解成了分类器和特征提取器两部分，分别实验: 在第一阶段使用交叉熵和resampling/reweighting训练整个网络 将第一阶段特征提取器的参数固定，使用交叉熵和resampling/reweighting训练分类器 第一个采样器是一个公平的采样器 第二个采样器是一个resample的采样器 两个分支共享权重，减少参数量的同时，让第二个分支受益于第一个分支中更好的特征 使用一个adaptor的策略，调节两个分支在网络训练中的权重Cost-sensitive learningClass-Balanced Loss Based on Effective Number of Samples链接:https://arxiv.org/pdf/1901.05555.pdf之前方法存在的问题：在reweighting等方法中，一般将样本数量的倒数作为该类别的权重，但是样本之间能提供的信息可能存在重合，简单通过样本数量判断权重会存在问题 提出了一种计算有效样本的方法 用有效样本数代替原始的样本频率，再用其倒数对损失进行加权有效样本的定义: 样本的有效数据量是样本的期望体积，一个新的采样数据只能存在两种情况 新样本存在于之前样本中的概率为p 新样本不存在于之前样本中的概率为1-p提出有效样本的计算公式:E_{n}=\frac{1-\beta^{n}}{1-\beta}当n=1时，E_1=\frac{1-\beta^1}{1-\beta}=1假设当n=k-1时成立,即E_{k-1}=\frac{1-\beta^{k-1}}{1-\beta}，设样本的体积为K，已经采样的样本体积为E_{k-1}。则p=\frac{E_{k-1}}{K},经过k次采样后，第k次采样时有以下情况: 第k次采样和之前样本存在重叠的情况,则样本体积为E_{k-1} 第k次采样是新的有效样本，与之前不存在重叠的情况,则样本体积为E_{k-1}+1期望体积为:E_k=pE_{k-1}+(1-p)(E_{k-1}+1)=1+\frac{K-1}{K}E_{k-1}其中\beta=\frac{K-1}{K}$$, 则$$E_k=1+\beta E_{k-1}=1+\beta \frac{1-\beta^{k-1}}{1-\beta}=\frac{1-\beta^{k}}{1-\beta}则有效数据量是数据总量n的指数函数，超参数\beta\in[0,1)有效数据量E_n具有如下性质: 当n很大时，有效数据量等于n 当n为1时，有效数据量为1CB-Loss:CB(p,y)=\frac{1}{E_{n_y}}L(p,y)=\frac{1-\beta}{1-\beta_{n_y}}L(p,y) Transfer LearningFeature Space Augmentation for Long-Tailed Data论文链接：https://arxiv.org/abs/2008.03673常见的解决方法，如data manipulation和Balanced loss function design在提升长尾数据集模型性能的同时会损害特征表示能力 两个假设: 头部类中类别无关的特征可以让尾部类特征更加丰富 高级特征空间具有更“线性”的表示，可以提取类通用和类特定的特征，并重新混合生成新的样本方法:CAM(Class Activation Map)M_c(x,y)=\sum_{k}w_k^cf_k(x,y)c: class; x,y: pixel position; k: channel; w: weight; f: feature，将M归一化到[0,1]，设定两个阈值。0]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>长尾优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Unbiased Scene Graph Generation from Biased Training》论文笔记]]></title>
    <url>%2Fpost%2Fb4822109.html</url>
    <content type="text"><![CDATA[任务定义场景图生成: 描述目标检测的物体之间，具有怎样的关系 之前算法存在的问题 数据集中关系词存在严重的偏见，原因有以下几点: 标注时,倾向于简单的关系 日常生活中确实有些事物的关联性比较多 语法习惯的问题 往往通过union box和两个物体的类别就预测了两个物体的关系，几乎没有使用visual feature，也就预测不出有意义的关系 Methods 为了让模型预测更有意义的关系，用了一个causal inference中的概念，即Total Direct Effect（TDE）来取代单纯的网络log-likelihood。在预测关系时更关注visual feature. 提出了新的评测方法mR@K:把所有谓语类别的Recall单独计算，然后求均值，这样所有类别就一样重要了 Biased Training Models in Causal Graph 节点I: Input Image&amp;Backbone.Backbone部分使用Faster rcnn预训练好的模型，并frozen bockbone的参数。输出检测目标的bounding boxes和图像的特征图.Link I-&gt;X:目标的特征，通过ROI Align提取目标对应的特征R={r_i}，获取目标粗略的分类结果L={l_i}.和MOTIFS和VCTree一样，使用以下方式，编码视觉上下文特征. MOTIFS中使用双向LSTM，VCTree中使用双向TreeLSTM，早期工作如VTransE中使用全连接层 节点X：目标特征。获取一组目标的特征x_e=(x_i,x_j)Link X-Z: 获取对应目标fine-tuned的类别，从x_i解码: 节点Z:目标类别，one-hot的向量Link X-&gt;Y: SGG的目标特征输入 Link:Z-&gt;Y：SGG的目标类别输入Link:I-&gt;Y:SGG的视觉特征输入节点Y:输出关系词汇Training loss：使用交叉熵损失预测label，为了避免预测Y只使用单一输入的信息，尤其是只使用Z的信息，进一步 使用auxiliary cross-entropy losses, 让每一个分支分别预测y Unbiased Prediction by Causal Effects机器学习中常见的解决长尾问题的方法: 数据增强/重新采样 对数据平衡改进的loss 从无偏见中分离出有偏见的部分与上述方法的区别是不需要额外训练或层来建模偏差，通过构建两种因果图将原有模型和偏差分离开。 Origin&amp;Intervention&amp;Counterfactual ntervention:清除因果图中某个节点的输入，并将其置为某个值，公式为:$do(X= \tilde x)$.某节点被干预后，需要该节点输入的其他节点也会受影响 Counterfactual:让某个节点被干预后，其他需要输入的节点还假设该节点未被干预总结: Counterfactual图实际上抹除了因果图像中object feature。只用image+object label预测两个目标间的关系。 Total Direct Effect (TDE)根据两个因果图: 原始因果图 Counterfactual因果图(可以认为是偏见)消除偏见:TDE=Y_x(u)-Y_{\tilde x,z}(u) Experiments Ablation Studies对比了几种常用的优化长尾问题的方法 Focal Reweight Resample X2Y: 直接通过X的输出预测Y X2Y-Tr：切断其他分支的联系，只使用X预测Y TE:TE=Y_x(u)-Y_{\tilde x}(u) NIE:NIE=TDE-TE TDE]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>场景图生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《GAN》论文笔记]]></title>
    <url>%2Fpost%2Ff84aad11.html</url>
    <content type="text"><![CDATA[论文链接: https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf 论文代码: https://github.com/devnag/pytorch-generative-adversarial-networks 顾名思义, GAN是一个对抗网络。具体来说，会有一个生成器(Generator)，和一个判别器(Discirminator)，两个模型互相对抗，共同进步。举一个生活中的例子，例如造假币的人是这里的生成器，查假币的人是判别器，那么生成器的任务就是让造出的假币让判别器以为是真钱一样，而判断器的任务就是能很好的分别出生成器造出的钱是假币。 生成器输入一个n维的noise的向量z，一般是随机产生的，例如满足高斯分布/均值分布的噪声，生成器的任务是将向量z生成图片x，可以通过MLP等神经网络 判别器输入是图片数据，输出是一个标量，用于判断输入数据是来自真实数据还是生成器 训练方式通过上面网络的简介知道，需要训练两个网络，所以训练方式目的也是让两个网络都能够有效的学习 上式中D是判别器，G是生成器 先看前一项，$E_{x \sim p_{data}}[logD(x)]$表示从真实数据$p_{data}$中采样样本x，让判别器判断x的来源。在辨别器判断正确的情况下$D_x=1$, 则$logD(x)=0$ 后面一项，$E_{z \sim p_{z}(z)}[log(1-D(G(z)))]$表示从随机噪声数据$p_{z}(z)$中生成噪声z, 先让生成器z生成类似x的数据$G(z)$,再让判别器判断生成的数据来源，在判别器完美的时候，能分出$G(z)$不是真实样本，则$D(G(z))=0$ 在D犯错的时候，$log(D(x)$为负数，因此需要最大化D的损失, 而$G(z)$需要生成的和x尽可能相似，需要最小化G的损失 整个模型设计用的是博弈论的思想，min和max都需要优化，且互相对抗。感兴趣可以看看minmax算法相关的知识:https://en.wikipedia.org/wiki/Minimax。在博弈论中，在包含两个或两个以上参与者的非合作博弈中，如果每个参与者都选择了对自己最有利的策略，则最后会达到均衡点，称为纳什均衡(Nash equilibrium) 网络的优化过程在图1中有形象化的示例，图1中，蓝色的线是判别器，黑色是真实数据，绿色是生成器 在图1(a)中，生成器生成的数据和真实数据相差比较大，但判别器判断的也不是太好 图1(b)更新了判别器，能很好的区分是否是生成器生成的数据 图1(c)中更新了生成器，生成的数据和真实数据变得很相似 图1(d)中随着生成器和辨别器都进行优化，最终生成器拟合的数据和真实数据基本一致，而判别器也无法判断数据来源，则模型优化完毕 代码逻辑如下所示: 第一个for循环是迭代的次数 然后会将判别器循环更新k次，再将生成器更新1次 其中k是一个超参数，如果判别器更新的太好，生成器就没法玩了，而要是更新的太差，生成器就没有动力继续优化 算法的原理到这里基本上就整理完了，论文中第四部分章节有关于损失函数正确的理论证明，感兴趣也可以阅读下 总结 gan算法实际上训练完后主要使用的是生成器，在方法中其实对生成器已经有了优化，所以为什么还要引入判别器呢？其实光从生成器的优化上，很难通过损失判断生成器的好坏，而引入判别器可以更全局的判断生成器的效果，也通过博弈的方法让生成器能达到更好的效果title: 《GAN》论文笔记author: 冲弱email: ouyang-sz@foxmail.comtimezone: Asia/Shanghaicategories: 论文笔记tags: 深度学习 生成网络copyright: truemathjax: truephotos: https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/GAN/WX20221006-161541@2x.pngabbrlink: f84aad11 论文链接: https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf 论文代码: https://github.com/devnag/pytorch-generative-adversarial-networks 顾名思义, GAN是一个对抗网络。具体来说，会有一个生成器(Generator)，和一个判别器(Discirminator)，两个模型互相对抗，共同进步。举一个生活中的例子，例如造假币的人是这里的生成器，查假币的人是判别器，那么生成器的任务就是让造出的假币让判别器以为是真钱一样，而判断器的任务就是能很好的分别出生成器造出的钱是假币。 生成器输入一个n维的noise的向量z，一般是随机产生的，例如满足高斯分布/均值分布的噪声，生成器的任务是将向量z生成图片x，可以通过MLP等神经网络 判别器输入是图片数据，输出是一个标量，用于判断输入数据是来自真实数据还是生成器 训练方式通过上面网络的简介知道，需要训练两个网络，所以训练方式目的也是让两个网络都能够有效的学习 上式中D是判别器，G是生成器 先看前一项，$E_{x \sim p_{data}}[logD(x)]$表示从真实数据$p_{data}$中采样样本x，让判别器判断x的来源。在辨别器判断正确的情况下$D_x=1$, 则$logD(x)=0$ 后面一项，$E_{z \sim p_{z}(z)}[log(1-D(G(z)))]$表示从随机噪声数据$p_{z}(z)$中生成噪声z, 先让生成器z生成类似x的数据$G(z)$,再让判别器判断生成的数据来源，在判别器完美的时候，能分出$G(z)$不是真实样本，则$D(G(z))=0$ 在D犯错的时候，$log(D(x)$为负数，因此需要最大化D的损失, 而$G(z)$需要生成的和x尽可能相似，需要最小化G的损失 整个模型设计用的是博弈论的思想，min和max都需要优化，且互相对抗。感兴趣可以看看minmax算法相关的知识:https://en.wikipedia.org/wiki/Minimax。在博弈论中，在包含两个或两个以上参与者的非合作博弈中，如果每个参与者都选择了对自己最有利的策略，则最后会达到均衡点，称为纳什均衡(Nash equilibrium) 网络的优化过程在图1中有形象化的示例，图1中，蓝色的线是判别器，黑色是真实数据，绿色是生成器 在图1(a)中，生成器生成的数据和真实数据相差比较大，但判别器判断的也不是太好 图1(b)更新了判别器，能很好的区分是否是生成器生成的数据 图1(c)中更新了生成器，生成的数据和真实数据变得很相似 图1(d)中随着生成器和辨别器都进行优化，最终生成器拟合的数据和真实数据基本一致，而判别器也无法判断数据来源，则模型优化完毕 代码逻辑如下所示: 第一个for循环是迭代的次数 然后会将判别器循环更新k次，再将生成器更新1次 其中k是一个超参数，如果判别器更新的太好，生成器就没法玩了，而要是更新的太差，生成器就没有动力继续优化 算法的原理到这里基本上就整理完了，论文中第四部分章节有关于损失函数正确的理论证明，感兴趣也可以阅读下 总结gan算法实际上训练完后主要使用的是生成器，在方法中其实对生成器已经有了优化，所以为什么还要引入判别器呢？其实光从生成器的优化上，很难通过损失判断生成器的好坏，而引入判别器可以更全局的判断生成器的效果，也通过博弈的方法让生成器能达到更好的效果]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>生成网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《ReferTransformer》论文笔记]]></title>
    <url>%2Fpost%2F149f20a.html</url>
    <content type="text"><![CDATA[由于之前看图片相关的论文比较多，而这篇论文偏向于视频理解的任务，为了避免有其他不了解这篇论文领域的同学看的时候比较吃力，在解读这篇论文前先介绍这篇论文的任务。任务定义: R-VOS(Referring video object segmentation): 给出一种物体对应的语言描述，分割出该物体对应的mask。这篇论文的相关视频在reddit上热度非常高，感兴趣的同学可以看下视频，相信也能对这篇论文应用的方向有更清晰的了解。[R] End-to-End Referring Video Object Segmentation with Multimodal Transformers 相关工作: Bottom-up 方法: 如图1(a)所示，用early-feature的方式融合视觉和语言特征，再用FCN decoder目标的掩码。 缺点: early-feature的方法无法获得较好的多模态特征，无法为跨模态推理提供明确的知识，并且会遇到由于场景变化而导致预测对象的差异。 Top-down 方法: 如图1(b)所示，两阶段的方式，先对图片/视频中所有的物体做实例分割，将实例分割的结果在视频中关联起来，形成一系列候选，再通过语言模型和Grounding Model筛选出语言描述提到的物体 缺点: 相比bottom-up的方式有更好的性能，但整个pipeline由于多阶段的方式太重。例如最近的一些方法，如HTC,CFBI中都需要再ImageNet，COCO，RefCOCO中预训练，然后在R-VOS数据集中fintune。而且将R-VOS的任务拆解为几个子问题分别优化由于误差传递等问题会造成次优的解决方案。 本文方法该论文的方法图1(c): 将文字的特征和queries的特征融合，作为conditional queries，生成的queries可以只聚焦在文字所提到的目标特征上，可以极大的减少queries的数量(例如detr中的100个) 考虑到需要从queries的特征中decode出object mask，使用instance-aware dynamic kernels从提取分割的mask特征 受启发于FPN，设计了CM-FPN(cross modal features pyramid network), 提取多模态的金字塔特征 网络结构如图2所示，主要由backbone,language as queries,Cross-modal Feature Pyramid Network,Instance Sequence Matching and Loss，inference五部分组成。下面我写的尽量详细点。 backbonevisual encoder使用通用的视觉backbone做视觉特征的编码器，例如可以使用经典的ResNet网络或者3D的特征编码器，如Video swin transformer。生成的特征是如下Sequence $F_v=\{F_t\}^T_{t=1}$,其中T表示T桢图像linguistic encoder使用现成的语言模型,如RoBERTa，提取文本的特征$F_e=\{F_i\}^L_{i=1}$，其中L表示L个word。在该任务中需要将$F_e$通过pooling提取句子级别的特征，这是由于该任务使用句子级别的特征和queries融合用于跨模态的特征融合 language as queries:transformer encoder: encoder部分和detr中一样，先用1x1卷积进行通道数压缩,再把宽和高压缩为一个维度，将特征序列化，再做位置编码transformer decoder: decoder部分也和detr比较类似，但有些差异。使用N个object queries提取每帧实例中的特征，区别在于queries在视频帧之间共享权重，好处是可以更灵活的处理长视频，并且对于同样的实例queries学习的特征更鲁棒。将句子级别的特征$F_e$复制N份和每一个object queries一起作为decoder的输入。对于T帧图像，会得到$N_q=TN$的预测集合prediction heads：具有三个预测头,分别是box head，mask head，class head(二分类，输出是否是文本中提到的目标)dynamic convolution： 使用动态卷积生成binary segmentation masks*illustration of conditional queries：得益于transformer的注意力机制，输入object queries和text embedding可以让object queries聚焦于text提到的物体 cross-model Feature Pyramid Network 该部分可以认为是这篇论文最大的创新点，提出了多模态融合的FPN网络，网络结构图如图4所示，和传统FPN网络的区别在于加入了vision-language fusion模块，该模块的结构类似transformer网络，图像特征作为encoder部分qkv的输入，文本特征作为decoder部分k和v的输入。 Instance Sequence Matching and Loss前面提到过，对于T帧图像，N个object queries，会得到$N_q=T*N$个预测结果。由于视频中跨帧的物体在相邻帧间保持相对相同的位置，可以将预测结果看成是N个实例在T帧上的轨迹 因此可以用instance matching strategy的方式对预测整体监督。将预测集集表示为$y=\{y_i\}^N_{i=1}$，第i个实例的预测表示为: y_i=\{p^t_i, b^t_i, s^t_i\}^T_{t=1}$p^t$表示一个概率的标量，用于表示实例是否在文本中被提到且在当前帧出现的概率；$b^t_i$是一个长度为4的向量，表示实例的坐标，具体定义是目标的中心坐标和宽高；$s^t_i$的维度为(H/4 * W/4),表示实例的分割mask。 损失函数部分由3部分组成，分类loss为focal loss，坐标回归loss为L1 Loss和GIOU Loss，分割loss为DICE Loss和binary mask focal loss Inference在推理时，模型会预测所有帧中对应实例类别的置信度，选择平均置信度最高的实例类别对应的定位和分割结果作为最终的输出结果 实验论文中有非常详细的和其他方法的对比和消融实验 表4中baseline的方法训练和推理阶段使用的都是长度为5帧的视频，baseline的方法不能有效的区分距离较近的相似物体，容易分割出最显著的区域，相比之下，通过表6(a)可以看出，只需要一个object queries就可以获得比baseline更好的效果，也证明了动态卷积对于分割任务是必要的 baseline的方式使用图像level的特征分割物体，而本文的方法是用图像-文本融合后的多模态特征 本文将动态卷积核的相对坐标和掩码特征融合起来，有助于模型定位出文本中提到的目标 不同backbone的影响 object queries数量的影响，从实验结果来看数量也不是越多越好，5个是实验中的最好效果。训练和测试帧数的影响，最好的是5帧的参数，但理论上应该帧数增多，效果还会提升，可能是受限于计算资源的影响预测头的影响，类别，边框回归，分割三个任务都预测，效果是最好的]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>视频理解</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Faster R-CNN》详解]]></title>
    <url>%2Fpost%2F79b7b6f1.html</url>
    <content type="text"><![CDATA[写在前面 最近实在是太忙了，很久没更新博客。想起前几周花了一些功夫给组内的小伙伴们做了个分享，为了做分享时自己能讲清楚，找了不少我认为描述的比较好的图片。Faster R-CNN可以说是目标检测领域的开山之作了，即使现今的顶会论文中，目标检测方面 的论文也有不少是对Faster R-CNN做的改进。这么经典的文章，细节又多如牛毛，真是常读常新。所以搬运到博客上应该也是有一些价值的。 Faster R-CNN 写作的过程是将网状的思考用树状的语法结构转换成线性的文字，阅读则是其逆向过程。读文章时并不用逐字逐句的按文章的书写顺序去读。可以按照合适的方式去更好的还原作者的思考过程。及时的停止读一篇文章，能及时止损，每个人精力和注意力都是宝贵的，不应该把时间花在读了对自己没价值的文章上。 检测任务目前的弊病之一就在于其网络的特征提取层直接从分类网络迁移学习来，并不是很贴合检测任务。]]></content>
      <categories>
        <category>输入分类</category>
      </categories>
      <tags>
        <tag>输入标签</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《GIoU》论文笔记]]></title>
    <url>%2Fpost%2F4065a784.html</url>
    <content type="text"><![CDATA[论文链接：https://arxiv.org/abs/1902.09630代码链接：https://github.com/generalized-iou &emsp;&emsp;这篇论文出自CVPR2019，算是目前已被录用且公布的位数不多的目标检测相关论文了。这篇论文提出了一种优化边界框的新方式——GIoU(generalized IoU，广义IoU)。目前关于IOU的新用法真是层出不穷，从Cascade R-CNN到IOU Net再到如今的GIoU，GIoU的方法是这些论文中相对简单的，相信很多朋友了解了这篇文章的远离后，内心的OS都是“总觉得损失函数可以优化，这么简单我怎么没想到呢？”，哈哈，反正我是这样想的了。下面来看看这篇文章所提出的方法吧。 动机&emsp;&emsp;目前目标检测中主流的边界框优化采用的都是BBox的回归损失(MSE loss, L1-smooth loss等)，这些方式计算损失值得方式都是检测框得“代理属性”，而忽略了检测框本身最显著的性质——IoU。如下图所示，在L1及L2范数取到相同的值时，实际上检测效果却是差异巨大的，直接表现就是预测和真实检测框的IoU值变化较大，这说明L1和L2范数不能很好的反映检测效果。 &emsp;&emsp;除了能反映预测检测框与真实检测框的检测效果外，IoU还具有尺度不变性。可是既然IOU这么好，为什么之前不直接用IoU呢，这是由于IoU有两个缺点，导致其不太适合做损失函数： 但检测框与gt之间没有重合时，IoU为0。而在优化损失函数时，梯度为0，意味着无法优化 在检测框与gt之间IoU相同时，检测的效果也具有较大差异，如下图所示： &emsp;&emsp;基于IoU的优良特性和其作为损失函数时的致命缺点，作者提出了一个新的概念——GIoU 方法&emsp;&emsp;GIoU的定义如下图所示， &emsp;&emsp;根据定义，GIoU具有如下性质： GIoU具有作为一个度量标准的优良性质。包括非负性，同一性，对称性，以及三角不等式的性质 与IoU相似，具有尺度不变性 GIoU的值总是小于IoU的值 对于两个矩形框A和B，0≤IoU(A，B)≤1，而-1≤GIoU≤1 在A，B没有良好对齐时，会导致C的面积增大，从而使GIoU的值变小，而两个矩形框不重合时，依然可以计算GIoU，一定程度上解决了IoU不适合作为损失函数的原因 &emsp;&emsp;GIoU作为损失函数时计算方式如下的算法2 &emsp;&emsp;从算法中可以看到和GIoU的计算方式和IoU的步骤基本保持一致，在得到IoU的值后在根据上面的算法1计算GIoU的值。这里还不太清楚方向传播时，梯度是怎么计算的。等我看看源码再来更新吧 实验&emsp;&emsp;作者分别在几种主流的目标检测算法上做了实验，分别是YoLo、Faster R-CNN和Mask R-CNN。这里贴上在Pascal Voc数据集上的实验结果，如下 &emsp;&emsp;实验结果中在YoLo v3上可以看到GIoU相比IoU的损失函数有较大幅度的提升，而在faster r-cnn中GIoU和IoU作为损失函数的区别不大，这里作者给出的解释是faster rcnn的anchor更密集，导致不易出现与gt不重叠的检测框。其实个人认为，anchor多的情形与gt不重叠的检测框才多，更根本的原因应该是RPN网络进行了一次粗检后，滤去了大部分跟gt没有重合的检测框。导致GIoU相比IoU的损失函数提升不明显吧 总结&emsp;&emsp;GIoU的方法很简单，巧妙的是优化的点。通过广义IoU作为损失函数替代bbox回归还是很有趣的。不过疑惑的是实验结果中的检测AP值都非常低，原生的faster rcnn在pascal voc上的检测效果都不会这么差。从实验对比上GIoU的损失函数相比原始的损失函数在准确率不到40%的效果上来说确实有较大幅度的提升。然而要是换到准确率较高的baseline上呢？这一点还需要实验验证。&emsp;&emsp;另外总感觉这篇论文有点点到即止，没有更多的实验验证bbox作为损失函数存在缺陷的原因。 欢迎关注我的公众号]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Mask TextSpotter》论文笔记]]></title>
    <url>%2Fpost%2Ff222e8ff.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;论文链接：https://arxiv.org/abs/1807.02242&emsp;&emsp;代码链接：https://github.com/lvpengyuan/masktextspotter.caffe2 &emsp;&emsp;除了自然场景的目标检测外，文本检测也是近年来热门的研究领域。Mask TextSpotter是ECCV 2018发表的一篇文本检测文章，具有以下特点： 端到端的检测+识别框架 基于Mask R-CNN结构 在处理不规则的文本形状时，优于之前的方法 除了文本行检测外，能进行字符分割 &emsp;&emsp;Mask R-CNN结构是近几年来最优秀的目标检测结构了，该篇文章将其应用到文本检测这一任务上，并且所做的优化算是十分巧妙了。 介绍&emsp;&emsp;Mask TextSpotter提出了一种名为掩码文本检测的文本检测器，它可以检测和识别任意形状的文本。 &emsp;&emsp;上图直观的比较了不同的检测方法其场景文本定位的效果，左图是水平水平文本定位方法，中间是支持倾斜框的文本检测方法，右图是Mask TextSpotter的检测方法。 &emsp;&emsp;可以看到Mask TextSpotter相比较于其他两种方法能够提供更准确的定位图，为后续的识别提供良好的检测效果。这篇文章总共有四点贡献： 提出了一种端到端的文本检测加识别模型，具有简单高效的特点 该方法可以检测和识别各种形状的文本，包括水平文本、定向文本和弯曲文本 与之前的方法相比，该方法通过语义分割来实现精确的文本检测和识别 在各种基准的文本检测和定位上都达到了state-of-the-art的效果 相关工作&emsp;&emsp;这部分主要介绍了场景文本检测、识别及结合的发展进程。强调了Mask TextSpotter基于Mask R-CNN，区别在于该方法不仅可以分割文本，也可以进行字符分割。 方法框架&emsp;&emsp;Mask TextSpotter的架构如下图所示： &emsp;&emsp;该架构由以下4部分组成 backbone：ResNet50；FPN网络（ top-down 结构） 生成文本建议：RPN 边界框回归：Fast R-CNN 文本和字符实例分割：mask branch &emsp;&emsp;首先由RPN生成大量的文本提案，然后将提案的RoI特征输入Fast R-CNN分支和mask分支中，生成准确的文本候选框、文本实例分割图和字符分割图。 &emsp;&emsp;每部分的细节如下： Backbone:自然场景的文本大小不一，为了提取更高层的语义特征。采用了ResNet-50网络，并用对小目标有较好效果的FPN网络提取特征 RPN:RPN网络为Fast R-CNN分支以及mask分支生成文本建议。anchor设置5种尺寸$\{32^2,64^2,128^2,256^2,512^2\}$，FPN网络中有5个层级$\{P_2,P_3,P_4,P_5\}$，3种比例$\{0.5,1,2\}$。区域特征映射方式采用ROI Align。 Fast R-CNN:包括分类和回归任务，主要作用是为了后续的检测提供更精确的检测框。 Mask 分支：掩码分支有两个任务，分别是全局文本实例分割和字符分割。如下图所示。Mask分支的输入是固定大小的ROI（1664）,经过4个卷积层和一个反卷积层将特征图降维到38个维度（特征图大小：32128）。这38个维度由以下3部分组成： 全局文本实例分割 背景分割 10个数字，26个字母 标签的生成&emsp;&emsp;为了满足训练的要求，ground truth要包含$P=P\{p_1,p_2…p_m\}$以及$C=\{c_1=(cc_1,cl_1),c_2=(cc_2,cl_2),…,c_n=(cc_n,cl_n)\}$。其中$p_i$表示一个文本定位的多边形。$cc_j$和$cl_j$分别是字符的类别和定位。值得一提的是并不要求所有样本都需要有标记$C$。首先用涵盖目标最小水平矩形面积的方法，将多边形转换为水平矩形。然后通过RPN和Fast R-CNN网络生成区域建议。对于具有ground truth P, C(可能不存在)的mask分支，需要生成两种类型的目标映射，以及RPN生成的建议：用于全局实例分割的映射和用于字符实例分割的映射。对于正样本的proposal,首先得到最匹配的水平矩形。相应的多边形及字符（如果有的话）可以进一步得到。在映射$H×W$上调整多边形和字符的proposal一致的公式如下： &emsp;&emsp;其中$(B_{x0},B_{y0})$是所有多边形和字符原始的顶点，$(B_x,B_y)$是所有多变形和字符更新后的顶点，$(r_x,r_y)$是rpoposal产生的顶点。 &emsp;&emsp;然后，全局映射图的生成规则：通过绘制zero-initialized mask的规则多边形；字符边界框的生成：固定所有字符的中心点，并将边缩短到原始边的1/4。如下图所示： 损失函数&emsp;&emsp;该部分讨论的是整个框架优化参数时的损失函数组成，正如上面介绍的Mask TextSpotter的框架由4部分组成，除了提取特征的网络外，其余的3部分都是损失函数的组成部分，如下所示： &emsp;&emsp;mask分支有两个任务，因此$L_{mask}$的计算如下： &emsp;&emsp;其中$L_{global}$为Cross entropy损失，而$L_{char}$为Softmax损失。其中作者设置的超参数$α_1,α_2,β$都为1。&emsp;&emsp;$L_{global}$的计算公式如下： &emsp;&emsp;$L_{char}$的计算公式如下： &emsp;&emsp;其中T为类别，X为预测的输出，Y为gt，W为权重。W的主要作用是为了样本均衡。不同类别的W计算公式如下： 推理(测试)&emsp;&emsp;不同于mask 分支的ROI来自于RPN网络，在推理阶段，mask分支的ROI来自于Fast R-CNN网络，而不是RPN网络，这是由于Fast R-CNN的输出更精确。推理阶段可以分为以下几个过程： 输入测试图片，获得Fast R-CNN的输出，并经过NMS; 保存下来的proposals被输入mask分支以生成全局映射(gloabal maps)和字符映射(character maps) 通过计算全局映射的map区域获得预测的多边形，通过pixel voting算法生成字符映射序列 pixel voting算法的细节如下： 二值化背景图，阈值为192 根据二值化中的联通与划分所有字符区域 实验&emsp;&emsp;作者总共使用了4个数据集，除了SynthText用来预训练以外，其余的三个数据集ICDAR 2013，ICDAR2015，Total_Text均做了测试实验。 &emsp;&emsp;通过以上实验数据比较，Mask TextSpotter水平文本、定向文本和弯曲文本等数据集上的良好性能证明了该方法对文本检测和端到端文本识别的有效性和鲁棒性。 欢迎关注我的公众号]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>目标检测</tag>
        <tag>文本检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单线性回归]]></title>
    <url>%2Fpost%2F7fda15c2.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;好长一段时间挺想重新系统的学习回顾下机器学习的知识，看了些深度学习的论文和框架后，觉得机器学习的知识确实太重要了。接下来的半年时间，想系统的总结实践下，也算是2019年的第一个flag吧。 使用单一特征预测响应值&emsp;&emsp;这是一种基于自变量值(X)来预测因变量值(Y)的方法。假设X和Y两个变量是线性相关的。线性回归就是尝试寻找一种根据特征或自变量(X)的线性函数来精确预测响应值(Y)。 怎样找到最佳的拟合线&emsp;&emsp;在这个回归任务中，我们将通过找到“最佳拟合线”来最小化预测误差——回归线应该尽量拟合X-Y的分布，即误差是最小的。例如$y_p$是预测值，$y_i$是实际值，这个过程就是使$y_p$和$y_i$之间的关系满足$min\{SUM(y_i-y_p)^2\}$ &emsp;&emsp;这里以学生分数数据集做这个实验，实验数据如下图所示： 实验数据预处理 导入相关库 导入数据集 检查缺失数据 划分数据集 特征缩放(这里使用简单线性模型的相关库进行) 通过训练集来训练简单线性回回归模型&emsp;&emsp;为了使用模型来训练数据集，这里使用python中的sklearn.linear_model库的LinearRegression类。然后实例化一个LinearRegression类的regressor对象。最后使用LinearRegression类的fit()方法。将regressor对象对数据集进行训练。 预测结果&emsp;&emsp;现在将预测来自测试集的观察结果。将实际的输出保存在向量Y_pred中。使用前一步中训练的回归模型regressor的LinearRegression类的预测方法来对结果进行预测。 可视化&emsp;&emsp;为了直观的查看线性回归的效果，这里将对结果进行可视化。使用matplotlib.pyplot库对我们的训练结果和测试集结果做散点图，以查看模型的预测效果。 123456789101112131415161718192021222324252627282930313233#python的数据处理库import pandas as pdimport numpy as npimport matplotlib.pyplot as plt# 第一步：数据预处理dataset = pd.read_csv('../datasets/studentscores.csv')X = dataset.iloc[ : , : 1 ].valuesY = dataset.iloc[ : , 1 ].valuesfrom sklearn.model_selection import train_test_splitX_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = 1/4, random_state = 0) # 第二步：训练集使用简单线性回归模型来训练from sklearn.linear_model import LinearRegressionregressor = LinearRegression()regressor = regressor.fit(X_train, Y_train)# 第三步：预测结果Y_pred = regressor.predict(X_test)# 第四步：可视化# 训练集结果可视化plt.scatter(X_train , Y_train, color = 'red')plt.plot(X_train , regressor.predict(X_train), color ='blue')plt.show()# 测试集结果可视化plt.scatter(X_test , Y_test, color = 'red')plt.plot(X_test , regressor.predict(X_test), color ='blue')plt.show() 实验结果&emsp;&emsp;训练集上的拟合结果： &emsp;&emsp;测试集上拟合结果 欢迎关注我的公众号]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(IOU-Net)《Acquisition of Localization Confidence for Accurate Object Detection》论文笔记]]></title>
    <url>%2Fpost%2F4395ef5a.html</url>
    <content type="text"><![CDATA[论文链接：https://arxiv.org/abs/1711.07767论文代码：https://github.com/ruinmessi/RFBNet &emsp;&emsp;目标检测框架中目前主要依靠边界框回归和非极大值抑制来定位对象，并且非极大值抑制算法去除重复框的依据是候选框的分类置信度，在这个过程中缺少了一个边框筛选的重要参考——定位置信度。因此这篇文章提出IOU-Net，IOU-Net具有以下优点： 通过在网络中加入定位置信度，在NMS算法中用定位置信度代替分类置信度作为去重复框的参考 提出了一种基于优化的边界框修正算法 绪论&emsp;&emsp;作者先根据实验定性的分析了仅用分类置信度评估检测框是否准确不太合适，缺少分类置信度回带来两个缺点： 分类置信度被用作对检测框排序的度量标准，并且在NMS算法中忽略了定位的精度 缺少定位置信度，使得广泛使用BBox回归缺少可解释性，这点在《cascade r-cnn》中也有提到多次使用BBox回归可能导致检测框的局部退化。 &emsp;&emsp;上图中，黄色框表示gt，绿色和红色都是是模型预测得到的检测框。如图(a)所示，绿色检测框与红色检测框相比更适合作为最终的预测结果，绿色框与gt的IOU(交并比)更高，定位置信度也更高，但分类置信度却低于红色的检测框，在以往NMS算法中以分类作为检测框排序标准，会保留红色的框，而滤去绿色的，这显然是不够合理的。图(b)比较的是基于优化的边界框修正比基于回归的边界框修正有更好的效果(多次应用基于回归的边界框修正，会导致检测框退化)基于上图中目前目标检测框架的不足，作者IOU-Net所做的两点改进： 通过在目标检测框架中引入定位置信度。在NMS算法中，预测检测框与gt之间IOU的顺序，代替原来的以分类置信度排序 提出了一种基于优化的边界框修正算法 &emsp;&emsp;IOU-Net中的IOU指的就是网络预测到的检测框与gt之间的IOU 深入研究目标定位&emsp;&emsp;作者在绪论中提到了所作的两点改进，绪论中定性的用图表表明了缺少定位置信度带来的缺点，接下来分别做了几个实验定量的验证所提出方法的有效性。以下实验所采用的训练集是MS-COCO trainval35k,测试集用minival，检测框架用FPN网络。 类别不匹配和定位精度&emsp;&emsp;作者先简单介绍了NMS算法的原理，及关于这方面的研究进展。随后指出NMS算法中，以分类置信度作为检测框的度量标准不太合理。 &emsp;&emsp;如上图所示，(a)横轴是预测的检测框与gt之间的IOU,纵轴是分类置信度。(b)横轴同样是预测的检测框与gt之间的IOU,纵轴是定位置信度。若以检测框与gt之间的IOU大于0.5作为是否检测到的阈值，作者计算了两张图横纵轴的Pearson correlation(皮尔森相关系数[wiki百科])，结论是(a)的相关的相关系数为0.217，而(b)的相关性为0.617。&emsp;&emsp;作者也总结了分类置信度与定位是否准确关系不大的原因：目标检测框架中分类和定位的相关性不强的原因在于正样本和负样本的选取方式，例如获取正样本只用其与gt之间的IOU大于指定阈值即可。关于正样本的生成方式，之前我也想过为什么这样生成，如此的生成方式一定程度上会带来定位不准确的问题。但是相比较于完全使用标注的gt这种方式具有以下的优点： 可以生成多个比例的候选框，覆盖样本的不同尺度，正样本会更好的回归 样本均衡一直是目标检测算法的一个难题，这种方式一定程度上可以生成更多的正样本 &emsp;&emsp;很明显正样本的选取方式带来的好处远大于其缺点，作者也表明，定位不准主要是缺少定位置信度的原因 &emsp;&emsp;传统NMS算法中由于用分类置信度作为MNS算法中检测框的度量标准，会导致与gt有更大IOU的检测框被抑制。如上图所示，蓝色代表传统的NMS算法；黄色表示以定位置信度作为度量标注的NMS算法；绿色表示不用NMS时，理论能生成的最多检测框数量。可以看到，在传统的NMS算法中由于缺少定位置信度，保留的检测框中与gt的IOU在0.9以上的会被抑制。 BBox回归的非单调性&emsp;&emsp;faster rcnn中使用了两次边界框回归，以达到位置精修的目的。但位置精修的次数是不是越多越好，《cascade r-cnn》中也提到了这样的疑惑，作者做了个实验。 &emsp;&emsp;上图中，横轴代表边界框回归结构的迭代次数，纵轴代表检测精度。从上图可以看到，无论是FPN网络还是Cascade R-CNN网络，在多次使用边界框回归之后，都会出现退化现象。作者解释出现这种情况的是由于缺少定位置信度，对模型不能进行细粒度(fine-grained)的控制，例如对不同的检测框采用不同的迭代次数。 IOU-Net&emsp;&emsp;为了定量分析IOU-Net的有效性，接下来作者给出了IOU-Net的框架和NMS算法中怎样用IOU进行预测以及基于回归的边界框精修算法。 IOU-Net架构 &emsp;&emsp;如上图所示，IOU-Net框架中提特征的网络部分与FPN网络一致，并估计每个边界框的定位精度(IOU)，通过数据扩充生成用于训练IOU-Net的边框和标签，而不是接受来自RPN的建议(说实话这里有点奇怪，RPN网络作用之一是用来位置精修的，不用之前RPN网络中的建议，这个没太理解作者的意思)。细节实施上，作者表明在生成正样本时，从候选集中移除与gt的IOU小于0.5的样本，然后对所有样本统一采样训练，据作者所说这样可以获得更好的性能和鲁棒性 使用IOU度量的NMS&emsp;&emsp;这里作者主要介绍了在NMS去除重复框时，如何使用IOU作为度量标准。伪代码如下所示： &emsp;&emsp;特点是采用了基于聚类的规则来更新分类置信度。具体实施过程是当boxi将boxj移除了，会更新boxi的分类置信度为max(si,sj) 将边界框精修作为一个优化过程&emsp;&emsp;边界框精修的数学公式定义如下： &emsp;&emsp;其中 $box_{det}$是检测到的边界框，$box_{gt}$是实际的边界框.trainsform是用参数c对边界框变换的转换函数。$crit$是两个边界框的度量标准(这个标准在faster rcnn中是smooth-L1) &emsp;&emsp;基于回归的算法用前馈神经网络直接估计最优解c* 。 然而，迭代边界框回归方法易受输入分布变化的影响，并可能导致非单调的回归退化，如图4所示。为了解决这些问题，作者提出一种基于优化的边界框精修方法。 利用IoU-Net作为鲁棒定位精度（IoU）估计器的方法。 此外，IoU估计器可以用作早期停止条件，以通过自适应步骤实现迭代精修。 &emsp;&emsp;IoU-Net直接估算IoU。 虽然所提出的精确RoI池化层能够计算关于边界框坐标的IoU的梯度，我们可以直接使用梯度上升方法找到方程1的最优解。在算法2中，将IoU的估计视为优化目标，我们迭代地使用计算的梯度更新边界框坐标和最大化检测到的边界框与其匹配的真值之间的IoU。 此外，预测的IoU是每个边界框上的定位置信度的可解释指示符，并且有助于解释所做的转换。 &emsp;&emsp;关于算法2中的初始坐标，初始化采用了一次边界框回归。 precise ROI Pooling&emsp;&emsp;为了细粒度的修正边界框，作者引入了一种新的ROI Pooling方法，以代替之前的ROI Pooling和ROI Align。关于ROI Pooling到PrROI Pooling我会专门写一篇博文比较其区别，这里就先简单介绍下。 &emsp;&emsp;如上图，绿色的点代表特征图(feature map)的值，虚线代表目标在映射在特征图中实际的位置。RoI Pooling：ROI Pooling的弊病，在于其在特征图中的边界框只能取整数，因此会从虚线取整到实线，映射回原图时，会带来误差。ROI Align:为了避免ROI pooling中取整带来的误差。ROI Align中会保持浮点数边界不变，将特征图中的值用双线性插值，由绿色的点映射到图中的4个红点。然后再做池化PrRoI Pooling: 如果付出浮点数边界内固定的点来做池化，这样对于目标实际的映射大小不具有适应性。PrRoI Pooling则使用二阶积分来做池化 实验&emsp;&emsp;针对作者提到的几个方法，分别做了几组实验。 下面的表1总结了在不同检测框架中，使用不同NMS算法性能上的变化。可以看到IOU指导的NMS算法，在高IOU指标上性能会优于其他算法 基于优化的边界框精修 耗时实验：耗时还在可以接受的范围 欢迎关注我的公众号]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Receptive Field Block Net for Accurate and Fast Object Detection》论文笔记]]></title>
    <url>%2Fpost%2Ff38a4f4.html</url>
    <content type="text"><![CDATA[论文链接：https://arxiv.org/abs/1711.07767论文代码：https://github.com/ruinmessi/RFBNet &emsp;&emsp;这篇文章是CV领域顶会ECCV2018中关于目标检测的文章,文中以SSD模型为基础提出了RFB结构,强调兼顾速度与性能。说来也巧,因为项目需要,在看这篇论文之前正好看过提出dilated convolution的那篇文章，但是dilated convolution的结构获得更大感受野的方式确实对细粒度的分割会比较好，适用图像分割领域。当我还在想能怎么用在目标检测上时，就看到了RFB网络。虽然作者说是为了兼顾速度与性能将其应用到one stage的SSD上，但我也在two stage的faster rcnn上，复现出了较好的效果。这是一篇我个人很喜欢的文章，实验充分，模拟视觉细胞的结构让我觉得即简单又巧妙。因此简单总结下这篇文章。 绪论&emsp;&emsp;作者指出目前图像领域深度学习的发展越来越倾向于用更深的网络以达到更好的效果，然而像ResNet等很深的网络往往具有较大的计算量，导致速度受限。相比之下作者提出的RFB结构具有以下优点： 模拟了人类视觉系统RFs的大小和离心率设置，增强轻量级CNN网络的特征提取能力 简单的替换了SSD的最后一级卷积层，在较少的计算增加的情况下，提升了模型的性能 除了SSD之外，也扩展到了MobileNet中取得了较好的结果，展示了结构的泛化性 相关工作&emsp;&emsp;这部分就不总结了，主要介绍了one stage和two stage的目标检测模型和目前论文中在感受野上做的研究。 方法视觉皮层&emsp;&emsp;如上图所示是人类感受野(pRF)的示意图,可以看到有以下规律: 距离中心越远的pRF越大，即pRF大小与偏心含有正相关的关系 不同图谱的pRF大小规模不同 感受野块 &emsp;&emsp;作者提出的RFB结构的原理如上图所示，该结构的特点有： 多分支卷积层：根据之前人类感受野(pRF)的示意图，为了仿照不同图谱的pRF大小规模不同，作者提出用不同大小的卷积核以实现多大小的pRF，这一方法应该优于共享固定大小的RFs。这一结构参考了Inception的结构。 膨胀卷积和池化层：膨胀卷积的基本意图在于生成分辨率更高的特征图，在相同计算量的情况下获得更大的感受野。而膨胀卷积核的大小和扩张与pRFs在视觉皮层的大小和偏心具有相似的功能关系。然后再将不同膨胀卷积处理过的层融合起来，以达到视觉皮层中感受野的效果。rFB的结构如下图所示： RFB检测框架&emsp;&emsp;作者提出的RFB的结构是在SSD的基础上改的，做的修改及替换如下图所示： 轻量级的结构：这里主要说的是SSD的有点，这里不赘述 多尺度结构中的RFB：在原始的SSD中，有着层叠的卷积层，形成一系列空间分辨率连续下降、感受野不断增大的feature map。在作者的实现中，保持了相同的SSD级联结构，但具有较大感受也的卷积层被RFB结构替代。作者还指出最后基层卷积层的特征图太小，适合用5X5大小的卷积核。这部分论文里Fig.4的a图中用的确实是5x5的卷积核，但是给出的代码中却用两个3x3的卷积核替代了，这部分我有点疑惑。 实验&emsp;&emsp;这一部分就是各种各样的实验图表了，也不赘述。从以下图表可以看到实验结果确实很惊艳，用了RFB结构的网络mAP会有不小的提升。 &emsp;&emsp;作者还给出了一张目前目标检测算法的准确率和耗时的图片，对比的多是one stage的模型，可以作为参考 欢迎关注我的公众号]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faster R-CNN代码的caffe封装]]></title>
    <url>%2Fpost%2Ffc7cd866.html</url>
    <content type="text"><![CDATA[目前部署服务的主流语言还是C++，因此项目上线前，在需要部署调试的时候需要对Faster R-CNN工程化为C++代码。这篇博文总结的部分主要将python版本的demo.py代码及其相关的部分改写成C++版本。 封装1封装听起来很复杂，其实就是隐藏代码的实现细节，只暴露出对应的接口。Faster R-CNN具有很广泛的流行度了，相关的资料在可以说在目标检测模型里是最多的。为了方便理解，首先给出C++工程demo的目录结构。 1234567.└── lib│ │── faster_rcnn.cpp│ │── faster_rcnn.hpp│ │── CMakeLists.txt│—— CMakeLists.txt│—— main.cpp 其中faster_rcnn.cpp与faster_rcnn.hpp是对应的demo接口，main.cpp可以直接调用。faster_rcnn.cpp文件如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#ifndef FASTER_RCNN_HPP#define FASTER_RCNN_HPP#include &lt;stdio.h&gt; // for snprintf#include &lt;string&gt;#include &lt;vector&gt;#include &lt;math.h&gt;#include &lt;fstream&gt;#include &lt;boost/python.hpp&gt;#include "caffe/caffe.hpp"#include "gpu_nms.hpp"#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;using namespace caffe;using namespace std;#define max(a, b) (((a)&gt;(b)) ? (a) :(b))#define min(a, b) (((a)&lt;(b)) ? (a) :(b))//background and carconst int class_num=2;/* * === Class ====================================================================== * Name: Detector * Description: FasterRCNN CXX Detector * ===================================================================================== */class Detector &#123;public: Detector(const string&amp; model_file, const string&amp; weights_file); void Detect(const string&amp; im_name); void bbox_transform_inv(const int num, const float* box_deltas, const float* pred_cls, float* boxes, float* pred, int img_height, int img_width); void vis_detections(cv::Mat image, int* keep, int num_out, float* sorted_pred_cls, float CONF_THRESH); void boxes_sort(int num, const float* pred, float* sorted_pred);private: shared_ptr&lt;Net&lt;float&gt; &gt; net_; Detector()&#123;&#125;&#125;;//Using for box sortstruct Info&#123; float score; const float* head;&#125;;bool compare(const Info&amp; Info1, const Info&amp; Info2)&#123; return Info1.score &gt; Info2.score;&#125;#endif 其对应的faster_rcnn.hpp文件为 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#ifndef FASTER_RCNN_HPP#define FASTER_RCNN_HPP#include &lt;stdio.h&gt; // for snprintf#include &lt;string&gt;#include &lt;vector&gt;#include &lt;math.h&gt;#include &lt;fstream&gt;#include &lt;boost/python.hpp&gt;#include "caffe/caffe.hpp"#include "gpu_nms.hpp"#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;using namespace caffe;using namespace std;#define max(a, b) (((a)&gt;(b)) ? (a) :(b))#define min(a, b) (((a)&lt;(b)) ? (a) :(b))//background and carconst int class_num=2;/* * === Class ====================================================================== * Name: Detector * Description: FasterRCNN CXX Detector * ===================================================================================== */class Detector &#123;public: Detector(const string&amp; model_file, const string&amp; weights_file); void Detect(const string&amp; im_name); void bbox_transform_inv(const int num, const float* box_deltas, const float* pred_cls, float* boxes, float* pred, int img_height, int img_width); void vis_detections(cv::Mat image, int* keep, int num_out, float* sorted_pred_cls, float CONF_THRESH); void boxes_sort(int num, const float* pred, float* sorted_pred);private: shared_ptr&lt;Net&lt;float&gt; &gt; net_; Detector()&#123;&#125;&#125;;//Using for box sortstruct Info&#123; float score; const float* head;&#125;;bool compare(const Info&amp; Info1, const Info&amp; Info2)&#123; return Info1.score &gt; Info2.score;&#125;#endif 初次在linux上运行C++代码时，真是各种蒙圈，看着Makefile文件也搞不明白其中关键字代表的意义，使用cmake后，感觉容易了很多。如下是lib文件夹底下，即faster_rcnn.cpp对应的CMakeLists.txt文件。 123456789cmake_minimum_required (VERSION 2.8)SET (SRC_LIST faster_rcnn.cpp)include_directories ( "$&#123;PROJECT_SOURCE_DIR&#125;/../../caffe-fast-rcnn/include" "$&#123;PROJECT_SOURCE_DIR&#125;/../../lib/nms" /usr/local/include /usr/include/python2.7 /usr/local/cuda/include )add_library(faster_rcnn SHARED $&#123;SRC_LIST&#125;) 然后依次执行cmake.和make就完成编译了。接下来是调用faster_rcnn.cpp接口的main.cpp文件。 123456789101112#include "faster_rcnn.hpp"int main()&#123; string model_file = "/home/ouyang/Program/py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt"; string weights_file = "/home/ouyang/Program/py-faster-rcnn/output/faster_rcnn_end2end/voc_2007_trainval/vgg16_faster_rcnn_iter_170000.caffemodel"; int GPUID=2; Caffe::SetDevice(GPUID); Caffe::set_mode(Caffe::CPU); Detector det = Detector(model_file, weights_file); det.Detect("/home/ouyang/Program/py-faster-rcnn/data/demo/90.jpg"); return 0;&#125; 里面的pt文件和model文件的路径替换成对应的就好。然后是main.cpp对应的CMakeLists.txt文件。 123456789101112131415161718192021222324252627#This part is used for compile faster_rcnn_demo.cppcmake_minimum_required (VERSION 2.8)project (main_demo)add_executable(main main.cpp)include_directories ( "$&#123;PROJECT_SOURCE_DIR&#125;/../caffe-fast-rcnn/include" "$&#123;PROJECT_SOURCE_DIR&#125;/../lib/nms" "$&#123;PROJECT_SOURCE_DIR&#125;/lib" /usr/local/include /usr/include/python2.7 /usr/local/cuda/include)target_link_libraries(main /home/ouyang/Program/py-faster-rcnn/Cplusplus2/lib/libfaster_rcnn.so /home/ouyang/Program/py-faster-rcnn/caffe-fast-rcnn/build/lib/libcaffe.so /home/ouyang/Program/py-faster-rcnn/lib/nms/libgpu_nms.so /usr/local/lib/libopencv_highgui.so /usr/local/lib/libopencv_core.so /usr/local/lib/libopencv_imgproc.so /usr/local/lib/libopencv_imgcodecs.so /usr/lib/x86_64-linux-gnu/libglog.so /usr/lib/x86_64-linux-gnu/libboost_system.so /usr/lib/x86_64-linux-gnu/libboost_python.so /usr/lib/x86_64-linux-gnu/libglog.so /usr/lib/x86_64-linux-gnu/libpython2.7.so ) 相同的执行cmake.和make完成该部分的编译。 封装2上面C++封装的Faster R-CNN结构简单，代码量也少，部署起来很方便。但在项目中也有着致命的弱点，检测速度大概比python版的Faster R-CNN慢了一个数量级。因此很有必要寻求另外的封装方式，这里参考D-X-Y纯C++版的Faster R-CNN代码，该代码将所有代码都改写为了C++版本，不像上一部分的代码中调用了很多python的库。D-X-Y所封装的代码中对应原python版本中demo.py的接口在G:\gitProgram\caffe-faster-rcnn\src\api\FRCNN\frcnn_api.cpp和G:\gitProgram\caffe-faster-rcnn\include\api\FRCNN\frcnn_api.hpp。这里我将所需要的文件整合到一起，目录结构为：123456789101112131415161718.└── lib│ │── frcnn_api.cpp│ │── frcnn_api.hpp│ │── CMakeLists.txt│—— CMakeLists.txt│—— main.cpp└── include│ │── api│ │ api.hpp│ │ └── FRCNN│ │ │ │── frcnn_api.hpp│ │ │ │── rpn_api.hpp│ │── caffe│ │ │──proto│—— libcaffe.so.1.0.0│—— libcaffe.so│—— test.prototxt 上述代码结构中的include来自D-X-Y代码中的include,/include/caffe/proto来自D-X-Y代码中编译后产生，对应的路径为./caffe-faster-rcnn/.build_release/src/caffe/proto。第二种方式就不贴对应的代码了，在这里我也将两种方式对应的代码放在github上，供大家参考代码 欢迎关注我的公众号]]></content>
      <categories>
        <category>深度学习笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>代码部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写博客的第100天&北漂的第30天]]></title>
    <url>%2Fpost%2F6341d9b.html</url>
    <content type="text"><![CDATA[博客&emsp;&emsp;从开始写博客到今天也差不多100天了，100天的时间写了15篇博文，大概一个星期写一篇，不敢说高产，但最起码坚持下来了。回顾这15篇博文，每篇博文从动笔到完成少则1个小时，多则要花大半天收集整理资料，事后还要自己读几遍，以修改文中的漏洞。在这100天里，有时我也在问自己，维护这个网站和对应的公众号，以及花费时间去写博客的价值是什么。其实这个问题在我决定开始搭建属于自己博客时也有思考，并在第一篇博客文对自己这样做的原因做了简短的回答。在生活节奏如此快的今天，自我管理，时间管理变得越来越重要，也应运而生了各种“21天计划”和”100天行动”等活动用来培养一个良好的习惯。我也相信通过21天或者100天的重复一件事情，可以形成对应的习惯。因此，如今到了开始写博客的第100天，我理应比第1天写博客时有了更深的感触。我一直相信，从一个人的一个优点中，可以看出这个人具备的其他优点，缺点也是一样。我想这也是古人说“3岁看大，7岁看老”的原因，虽然年纪越大，可塑性越差，但养成一个对自己有益的习惯总是没差的。潜移默化中，写博客这个习惯为我带来了很多收获，我想这就是花心思做这件事的价值。 写博客让我把问题想的更明白：在看到《孟子》中写道“尽信书，不如无书”前，我一直以为书上写的东西是完全正确。总觉得能把自己的思想和见识以书本为载体，复制出无数多份发行出去，是一件很酷的事情，一本书的作者提出的观点一定是经过深思熟虑，遣词造句都尽量贴合他的想法。但在自媒体如此发达的今天，也看到了不少漏洞百出，毫无逻辑的书或文章，会有这样的文章出现无非是作者水平不够，或者作者只求强行牵强的证明他提出的观点有些许正确而有意而为之。《如何阅读一本书》中说读文章就好像与作者交流，当作者对一件事情比你有更深刻见解时，读者才会觉得有所收获。因此在我自己写博文时，为了尽量能把一个想法弄的更明白，常常会一遍遍的查找资料，以免闹出笑话，或让自己和别人看了感觉毫无营养。这无疑对自己是一种鞭策，当脑海中的一个想法变成了字符呈现在自己眼前时，也像是与自己交流，写出的东西首先要过自己这关，没有什么错误才能继续往下写。 让我更会表达自己的想法：读研期间和导师聊天时导师常和我说出了学校怎么让领导和周边人发现你的价值，实际上程序员这个职业，经过一个不错的正规学校培养，大家的编程能力都差不多，这个时候需要比别人更会表达自己的想法，表达能力有时比编码能力更重要，往往比别人优秀一点，出众的就会是你。我深以为然，但培养自己的表达能力不是一朝一夕的事情。翻看自己的这十多篇博文，很高兴从这些文章中我发现自己对于一个事情的表达，能说的更明白。工作的这一个月我也发现，在工作问题上，自己比想象中更能表达自己的想法，我想这多少也与写博文的积累有关。 好记性不如烂笔头：在关键时刻能想起一个知识点极其重要，对于做技术而言，能随时随地使用的技能构成了技术人的基本价值，看了就忘像是在做无用功。学习的过程更重要的是构建知识体系，面对问题，脑海中需要有着几种可行的解决方案，这就需要大量的积累与记忆。常看常新，有时极力回想某篇论文中某个新颖的观点时，却怎么也想不出，这时翻看对应的博文又能重新的回顾当时看这篇论文时的思路。在一遍遍的翻看中，总能发现当时思考的不足，也更能加深对某个点的记忆 为了收获志同道合的朋友：回想起本科参加电赛和飞思卡尔智能车比赛时，常常混迹于各个电子论坛，为了请教别人或者偶尔被人请教，加了很多好友。印象很深刻的是当时和一个南京农业大学的学长经常会交流到很晚，没有他的帮助，也许也很难在比赛中取得之后的成绩。比赛结束后，两人一起建了个群，为之后两个学校参加比赛的学弟学妹提供帮助，这是我大学中一件很有意义的事情。其实每个做技术的大牛都是从小白一路走过来，都明白在成长路上遇到困惑时求助他人会得到更快的成长，只要不是伸手党，在提问前对一个问题有切实的思考并尝试解决过，“大佬”都会乐于相助。写博客时书写的东西被人点赞关注，是一件值得开心的事情，每一次的点赞与关注都是一种激励。也希望通过这种方式结识各种牛人。 北漂&emsp;&emsp;万万没想到自己会加入“北漂一族”,毕业时选择了相对难走的路,当然是希望自己能成长的更快。来北京的这一个月，磨练最多的就是工作能力，基本每天的工作时间都在12小时以上，有冒着大雨回家，也有晚上12点半还在等快车，深夜1，2点昏昏沉沉的写一天的工作总结。晚上10点后的北京是属于滴滴司机和程序员的，滴滴拼车时能碰到很多其他公司的程序员。每天下楼看见周边的其他楼都灯火通明时，有被震撼到。看见整个环境中的人都在为梦想拼搏，也多少更能激起自己的斗志。在这段时间的工作中也算是参与到了几个项目中，python脚本，C++，opencv，caffe，tensorflow在这段时间都有了更深的理解。无奈分身乏术，总觉得这些技能自己多少都会点，但也只是略懂皮毛，在接下来的时间里要多看论文，精进各种技术。&emsp;&emsp;除了工作外，最重要的当然是生活了。这段时间学会了很多生活技能，修过空调，热水器，也开始自己做饭。内心也比想象中更加强大了，还记得来时的第二个周末，周五的晚上发烧，10点才到家，第二天还要加班，猛灌了两杯热水，第二天醒时也能跟没事一样的去上班。希望自己能如来时想的一样，在各方面都能有更快的成长。&emsp;&emsp;北京有45%的非北京户籍人口，作为一个离梦想最近的城市，北京似乎为每个外来者都提供了希望的种子。这个种子无论怎样成长，在北京待上一段时间都会有属于自己的故事.。在北京待的这一段时间，见到了形形色色的人，有地痞无赖般的“中介”，扬言不给15元的辛苦费就让手机号报废，也有十分热切的提醒我东西掉了的陌生人，还有拿着微薄的工资依然每天斗志昂扬为了梦想奋斗的热血青年。也遇到了很多看起来一样的人，早上都蜂拥而至各个写字楼，晚上都拖着疲惫的身体挤着公交地铁，回到暂时属于自己的那一间屋子。我相信每一个能坚持下来的“北漂”人心中都有着深深的执念，有着对现状的不甘和对美好生活的向往。向那些为梦想为生活长期在这个城市奋斗的人致敬。&emsp;&emsp;而对于我而言，希望以后回想起来北京这个决定，不说后不后悔，只讲在北京有多少收获。 &emsp;&emsp;这篇博客从两周前开始动笔，但无奈最近实在太忙，直到今天才算写完。七夕的晚上，把音乐开到最大声，自嗨的整理完这篇博客也是很无奈。唯一值得慰藉的是工作上提前一周完美完成任务，在周会上被导师和主管说很牛。愿接下来的日子，自己能为自己和别人带来更多惊喜吧。&emsp;&emsp;晚安！2018年8月17日。 欢迎关注我的公众号]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二维图像分割之分式分割]]></title>
    <url>%2Fpost%2F7f9c7afb.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;从上周开始研究各种数学式子的切割，包括分式，竖式和脱式。本着由易到难的原则，开始做分式切割的调研。除了做PPT，写文档外，也将部分调研的结果整理成博文。 图像数学公式定位的关键问题&emsp;&emsp;一般地，公式定位流程如下： 预处理：包括灰度化、二值化、去除噪声和倾斜矫正等步骤 统计版面参数：包括字符的位置、字符的尺寸和相邻字符间的间隔等 版面分析：标注出字符域、图像域、表格域和图形域等 行提取：对字符域以行为单位进行划分 定位孤立数学公式：从提取的行中区分孤立数学公式行 定位内嵌公式：从非孤立数学公式行中区分出含数学公式的文本行 公式定位基本算法 预处理：对图像灰度化、二值化、倾斜校正、去噪等处理，使其更有利于数学公式定位 数学公式字符块提取 1.行提取 进行联通区域搜索，得到图片中所有连通区域 合并具有相交或包含关系的连通区域 根据连通区域宽高度统计直方图得到版面的字符宽、高度阈值threshold_w和threshold_h，据此去除图片中的无关信息，得到公式的候选区域 根据候选字符的连通区域的位置关系，合并候选字符连通区域，提取本文行及相应参数：行间距Line_d，行内相邻连通区域水平间隔平均值threshold_d 2.行内字符块提取:行内字符块提取是根据threshold_d将文本中的字符连通区域合并成字符块3.后处理：数学公式字符块合并。对于分式，若将分数线与分子分母分别切割，可以根据其上下相邻行为单字符或行内字符均处于具有二维运算结构的运算符作用范围内，且两者之间的垂直距离小于行距Line_d的特点将其合并。 &emsp;&emsp;图像倾斜的影响：当倾斜角增加时，数学公式定位准确率急剧下降。这是由于在数学公式定位算法采用连通区域空间位置关系特征提取文本行，很容易受到倾斜影响，进而影响公式定位准确性，因此首先需要对拍照图片进行测斜和校正处理，以确保公式定位算法的鲁棒性。 公式定位错误的校正方法 考虑根据图片中印刷体字符的大小，近似判断拍照的远近，从而确定各个阈值的大小 影响最大的参数有行内相邻连通区域的水平间隔Character_dist,可以考虑将行内连通区域按从左到右排列，然后计算版面中相邻连通区域的间隔，取数量最多的连通区域间隔座位阈值Character_dist。对与行间距阈值Line_d也可以用类似方法 分式分割的难点印刷体&emsp;&emsp;对于印刷体分式，如下图，如同印刷体文本的字符分割算法一样。采取水平投影的方式即可找到分割线。 手写体&emsp;&emsp;分式分割的难点主要集中再手写体分式上。手写体的格式没有印刷体那么规范，分子与分母经常会出现粘合，带分式中也会出现整数与分数线、分数粘合在一起的情况，因此无法直接通过投影拆分分式。 分式分割的处理思路&emsp;&emsp;分式分割的关键点是找到分数线，分数线能作为带分数中整数与分数分割的参考，且能作为分数部分的分割线。因此分数线具有重要的参考价值。 &emsp;&emsp;考虑到手写体中分数线扭曲，各部分粘合的情况调研了许多直线检测的算法。有时间会做系统的整理。在这里线整理几种常用的直线检测算法。 霍夫变换&emsp;&emsp;Hough是最经典也是应用最广泛的直线检测算法，hough使用极坐标的方式表示直线。极坐标下，直线的表达式可定义为：$\left(-\frac{cos\theta}{sin\theta}\right)x+\left(\frac{r}{sin\theta}\right)$，化简可得到$r=xcos\theta+ysin\theta$。对于每一点$(x_0,y_0)$，可以将通过该点的直线定义为$r_\theta=x_0cos\theta+y_0sin\theta$。 &emsp;&emsp;通过以上的推导，意味着每一对极坐标的参数$(r_\theta, \theta)$代表着一条通过$(x_0,y_0)$的直线。对于每个定点$(x_0,y_0)$，画出通过该点的所有直线并以极坐标表示，会得到一条正弦曲线。因此越多的点具有所描绘的正弦曲线相交，意为着这些点能组成平面内的一条直线。&emsp;&emsp;如下图所示，点$x_1=9$,$y_1=4$，点$x_1=12$，$y_1=$以及$x_1=8$,$y_1=6$所描绘的通过它们的所有直线的正弦曲线。 hough的做法是追踪图像中每个点对应曲线间的交点，如果这些交点的数量超过了一点的阈值，即认为交点的参数$(r_\theta, \theta)$在原图像中为一条直线。 &emsp;&emsp;Hough变换的基本思想是利用图像的全局特征将特定形状的边缘连接起来，Hough通过点线的对偶性，将原图像中的点映射到用于累加的参数空间，将在原图像中寻找特定曲线的检测转化为寻找参数空间中的峰值问题。Hough的优点和缺点都来源于全局特征，因为全局特征，Hough提取的曲线受噪声和边界的影响较小，具有较好的鲁棒性，但也会带来效率低的缺点 LSD算法&emsp;&emsp;LSD发表于2012年，算是较新的直线检测算法。与Hough利用全局特征不同，LSD是一种局部提取直线的算法，在线性时间(liner-time)内能得到亚像素级准确度的直线。&emsp;&emsp;LSD算法的流程如下图，LSD算法的核心思想是合并像素生成直线，合并的规则是根据每个像素点的梯度值建立状态列表，并将所有点设置为NOT USED。然后去除列表中梯度最大的点作为直线的第一个点，并将对应的状态设置为USED。再基于区域生长算法，得到line support region。 &emsp;&emsp;上面说了Hough由于全局的属性带来的优缺点，下面也说说LSD这种算法因为局部提取直线带来的缺点。LSD号称是一种无需设置任何参数的算法，但在实际使用中，需要设置采样率，并且区域生长算法中，需要设置梯度角度变化的容忍（tolerance ）值。 由于LSD算法的每个点都有状态值“NOT USED”和“USED”。因此每个点都智能属于一条直线，遇到相交直线时会出现至少一条直线被分割成多条直线的情况。 LSD算法在找寻line support region时，用了区域生长算法的思想。会由于线段间的遮挡和局部模糊导致一条直线被割裂成多条。 &emsp;&emsp;由于是做手写分式分割中的分数线检测，直线往往弯曲而且易被其他线“切断”。因此做实验时，尝试将采样率与区域生长的容忍值调大了不少，让我困惑的是，在我将采样率和容忍值调大后，LSD算法的耗时呈大幅增长，失去了LSD算法的效率优势，这里暂时还不知道是我自己写算法的问题，还是LSD在增大局部搜索范围后效率会显著下降的原因。日后有了确切的结论会来更新，也欢迎大家指正。 &emsp;&emsp;下面贴几张LSD算法在分数检测的效果图，不同颜色的线段代表检测出的不同直线。可以看到对于扭曲小且没相交直线的情况下，LSD有较好的效果，而一旦干扰多了，一条直线会被分割成多条直线。 &emsp;&emsp;关于直线的论文有很多，还有使用图像分割或提取边缘算法得到图像边缘后，使用动态聚类算法聚合线段，再用直线拟合同一聚类中图像。 实验与结论&emsp;&emsp;不得不说，手写分式可能出现各种各样的情况。例如无法有效判断检测到的那条直线为分数线，对于带分式中整数与分子分母贴合紧密的情况，也暂时没有有效的解决思路。 &emsp;&emsp;最近看了很多图像处理的论文以及思路，很多棘手的任务，论文作者都巧妙的用图像处理的算法解决了。相信关于分式分割的处理，也会有相应的解决办法。事情总是要一步一个脚印的处理。先贴一个图，下图是这周做的真分式和假分式的切割算法，在525张手写分式图片中，正确分割了512张，也算是达到了预期的效果。 参考文献[1] 毛星云. OpenCV3编程入门[M]. 电子工业出版社, 2015. [2] Gioi R G V, Jakubowicz J, Morel J M, et al. LSD: A line segment detector[J]. Image Processing on Line, 2012, 2(4):35-55. 欢迎关注我的公众号]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《我不是药神》:今后会越来越好的]]></title>
    <url>%2Fpost%2F172fcfc5.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;最近刷朋友圈，以及和同事聊天总是听到谈论《我不是药神》。本来近期没看电影的打算，趁着周末的闲暇和好奇心驱使，去影院看了这部现象级电影。&emsp;&emsp;电影的剧照是几个主演坐在一家“王子印度神油”的店前开怀大笑，一度以为是部恶搞印度神油的喜剧电影。但事实上，这部电影很多场景都很煽情，映像中好像没哪部电影看的让我感觉眼眶有点湿。说几个我记得的点 吕受益随身带着橘子，这是因为橘子富含维C，据说有抗癌的作用，而且便宜不用削皮，但橘子除了少数成熟的月份以外都很酸，可想活命也只能如此。 从散伙饭，到吕受益去世才短短一年的时间，乐观向上的一个人就这样被疾病迅速摧毁，让人心疼。 吕受益死后的房门外，黄毛吃的干巴巴的橘子，应该是吕受益生前给他的。 一帮病友被带到警局，要求供出药贩子信息时，一位老奶奶苦苦哀求警察曹斌不要追究药贩子的对话。 最感人的是程勇整个人的升华，从最初唯利是图的商人模样，到最后不赚钱卖药，送走儿子后，一心打算卖药到被抓为止的心态转变。 &emsp;&emsp;电影中的很多细节做的也无比用心，片中的人民币有新版的红色百元，也有符合那个年代的老版蓝白色百元。程勇平时都抽纸烟，唯独曹斌去的时候抽起了雪茄，口口声声说自己每月赚几十万，但应该赚不到这么多。黄毛不会开车，为了让程勇不被抓，起步时摇摇晃晃开着车去冲警察的追捕线。吕受益去世前后，房子都不一样了，在说明治病花去了很多钱。&emsp;&emsp;不得不说这是一部很棒的电影，整个电影很沉重，但也适时的加入了一些包袱。电影中的瑞士公司高层无疑是一个负面形象，面对百姓抗议药价太高时，理直气壮的说定价合理合法，而印度公司生产的仿制药在药效几乎一样的情况下，其费用仅为瑞士药的1/80。看电影时也不由的觉得印度真是良心公司，而瑞士药的高昂价格无异于是将白血病人当成了勒索对象。但电影的冲突与矛盾是非常强烈的。有中国那时进口药价格高昂引起的社会矛盾，正版药与仿制药的矛盾，还有人情与法理的矛盾。&emsp;&emsp;有一个学制药的朋友和我讲过研制一款药有多难，看了电影后也去查了下。 根据 2016 年德勤会计事务所发布的一份研究报告，从其对 12 家大型药企的持续 6 年的追踪结果可看出，研发巨头的投资回报率从 2010 年的 10.1% 下降至 2016 年的 3.7%。与此同时，研发一个新药的平均成本已经从低于 12 亿美元增长至 15.4 亿美元，而且需要耗时 14 年才能推出一个新药。 &emsp;&emsp;这个费用还没有算上审批通过后，药企研究剂量强度、配方和新的适应症的后续资金投入。制药企业的目的之一当然是为了盈利，在高昂的药价研制费用加上十多年的人力物力投入。而且药物的专利期一般是20年，药企只能在20年内把巨额的药物研发成本赚回来。而且对于罕见的病，由于患者少，企业为了追求利润，高额的定价似乎也是没办法的事。除此之外，中国对于进口药必须进行临床试验，一款药临床试验的成本在300-700万美元。累加的成本当然由患者买单。都知道香港的很多进口商品会便宜很多，一些药品也类似，2017年，在北大卫生经济论坛上，国家发改委价格司副巡视员郭剑英解释了为什么大陆80%的进口原研药价格会高于香港 香港没有5%的关税和17%的增值税，没有15%的医院加价，流转费用也不会达到20%多 &emsp;&emsp;电影的最后字幕，向观众介绍了今年4月25日，中国取消了28种进口药的关税。但这部分只占5%。除了以上的层层加价外，在中国，过了20年专利保护期的原研药仍然拥有自主定价权。不需要遵循政府的指导定价。 在药品的使用上，大部分二级以上医疗机构执行了《处方管理规定》中的“一品两规”，也就是说，同一种药物，只使用两个厂家的产品。在实际操作中，往往就会变成一个国产品种和一个进口品种。国内的药厂有6000多个，竞争激烈，而进口药物缺乏竞争，等于保证了自己的销路。 &emsp;&emsp;这部电影的总体口碑很好，评价负面的人多是说妖魔化药企，看这部电影时，有让我想起韩国的一些反映社会问题的电影，但毕竟不像韩国电影什么内容都能拍，在国内的审查制度下，这样一部电影能即叫好又叫座而且能唤起民众的反思已实属不易。最后关于医疗，愿如电影中所说“今后会越来越好的”。]]></content>
      <categories>
        <category>影评</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>影评</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[毕业]]></title>
    <url>%2Fpost%2F9b1b667b.html</url>
    <content type="text"><![CDATA[从最初面红，到现在双眼通红 &emsp;&emsp;从研究生入学到毕业前，我一直觉得读研期间，没有本科那样像“学生”，在我的潜意识里，“学生”时代有上不完的课，考不完的试，关于学习的讨论也是在课堂和题目上。而读研期间，半学期紧张的课程和考试结束后，就再也没有了必须要去上的课和必须要完成的考试。即使我早就知道研究生要培养自己的自主学习能力，我还是会去怀念那时候的“学生”时代。可直到今天，往常的收发快递点门前待寄包裹堆积如山；互相询问离校时间，筹划实验室和寝室的散伙饭；导师对我常说的话由“抓紧时间”变成了祝福的话语。拿到学位证的欣喜与即将离开学校的失落交杂其中。我才知道，这两年我所经历的才是最值得缅怀的。&emsp;&emsp;研究生的两年出现了许多足以影响我一生的人和事。 我的导师&emsp;&emsp;读研期间最该感谢的人就是我的导师钟珞教授。入学前就不断有耳闻各个学校研究生导师的负面新闻，读过研的哥哥也告诉我，他所知道的导师压榨学生的事情。本想选个年轻导师，跟着一起多做项目的我却偏偏选了学院最有资历的老院长作为导师。从入学起就能感觉到钟老师对我的关照，研一时虽然每次都只是周一开例会才会见面，但开会时，聚餐时都能感觉到钟老师对我的关照。从研一到研二有个转变，开始变得喜欢开会，也更多的找导师讨论大论文和小论文的事。整理寝室时才发现，上一篇小论文打印出来的修改稿有6份。研二开始更觉得导师像是朋友，讨论论文之余，也会聊聊家事和导师读研读博时期的一些趣事。相比较于学业上的帮助，更多的收获是关于做人做事的一些道理，钟老师用自己的亲身经历对我的指导让我受益匪浅。 关于读博&emsp;&emsp;虽然没有读博，但这也算是一个会改变一生的决定。导师在研二下时和我说过几次读博士的事情，与企事业工作的优缺点做对比。心里一方面很感激导师对我的认同。可能很早给自己定的目标就是最多读到硕士，便没有读博士的打算。而且自认为自己不能静下心来4，5年坐在实验室里写论文 。自作主张的我没有和家里商量读博士的事，快毕业说起这事才知道父母很赞同我继续读下去。想着商量了或者早点去考虑这事会变得不一样，不得不说心里会有点失落。以后如果心态不一样了，会去考虑这个事。 实习与工作&emsp;&emsp;研一下学期找实习之前，只是抱着试一试的心态投了华为一家公司，后续也没有做其他公司的笔试面试。很幸运的通过了华为的笔面试，怀着对大公司的向往和导师商量了去实习的事情。去了后感到遗憾的是岗位不是我喜欢的，也错过了不少公司的提前批招聘，尽管华为在最后录用阶段做的很让人糟心。但去华为那段时间认识了不少好朋友，在深圳也度过了整个研究生期间最开心的时光，现在想起来，是我研究生期间最大的遗憾了，后来做选择时，竟有点没勇气再去面对这座城市，我还是很感激17年的那个夏天。&emsp;&emsp;实习结束回来找工作，我心里想的是深圳或者广州的大公司，心里面是抵触去北京的。投的一些北京的公司，是想着多涨些笔面试经验。可最后一连串的事情，我也说不清楚怎么会最后签约北京的公司。我始终觉得做每一个决定都是复杂的，只言片语或者我刻意去想都多少会觉得有点不合适。但正是这一个个因素，最后促使了我这个决定。想起某个多少会有遗憾的决定时，我会经常问自己“后悔吗？”，我的回答一直是“多少有点”。每一个会让我感到遗憾的决定，我都会再三犹豫与挣扎，可我真的尽力了。我只能安慰自己，再来一次，我的“决定”还会是这样。 关于多肉&emsp;&emsp;我想我对多肉有一种特殊的感情，在学校折腾了两天。回到家的第一件事情就是为“观音莲”安置新家。我很抱歉，因为我怕麻烦，说了两年要为你安置大一点的花盆，直到我快要走了，才做到。希望你茁壮成长，无忧无虑。 &emsp;&emsp;毕业前总是感慨万千，再过两天就要上班了，祝自己一切顺利。感恩这两年所有的人与事。 欢迎关注我的公众号]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[极简人类史]]></title>
    <url>%2Fpost%2Fad14e7f5.html</url>
    <content type="text"><![CDATA[写在前面最近一段时间开始对历史感兴趣起来，看历史的时候能获得一些力量和安慰，一段历史的厚重感和其中的伟人能让人肃然起敬，现在面临的一些不顺与一段历史中被命运捉弄的人来说也变得不值一提。趁着即将工作的间隙，追完了一部电视剧《大明王朝1566》，看完了《极简人类史》和另一本还在看的《人类简史》。《大明王朝1566》这部剧给我的感受是厚重且华丽，是我看过的最棒的国产剧了。剧中大量运用了黑白 闪回，与前一刻的剧情形成反差，或重复加强的效果，有不少闪回表现的人物内心活动。有一篇关于该剧的评论《大明王朝1566》：太极·利剑·雪，太极那部分分析的很好。《极简人类史》以时间为轴线，主要介绍了人类发展历史中的三个阶段，脉络清晰，书中没有细讲一个国家或一位伟人，但描述了人类历史一步步的发展轨迹，讲述的是一部简洁易懂的人类历史。 前传宇宙史是比人类历史更大的一个范畴。这一部分书中也只是简短的介绍，关于这部分看书中的描述只能有个大致的时间概念和各阶段发生的事情。有许多制作精良的纪录片绝对能带来比看书更好的体验。这里放一个简短的科普视频。 采集狩猎时代首先看看采集狩猎的定义： 采集狩猎时代是人类历史中这样一个时代：整个人类社会依靠采集或狩猎，而不是通过种植或制造，来获取食物和其他必需品。此时的人类被称为“采集狩猎者”。这个时代也被称作“旧石器时代”。采集狩猎时代是人类历史上的第一个时代，也是迄今为止最长的时代，这是为人类历史奠定基础的时代。采集狩猎者始于25万年前，独特的文化和技术创新，将他们的生活方式与其他非人类物种区分开来 采集狩猎时代距今很遥远，研究那个时代的学者采用三种截然不同的证据： 远古社会留下的物质遗迹。如石器、制作品或者事物残渣。如对牙齿的细致研究，可以获知早期人类日常的饮食信息；男女之间骨骼的大小，可以反映两性关系；研究海床和数万年前形成的冰盖中提取的划分和果核样本，考古学家能重构当时的气候和环境变化模型。 研究现代采集狩猎部落。 基于现代基因差异进行对比研究。基因研究可以测定现代族群之间的基因差异程度，帮助预估自己族群的历史。 采集狩猎的生活方式 生产力水平低下的，那时人类每天从自然环境中获取的热量很难超过3000卡路里，而这是一个成年人类维持基本生存所必须的能量。 人口密度低，平均每平方公里不足1人。我查阅了资料，中国人口平均密度目前是每平方公里130人，但人口分布及不平衡。东部沿海地区，每平方公里超过400人，中部地区每平方公里200多人，而西部高原地区，每平方公里不足10人。香港旺角是世界人口最密集的地方，每平方公里有13万多人！！！（真正意义上的寸土寸金）。亲缘关系狩猎时代几乎所有的人类部族都鼓励与外族通婚，能确保邻近族群之间的团结意识和语言之间的相互重叠。生活水平与如今的人们没有私有财产就是贫穷的标志不同，采集狩猎时代随时从周围环境获取生存所需的的物质，不积累财富。因此生活在温带地区的采集狩猎者生活水平相对较高，因为他们饮食多种多样，免受饥荒的困扰。生活闲适，但生命短暂采集狩猎者居住的小型社会使他们和流行疾病隔离开，频繁的迁移活动也避免了招致病害虫的垃圾堆积。但生活艰苦，平均寿命可能低于30岁左右（由于婴儿死亡率高，意外事故和人为暴力）。最终，采集狩猎时代技术发展的足够高潮，使某些地区的一些部落能更加深入，集中的利用当地资源。标志着迈向农耕社会。采集狩猎时代的重大变革 技术变革：出现了新的石器 向非洲以外地区迁徙：向东向西迁徙，来到亚欧大陆更偏南、更温暖的地区。这里面有几个大事件，一个是距今5.5万年至4万年，人类出现在冰河时代的澳大利亚，这被视为技术创新的明显标志，因为抵达澳大利亚大陆需要高超复杂的航海技术。还有一个是距今3万年前出现在西伯利亚，在这里生存需要捕获大型哺乳动物（鹿、马和猛犸），表明掌握了高超的狩猎技术。 人类对环境的影响：许多大型动物灭绝，刀耕火种 人类历史的开端关于人类历史开端的问题，一直有着争议。主要存在两种假说： 多地起源模式，这种模式的证据来自对骨骼遗迹的对比研究。 走出非洲假说，主要依赖于现代人类的基因对比。 作为一个没有查阅过人类起源的门外汉，如今听到的比较多的关于人类起源的说法是“走出非洲假说”，得益于基因检测技术的发展，这种假说的可信度越来越高。 农耕时代距今1.1万年至1万年前，农耕社会诞生了。直到近250年，工业革命的开始，农耕社会才走向消亡。虽然和长达25万年的狩猎时代相比，农耕时代才延续了1万年。但迄今为止，70%的人类成员都生活在农耕社会。从生态学讲，农业能比采集狩猎更有效率地获取自然界通过光合作用存储的能量与资源。农业通过砍伐森林、使河流改道、开垦山坡和耕种土地，农业耕种者极大地改变了地球的面貌，使其变得更受人类活动控制。 农耕时代的最早证据迄今为止，对农业诞生的源头仍缺乏令人信服的解释。有趣的是在直至公元前1500年，几个“世界区域”——非洲、亚欧大陆、美洲和澳大利亚记忆太平洋各岛屿完全没有联系，但却相继进入了农耕时代。正是大家在没有交流的情况下，相继进入了农耕时代，这推翻了农业是一项绝妙发明的观点。而且现代的狩猎者反对进入农耕时代，有种猜想是早期的农耕者并非心甘情愿的接受这种生活方式，而是被迫接受！ 总体特点和长期趋势农耕时代具有超乎寻常的文化多样性，农耕部落之间共享着一些重要的特征，这些特征确保了农耕时代的延续。 以村庄为基础的社会构成：都需要家庭内部和家庭之间的协同合作，都需要处理与外部族群之间的关系。 人口活力增强：世界人口由1万年前的600万增长到1750年现代社会初期的7.7亿 技术创新加速：本地人口压力、新环境的扩张和不断增长的思想和贸易交流促使农耕技术不断进步。 农副产品革命：纤维、奶和肥料 水利技术 流行性疾病：相比于采集狩猎时代没有流行疾病的优势，农业时代的定居且人口多、与牲畜的密切接触导致流行性疾病的产生。 权力等级：为了控制不断增加的宝贵粮食库存，冲突时有发生，导致了新形势的社会不公，形成了新的权力体系。 城市出现之前的农业社会这个时代已经有了农耕部落，但尚未出现大型城市和国家。在非洲和亚欧大陆地区，这一时期从约公元前8000年延续到公元前3000年。在美洲，这一时期开始的晚，持续时间也更长。太平洋和太平洋岛屿，这一时期延续至现代。 村庄组成的世界 等级制度出现：由于部落的扩大，人们需要定义自己与邻里关系。司法、战事、贸易和宗教等都需要人管理。有着政治和经济制度。 早期妇女地位限制：女性通常没有机会承担专业化的角色，随着部落间竞争加剧，男性开始垄断暴力组织。 最早的城市和国家公元前3000年到公元前500年才是人类历史真正的开始时期。在非洲和亚欧低地区第一批城市和国家出现在公元前3000年左右，美洲出现在公元前1000年，而大洋洲在距今1000年左右，国家才出现在一些海岛（夏威夷和汤加）。国家出现的首要原因是不断增加的人口密度。 农耕文明：随着国家模式的不断扩张，与其相关的制度和实践也固定下来，称为“农耕文明” 帝制国家：随着国家规模的扩大，独裁者掌控的众多城镇区域内形成了帝国体制。通过地方统治者直接或间接扩大了征税和管辖的区域。 农业、城市与帝国在公元前500到1000年，随着世界各国人口增多、国家势力和数量的不断增长，交换网络的范围扩大。这期间诞生了很多王朝。 非洲，亚欧大陆，最早的帝国是创建于公元前6世纪的波斯（现伊朗）王朝。该王朝掌控的区域达到其过往朝代最大疆域面积的5倍。在此后的 1500年里，类似规模的国家被称为帝国。 美洲，公元后第一个千年里，复杂的城邦体制与初创的帝国出现在中美洲。处于鼎盛时期的墨西哥特奥蒂瓦坎城，拥有超过10万人口，控制着跨越中美洲大部分地区的贸易网络。 农耕文明以外的地带，人口增长促使了新的阶层结构产生。在亚欧大陆人烟稀少的地区，匈奴人于公元前2世纪创立了帝国。 现代革命前夕的农业社会农耕时代的最后一个阶段，是1000年至1750年。该时期农耕文明传播到以往边缘化的区域，例如北美洲、非洲南部、中国西部地区。这一时期最为重要的变化就是世界主要地区在16世纪实现了统一。在此基础上，第一个全球交换网络诞生了。将数千年来从未来往的区域联系在一起，形成商业和知识协同，为现代社会的兴起发挥了至关重要的作用。 我们的世界——近现代在人类史的三个时代中，近现代才维持了250年，确实最动荡不安的。近现代的主要特征如下: 人口增长和生产力提高：1750年至2000年间，世界人口从7.7亿左右增长到近60亿，人均生产量也提高了9倍。 城镇扩展：在1500年，全球只有约50个城市剧名人口超过10万，到2000年，数千个城市的居民人口超过10万。 日益复杂和强大的政府：人口的增长及人们间的相互关系，需要更复杂的管理方式运作。 日益增大的贫富差距 女性享有更多机遇 前现代生活方式的消亡：采集狩猎和农业耕作的生活方式都走向没落。工业革命工业革命从 1750年到1914年。起源于苏格兰发明家詹姆斯·瓦特改良的蒸汽机以及第一列机车。工业革命的技术创新呈现波浪式发展态势，每一波都带来了新的生产力提升技术，并将工业化进程扩展到新的区域。工业革命带来了以下改变： 经济发展：从1820年至1913年，英国国内生产总值增长了6倍，德国增长了9倍，美国增长了41倍。与此同时，中国和印度等传统农耕社会受到了冲击，中国国内生产总值在世界的占比从33%下降至9%，印度从16%降至8%。 民主革命：经济基础决定上层建筑，产生了如法国大革命等变革。 文化变革：北美及欧洲大部分地区，大众教育将读写能力传授给大多数民众。所有的宗教传统此时都必须直面现代科学提出的挑战，例如达尔文提出的进化论对宗教的冲击。 20世纪危机从1913到1950年间，世界经济增长缓慢，曾经促进工业革命发展的国际金融业和贸易体系的崩溃是增速减缓的部分原因。各国将经济增长视为零和博弈，排挤市场中其他竞争对手。随后爆发了第一次世界大战，第一次世界大战将工业化战争的惊人规模和破坏力展现得淋漓尽致。在第一次世界大战后，德国出现了以西特勒为首的法西斯政权，俄国出现了由马克思主义指导，决心推翻资本主义的社会主义国家。20世纪30年代期间，第二次世界大战起源于日本和德国妄图创建各自的陆上帝国。第二次世界大战后，欧洲不再主导全球经济体系，美国和苏联成为新的超级大国。 现代历史现代历史从1945年至今，第二次世界大战后，资本主义引擎再次轰鸣，早就了世界历史上最快的经济增速。美国的“马歇尔计划”提供了大规模 的重建援助资金，推动了全球 监管机构，如联合国（1945年）和国际货币基金组织（1947年）的成立，国际经济秩序回复 了稳定。在1945年后的40年间，大约有100个国家从欧洲领主手中取得了 独立，另一批新兴国家涌现于1991年苏联解体之后。最后做一个总结： 欢迎关注我的公众号]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>历史</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Deep Facial Expression Recognition:A Survey》论文笔记]]></title>
    <url>%2Fpost%2F5d962f61.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;论文链接：https://arxiv.org/abs/1804.08348&emsp;&emsp;这篇文章1是北邮的邓伟洪教授关于深度人脸表情识别(Deep Facial Expression Recognition,DFER) (情感识别)的一篇综述性文章，该文章被计算机视觉顶会CVPR收录。对于像我这样对情感识别感兴趣，但又没做过具体应用的小白来说研读这篇文章再合适不过了。 介绍&emsp;&emsp;表情可以说是一门世界语，不分国界、种族以及性别，可以说所有人都有着通用的表情。FEP在机器人、医疗、驾驶员驾驶疲劳检测和人机交互系统中都有广泛应用，最早在20世纪，Ekman和Friesen通过跨文化研究，定义了6种基础表情：生气、害怕、厌恶、开心、悲伤和吃惊，随后又加入了“蔑视” 这一表情。开创性的工作和直观的定义，使该模型在自动人脸表情识别(automatic facial expression analysis, AFEA)中依然很流行。&emsp;&emsp;根据特征表示，FER系统可以划分为图片FER和视频FER两类。图片FER只提取当前图片的特征，而视频需要考虑相邻帧之间的关系。实际上所有计算机视觉的任务的处理对象都可以划分为图片和视频两类。&emsp;&emsp;FER传统的方式使用手工提取的特征和浅层学习，这种方式的弊端就不多赘述了。得益于深度学习的发展和更具有挑战性的数据集FER2013的出现，越来越多的研究者将深度学习技术运用到FER中。 深度人脸表情识别&emsp;&emsp;这一节讨论了深度学习在人脸表情识别应用上的三个步骤。分别是预处理、特征提取和特征分类，简述了每一步的具体做法，并引用了相关论文。 预处理人脸对齐&emsp;&emsp;给定一个数据集，第一步是移除与人脸不相关的背景和非人脸区域。ViolaJones(V&amp;J)人脸检测器2 (在OpenCV和Matlab中都有实现)，该检测器能将原始图片裁剪以获得人脸区域，&emsp;&emsp;第二步是面对齐，这一步至关重要，因为可以减少人脸尺度改变和旋转产生的影响。最常用的面部对齐的实现是IntraFace3,IntraFace采用SDM算法，定位出49个面部特征点（双眼、两个眉毛、鼻子和嘴巴） 数据增强&emsp;&emsp;数据增强包括在线和离线两种方式： 离线方式：随机扰动，图像变换（旋转、评议、翻转、缩放和对齐），添加噪声（椒盐噪声和斑点噪声），以及调整亮度和饱和度，以及在眼睛之间添加2维高斯分布的噪声。此外，还有用对抗神经网络GAN4生成脸，3DCNN辅助AUs生成表情。使用GAN生成脸对网络性能是否有提升还没有验证过。 在线方式：包含在训练时，裁剪图片，水平翻转。主要是通过随机扰动训练模型。人脸归一化&emsp;&emsp;人脸的光照和头部姿势变化会削弱训练模型的性能，有两种脸部归一化的策略削弱影响，分别是亮度归一化和姿态归一化。 亮度归一化:Inface 工具箱5是最常用的光照不变人脸检测箱。除了直观的调整亮度以外，还有对比度调整。常见的对比度调整方法有直方图归一化、DCT归一化、Dog归一化。 姿态归一化：这是一个棘手的问题，目前的方法都不太理想。有2D的landmark对齐,3Dlandmark对齐，有通过图像和相机参数估计，也有通过深度传感器测量然后计算出来。比较新的模型都是基于GAN的，有FF-GAN、TP-GAN和DR-GAN。 深度特征学习&emsp;&emsp;这一部分主要讲的是使用深度学习模型提取特征，包括卷积神经网络(Convolutional neural network，CNN)、深度置信网络（Deep belief network ，DBN）、深度自动编码器(Deep autoencoder，DAN)和递归神经网络(Recurrent neural network，RNN)。深度人脸表情识别的流程如下，下图可以看出，深度网络模型部分有四种常用的模型。作者只是简单的介绍了几种网络模型，在这里我也不过多的赘述。CNN模型在我前几篇博文卷积神经网络的结构与相关算法和卷积神经网络模型解读汇总——LeNet5，AlexNet、ZFNet、VGG16、GoogLeNet和ResNet有详细介绍。其余的网络模型以后有时间会逐一整理。 人脸表情分类&emsp;&emsp;完成了特征提取后，最后一步对其进行分类。在传统FER系统中，特征提取和特征分类是独立的。而深度学习的FER是端到端的模型，可以在网络的末端添加损失层调节反向传播的误差，预测概率可以由网络直接输出。也可以将两者结合，即用深度学习提取特征，再用SVM等分类器分类。 面部表情数据库&emsp;&emsp;该部分总结了FER可用的公开数据集。 CK+:包括123个subjects, 593 个 image sequence。该数据库由118名受试者录制，在这593个image sequence中，有327个sequence 有 emotion的 label。除了中性外包含7种表情：愤怒、蔑视、厌恶、恐惧、高兴、悲伤和惊讶。 MMI：包括32个subjects，326个image sequence。213个sequence 有 emotion的 label。包含6中表情（相比较于CK+没有蔑视），MMI更具挑战性，因为很多人都戴有配饰。 JAFFE：包含213副（每幅图像分辨率为256*256）日本女性的脸部图像，包含7种表情。该数据库均为正面脸相，且对原始图像进行了调整和修剪，光照均为正面光源，但光照强度有差异。 TFD：改数据库是几个面部表情数据集的集合，TFD包含112234张图片(每张图片被调整到48*48大小)，所有实验对象的眼睛都是相同的距离。其中4189张有标注，包含7种表情。 FER2013：改数据库通过谷歌图片API自动收集，数据库中所有图片都修正了标签，将图片调整到48*48大小。包含28709张训练图像，3589张测试图像，包含7种表情。 AFEW：AFEW数据集为Emotion Recognition In The Wild Challenge (EmotiW)系列情感识别挑战赛使用的数据集，该比赛从2013开始每年举办一次。 该数据集的内容是从电影中剪辑的包含表情的视频片段，包含7类表情。训练集、验证集和测试集分别包含773、383和653sample。 SFEW：该数据集是从AFEW数据集中抽取的有表情的静态帧，包含7类表情。训练集、验证集和测试集分别包含958、436和372sample。 Multi-PIE：包含4个场景9种光照条件15个视角下337个subject，总计有755370张图片。包含6种表情（没有蔑视） BU-3DFE：从100个人获取的606个面部表情sequence，包含6种表情（没有蔑视），多用于三维面部表情分析。 Oulu-CASIA：80个没被标记的subject收集了2880个image sequence。包含6种表情（没有蔑视）。有红外（NIR）和可见光（VIS）两种摄像头在3种不同光照条件下拍摄。 RaFD：包含67个subject的1608张图片，眼睛有不同的三种注视方向，包括前、左和右。包含7种表情。 KDEF:最初用于医学和心理学研究。数据集来自70个演员从5个角度的6种表情。 EmotioNet：包含从网上收集到的接近100万张面部表情图片。 RAF-DB：包含从网上收集的29672张面部图像，包含7中基本表情和11种复合表情。 AffectNet：包含从网上收集的100多万张面部图像，其中45万张图片手工标注为7种表情。 FER目前发展水平&emsp;&emsp;总结了基于静态图像和动态图像序列(视频)的FER进展。 静态图像FER进展&emsp;&emsp;对于每一个数据集，下表显示了目前最优异的方法，在该数据集上取得的效果。 预训练和微调&emsp;&emsp;在相对较小的数据集上直接训练深度网络很容易导致过拟合。为了缓解这个问题，许多研究会在大数据集上先预训练网络，或者对已经训练好的网络进行微调。 &emsp;&emsp;如上图所示，先在ImageNet数据集上训练，然后再在具体的人脸表情数据集上微调。微调有较好的效果，人脸表情识别有各种微调方式，比如分级、固定某些曾，不同网络层用不同数据集微调，具体可以看看原文中所引用的论文。&emsp;&emsp;此外，文献6指出FR和FER数据集存在巨大差异，人脸似乎别模型弱化了人脸表情的差异，提出了FaceNet2ExpNet网络消除这种影响。该模型分为两个阶段，首先用人脸识别模型提取特征，然后用表情识别网络消除人脸识别模型带来的情绪差异弱化。如下图所示。 多样化网络输入&emsp;&emsp;传统的做法是使用原始的RGB图像作为网络的输入，然而原始数据缺乏重要的信息，如纹理信息，以及图像缩放、旋转、遮挡和光照等方面的不变性。因此可以借助一些手工设计的特征。如SIFT、LBP、MBP、AGEhe NCDV等。PCA可以裁剪出五官进行特征学习而不是整个脸部等。 辅助块与层改进&emsp;&emsp;基于经典的CNN架构，有些研究设计了良好的辅助模块或者改进了网络层，这部分文中有列举几个例子，感兴趣可以找出相关论文翻看。&emsp;&emsp;值得注意的是，Softmax在表情识别领域的表现不太理想。这是由于表情的类间区分度较低。作者整理了几种针对表情分类层的改进。 受到center loss的启发，对特征与相应的类距离加了惩罚项，这分为两种 一种是增加类间距离的island loss7，如下图所示 另一种是减下类内距离的LP8 loss,使同一类的局部相邻特征结合在一起。 基于triplet-loss，关于triplet-loss的想法可以参考原文和这篇博文。 exponential triplet-based loss(增加困难样本的权重) (N+M)-tupes cluster loss(降低anchor的选择难度，以及阈值化triplet不等式),如下图所示。网络集成&emsp;&emsp;之前的研究表明，多个网络的集合可以比单个网络表现的更好。在网络集成时，要考虑两点： 网络模型要有充分的多样性，以确保网络之间具有互补性 要有可靠的集成算法 &emsp;&emsp;关于第一点，网络的多样性产生有很多方法，不同的训练数据、不同的预处理方式、不同的网络模型、不同的参数都能产生不同的网络。&emsp;&emsp;关于第二点集成算法。这其中也主要有两点，一个是特征集成，另一个是输出的决策集成。特征集成最常见的做法是将不同网络模型的特征直接链接，还有如下图的方式 &emsp;&emsp;关于决策集成采用投票的机制，不同网络有不同的权重。关于决策集成的几种策略如下表所示。 多任务网络目前许多网络都是单一任务的输出，但在现实中，往往需要考虑其他多种因素的作用。多任务模型能从其他任务中学习到额外的信息有助于提高网络的泛化能力。关于多任务模型的好处，可以参考这篇博文。如下如所示，在MSCNN9模型中将脸部验证与表情识别两个任务集成在一个网络中。 网络级联&emsp;&emsp;在级联网络中，将不同模块处理不同的任务组合在一起设计一个更深层的网络，前一个模块的输出被后一个模块使用。如下图所示，在AUDN网络中，该网络由三部分组成。 动态图像序列FER进展&emsp;&emsp;基于动态的表情识别相比静态图片能更全面，这里指的动态图像序列，即在视频中。 帧聚合&emsp;&emsp;考虑到表情在不同时刻有不同的变化，但又不可能单独的统计每帧的结果作为输出，因此需要对一段帧序列给出一个识别结果，这就需要用到帧聚合。即用一个特征向量表示这一段时间序列。与集成算法类似，帧聚合有有两类，分别是决策级帧聚合和特征级帧聚合。这两部分感兴趣的可以参看论文。 强度表达网络&emsp;&emsp;在视频中表情会有微妙的变化，而强度是指在视频中，所有帧表现某个表情的程度。一般在中间位置最能表达某个表情，即为强度峰值。大多数方法，都关注峰值附近而忽略了开始和结束时的低谷帧。这部分，主要介绍几个深度网络，输入是具有一定强度信息的样本序列，输出是某一个类表情中不同强度帧之间的相关性结果。如PPDN（peak-piloted），用以内在表情序列里帧之间相关性识别，还有基于PPDN的级联PPDN网络DCPN，具有更深更强的识别能力。虽然，这些网络，都考虑了一段序列里的表情变换，甚至为了计算表情的变化趋势，设计了不同的损失函数，但是，真心觉得，这种代价，对于工程来说，其实是没有意义的。有兴趣的，可以看看论文里对应的方法，这里不再赘述了。 深度时空FER网络&emsp;&emsp;前面介绍的帧聚合和强度表达网络都属于传统的结构化流程，而在视频中将一些列帧作为单独的图像序列输入，输出某一类表情的分类结果。而RNN网络能利用”序列信息”，所以视频FER模型用RNN网络，还有C#D: RNN: 从理论上讲，它可以利用任意长序列的信息,RNN呢能对时间序列上的变化建模。 C3D: 在通常图像上的2D空间卷积的基础上，沿着时间轴加了一个时间维度，就形成了3D时空卷积。例如3DCNN-DAP10，网络模型如下图所示。 &emsp;&emsp;还有种“暴力”做法，不考虑时间维度，将帧序列拼接成大向量，再进行CNN分类，如DTAN11。 面部landmark运动轨迹：通过研究五官的变化轨迹，进而分析表情的变化，如深度几何空间网络(deep temporal geometry network，DTGN)。该方法联合每帧landmark的x,y坐标值，归一化处理后，将landmark作为一个运动轨迹维度，或者或者计算landmark特征点的成对L2距离特征，以及基于PHRNN用于获取帧内的空间变化信息。还有根据五官将landmark点分成4块，输入到BRNNs，定位局部特征，如下图： 级联网络：跟之前静态图像的级联网络思路一样，主要是CNN提取特征，级联RNN做序列特征分类。如LRCN，级联CNN与LSTM，类似的，还有级联DAE作为特征提取，LSTM进行分类，还有ResNet-LSTM,即在低级CNN层，直接用LSTM连接序列之间的低级CNN特征，3DIR用LSTM作为一个单元构建了一个3D Inception-ResNet特征层，其他还有很多类似的级联网络，包括，用CRFs代替了LSTM等等。 网络集成：如两路CNN网络模型用于行为识别，一路用多帧数据的稠密光流训练获取时间信息，一路用于单帧图像特征学习，最后融合两路CNN的输出。还有多通道训练，如一通道用于自然脸和表情脸之间的光流信息训练，一路用于脸部表情特征训练，然后用三种融合策略，平均融合，基于SVM融合，基于DNN融合。也有基于PHRNN时间网络和MSCNN空间网络相结合来提取局部整体关系，几何变化以及静动态信息。除了融合，也有联合训练的，如DTAN和DTGN联合fineturn训练。 &emsp;&emsp;目前各个数据集上，动态序列的表情识别的最佳效果如下表所示： 参考文献 1. Li S, Deng W. Deep Facial Expression Recognition: A Survey[J]. 2018. &#8617; 2. Viola P, Jones M. Rapid object detection using a boosted cascade of simple features[J]. Proc Cvpr, 2001, 1:511. &#8617; 3. Torre F D L, Chu W S, Xiong X, et al. IntraFace[C]// IEEE International Conference and Workshops on Automatic Face and Gesture Recognition. IEEE, 2015:1-8. &#8617; 4. Goodfellow I J, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets[C]// International Conference on Neural Information Processing Systems. MIT Press, 2014:2672-2680. &#8617; 5. http://luks.fe.uni-lj.si/sl/osebje/vitomir/face tools/INFace/ &#8617; 6. Ding H, Zhou S K, Chellappa R. FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition[J]. 2016:118-126. &#8617; 7. Cai J, Meng Z, Khan A S, et al. Island Loss for Learning Discriminative Features in Facial Expression Recognition[J]. 2017. &#8617; 8. Li S, Deng W, Du J P. Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2017:2584-2593. &#8617; 9. Zhang K, Huang Y, Du Y, et al. Facial Expression Recognition Based on Deep Evolutional Spatial-Temporal Networks[J]. IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 2017, PP(99):1-1. &#8617; 10. Liu M, Li S, Shan S, et al. Deeply Learning Deformable Facial Action Parts Model for Dynamic Expression Analysis[M]// Computer Vision — ACCV 2014. Springer International Publishing, 2014:143-157. &#8617; 11. Jung H, Lee S, Yim J, et al. Joint Fine-Tuning in Deep Neural Networks for Facial Expression Recognition[C]// IEEE International Conference on Computer Vision. IEEE, 2016:2983-2991. &#8617; 欢迎关注我的公众号]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>CVPR</tag>
        <tag>FER</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学公式语法——Mathjax教程]]></title>
    <url>%2Fpost%2F8611e6fb.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;在着手写博客前，喜欢在“印象笔记”上记录学习笔记，当时觉得“印象笔记”的富文本编辑器用着还挺顺手。在搭建博客开始学着用Markdown写作后，再看原来在“印象笔记”中的笔记，格式排版真是惨不忍睹，Markdown的使用很大程度上提升了写作效率，也统一了排版。这里顺便推荐一款Markdown的编辑器——小书匠，小书匠支持标准的Markdown语法，也具有强大的语法扩展功能，支持大多数图床和“印象笔记”等第三方存储。&emsp;&emsp;在上一篇博客《Relation Networks for Object Detection》论文笔记中由于论文中有不少公式需要介绍，又不想用图片代替影响阅读体验，好在Markdown支持Mathjax语法。但不得不说刚开始使用Mathjax编辑公式，还是很不习惯，几千字的博文，公式编辑花了很长时间。因此用这篇博文总结一下Mathjax的语法，搬砖的过程也让自己熟悉Mathjax。 Mathjax简介&emsp;&emsp;Mathjax是一款运行在浏览器中的开源数学符号渲染引擎，使用MathJax可以方便的在浏览器中显示数学公式，不需要使用图片。 基本语法 在正文中同一行插入LaTeX公式用$...$定义 例如语句为$\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$ 显示为$\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$ 另起一行显示LaTeX公式用$$...$$ 例如语句为$$W_G^{mn}=max\{0,W_G.\xi_G(f_G^m,f_G^n)\}$$ 显示为W_G^{mn}=max\{0,W_G.\xi_G(f_G^m,f_G^n)\}希腊字母 显示 命令 显示 命令 $\alpha$ \alpha $\beta$ \beta $\gamma$ \gamma $\delta$ \delta $\epsilon$ \epsilon $\zeta$ \zeta $\eta$ \eta $\theta$ \theta $\iota$ \iota $\kappa$ \kappa $\lambda$ \lambda $\mu$ \mu $\nu$ \nu $\xi$ \xi $\pi$ \pi $\rho$ \rho $\sigma$ \sigma $\tau$ \tau $\upsilon$ \upsilon $\phi$ \phi $\chi$ \chi $\psi$ \psi $\omega$ \omega 若需要大写希腊字母，将命令首字母大写即可。$\gamma$呈现为$\Gamma$ 若需要斜体希腊字母，将命令前加上var前缀即可。$\varGamma$呈现为$\varGamma$ 关系运算符 显示 命令 显示 命令 $\mid$ \mid $\nmid$ \nmid $\cdot$ \cdot $\leq$ \leq $\geq$ \geq $\neq$ \neq $\approx$ \approx $\equiv$ \equiv $\prec$ \prec $\preceq$ \preceq $\ll$ \ll $\succ$ \succ $\succeq$ \succeq $\gg$ \gg $\sim$ \sim $\simeq$ \simeq $\asymp$ \asymp $\cong$ \cong $\doteq$ \doteq $\propto$ \propto $\models$ \models $\parallel$ \parallel $\bowtie$ \bowtie $\perp$ \perp $\circ$ \circ $\ast$ \ast $\bigodot$ \bigodot $\bigotimes$ \bigotimes $\bigoplus$ \bigoplus 算术运算符 显示 命令 显示 命令 $\pm$ \pm $\mp$ \mp $\times$ \times $\ast$ \ast $\star$ \star $\circ$ \circ $\bullet$ \bullet $\cdot$ \cdot $\div$ \div $\sum$ \sum $\prod$ \prod $\coprod$ \coprod $\oplus$ \oplus $\bigoplus$ \bigoplus $\ominus$ \ominus $\otimes$ \otimes $\bigotimes$ \bigotimes $\oslash$ \oslash $\odot$ \odot $\bigodot$ \bigodot $\diamond$ \diamond $\bigtriangleup$ \bigtriangleup $\bigtriangledown$ \bigtriangledown $\triangleleft$ \triangleleft$ $\triangleright$ \triangleright $\triangleright$ \triangleright $\bigcirc$ \bigcirc 字母修饰上下标 上标:^ 下标:_ 例如:C_n^2，显示为$C_n^2$矢量 \vec a，显示为$\vec a$ \overrightarrow{xy}，显示为:$\overrightarrow{xy}$字体 打印机字体Typewriter：\mathtt{A}显示为$\mathtt{A}$ 黑板粗体字Blackboard Bold：\mathbb{A}呈现为$\mathbb{A}$ 无衬线字体Sans Serif：\mathsf{A}呈现为$\mathsf{A}$ 手写体:\mathscr{A}呈现为$\mathscr{A}$ 罗马字体:\mathrm{A}呈现为$\mathrm{A}$括号 小括号:()，显示为() 中括号：[]，显示为[] 尖括号：\langle,\rangle呈现为⟨⟩ 自适应括号：\left(...\right)能使符号大小与邻近公式相适应 (\frac{x}{y})，显示为$(\frac{x}{y})$ \left(\frac{x}{y}\right)，显示为$\left(\frac{x}{y}\right)$求和、极限与积分 求和：\sum 举例：\sum_{i=1}^n{a_i}呈现为$\sum_{i=1}^n{a_i}$ 极限：\lim 举例:\lim_{x\to 0}呈现为$\lim_{x \to 0}$ 积分:\int 举例:\int_0^xf(x)dx呈现为$\int_0^xf(x)dx$ 分式与根式 分式:\frac 举例:\frac{分子}{分母}呈现为$\frac{分子}{分母}$ 根式:\sqrt 举例：\sqrt[x]{y}呈现为$\sqrt[x]{y}$特殊函数 \函数名 举例:\sin x，\ln x，\max(A,B,C)呈现为$sin x$,$ln x$,$max(A,B,C)$空格 LaTex语法会忽略空格，需要用转义字符\ 小空格:a\ b呈现为$a\ b$ 四个空格:a\quad b呈现为$a\quad b$矩阵基本语法 起始标记\begin{matrix}``，结束标记``\end{matrix} 每一行末尾标记\\，行间元素以$分割 举例12345$$\begin&#123;matrix&#125;1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1\\\end&#123;matrix&#125;$$ 呈现为:\begin{matrix} 1&0&0\\ 0&1&0\\ 0&0&1\\ \end{matrix} 矩阵边框 在起始、结束标记处用下列词替换matrix pmatrix：小括号边框 bmatrix：中括号边框 Bmatrix：大括号边框 vmatrix：单竖线边框 Vmatrix：双竖线边框省略元素 横省略号：\cdots 竖省略号：\vdots 斜省略号：\ddots 举例123456$$\begin&#123;bmatrix&#125;&#123;a_&#123;11&#125;&#125;&amp;&#123;a_&#123;12&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;1n&#125;&#125;\\&#123;a_&#123;21&#125;&#125;&amp;&#123;a_&#123;22&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;2n&#125;&#125;\\&#123;\vdots&#125;&amp;&#123;\vdots&#125;&amp;&#123;\ddots&#125;&amp;&#123;\vdots&#125;\\&#123;a_&#123;m1&#125;&#125;&amp;&#123;a_&#123;m2&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;mn&#125;&#125;\\\end&#123;bmatrix&#125;$$ 呈现为:\begin{bmatrix} {a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\ {a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\ {\vdots}&{\vdots}&{\ddots}&{\vdots}\\ {a_{m1}}&{a_{m2}}&{\cdots}&{a_{mn}}\\ \end{bmatrix} 方程组 需要cases环境：起始、结束处以{cases}声明 举例123456$$\begin&#123;cases&#125;a_1x+b_1y+c_1z=d_1\\a_2x+b_2y+c_2z=d_2\\a_3x+b_3y+c_3z=d_3\\\end&#123;cases&#125;$$ \begin{cases} a_1x+b_1y+c_1z=d_1\\ a_2x+b_2y+c_2z=d_2\\ a_3x+b_3y+c_3z=d_3\\ \end{cases}公式编号 用\tag{n}标签 举例f(x)=x\tag{1}显示为$f(x)=x\tag{1}$ 后记&emsp;&emsp;以上列举的都是常用的Mathjax语法，以后有用到新的会继续补充。 欢迎关注我的公众号]]></content>
      <categories>
        <category>Hexo博客搭建教程</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Mathjax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Relation Networks for Object Detection》论文笔记]]></title>
    <url>%2Fpost%2F1b649e52.html</url>
    <content type="text"><![CDATA[Summary of background论文链接：https://arxiv.org/abs/1711.11575论文代码：暂未公布&emsp;&emsp;在深度学习兴起前，目标检测领域不少改进方法都是在经典检测模型上加入对象的上下文信息和对象间的关系以提升目标检测的性能，但这种方式在深度学习的架构中似乎不怎么起作用（以前我也想过这种改进方法，但深度学习发展的还真像炼丹，直接丢原始数据训练效果反而更好），这是由于深度学习发展到如今仍然是一个黑盒模型，主流的观点认为卷积神经网络具有很大的感受野，在网络训练时已经学习到了对象的上下文信息。&emsp;&emsp;这篇文章算是将关系模型应用到CV领域很成功的文章了，由于是受到谷歌在NLP方向的文章《Attention is All You Need》的启发，文中的许多变量公式都与谷歌的文章对比。谷歌的文章中完全基于Attention，而没有使用任何神经网络的结构，却取得了state-of-the-art的效果。由于我对NLP不怎么了解，在看这篇文章前还没有看过谷歌的文章。不过这篇文章的思路很有趣，相信从Attention模型的巧妙和CNN强大的特征提取能力结合这个角度出发，会有更多值得期待的改进。 Object Relation Module&emsp;&emsp;在之前使用CNN进行目标检测的方法中，都是每个对象被单独识别，而这篇文章将一组对象同时做了Relation Module处理，即一个对象上融合了其他对象的关系特征，好处在于丰富了特征，而且Relation Module处理前后，维度不会发生变化，这意味着该模型可以扩展到经典的任意基于CNN的目标检测框架中。该模型的模块示意图如下： &emsp;&emsp;其中对每一个对象有几何特征$f_G$(四维的bbox)，外观特征$f_A$(文中为1024维)，给定N个对象的输入集合为$\{f_A^n,f_G^n\}_{n=1}^N$，对于第n个对象，则它的关系特征为 f_R(n)=\sum_m\omega^{mn}.(W_V.f_A^m)\tag{1}&emsp;&emsp;其中$W_V$为显性转换，$\omega^{mn}$为关系权重，表示该对象受到其他对象的影响。 \omega^{mn}=\frac{\omega_G^{mn}.exp(\omega_A^{mn})}{\sum_G^{kn}.exp(\omega_A^{kn})}\tag{2}&emsp;&emsp;分母是对分子的 归一化，$\omega_A^{mn}$为外观权重，用点积dot计算 \omega_A^{mn}=\frac{dot(W_Kf_A^m,W_Qf_A^n)}{\sqrt{d_k}}\tag{3}&emsp;&emsp;其中$W_K$与$W_Q$用于将原始特征$f_A^m$和$f_A^n$投影到子空间，以测量它们之间的匹配程度。$W_G^{mn}$为几何权重， W_G^{mn}=max\{0,W_G.\xi_G(f_G^m,f_G^n)\}\tag{4}其中$W_G^{mn}$的计算分为两步 将两个对象的几何特征嵌入到高维表示中，记为$\xi_G$，计算目标m和n的相对位置$(log(\frac{x_m-x_n}{\omega_m}),log(\frac{y_m-y_n}{h_m}),log(\frac{\omega_n}{\omega_m}),log(\frac{h_n}{h_m}))^T$,这是一个四维的向量，分别表示中心点的坐标和宽高。 将4维的相对位置矩阵映射到64维向量，再与$W_G$做内积，然后再通过ReLU激活函数。 &emsp;&emsp;式(1)中的$f_R(n)$表示第n个对象提取的一个关系特征(relation feature)，一个对象会提取$N_r$种关系特征(作者的论文中是16种),然后将$N_r$种关系特征concat起来，再与原来第n个对象本身的特征相加，得到融合关系特征后的特征，公式如下： f_A^n=f_A^n+Concat[f_R^1(n),...,f_R^{N_r}(n)],for\ all\ n\tag{5}&emsp;&emsp;为了匹配关系特征和对象本身的特征间的维度，$W_V$能对$f_A^m$起到降维的作用。 Relation Networks For Object Detection&emsp;&emsp;这篇文章是将提出的relative module应用于目标检测，目前基于CNN的目标检测架构包含4个步骤 现在大数据集(一般是ImageNet)上预训练网络模型； 提取候选区域特征 实例检测 去除重复检测框 &emsp;&emsp;作者基于relative module的特性，将relative module用于每个全连接层之后，并且替代常用的NMS算法去除重复检测框。如下图所示 Relation for Instance Recognition&emsp;&emsp;在原本的RCNN模型中，经过ROI Pooling处理后，会经过两个全连接层后再进行边界框回归和目标分类，步骤如下图 &emsp;&emsp;由于relative module处理特征后，特征的维度不会发生变化，因此可以在每个全连接层后接一个relative module。则实例检测的流程变为： &emsp;&emsp;在上式中，R1和R2表示为relative module重复的次数。Instance Recognition的检测示意图如下图所示。 Relation for Duplicate Removal&emsp;&emsp;作者首先指出NMS由于是贪心算法且需要手工选择参数，是一个次优选择，然后说明Duplicate Removal问题实际是一个二分类问题，即对于每一个ground truth object，只有一个检测框是正确的，其余的检测框都可认为是duplicate。&emsp;&emsp;作者提出的这个模块的输入是instance recognition的输出，也就是一系列检测对象，每个对象都有1024维的特征，携带的信息有Bbox和分类分数$s_0$，从下图可以看到模块的输出是$s_0$和$s_1$的乘积，接下来看看$s_1$的计算方法。这个模块的具体步骤如下 首先作者指出将分类分数转换为rank更有效，而不是具体的数值。然后将rank和1024维的appearance feature转换为128维（通过上图中的$W_{fR}$和$W_f$） 将融合后的特征通过relation module改变所有对象的外观特征 将每个转换的特征通过线性分类（下图中的$W_s$）,再通过Sigmoid将输出归一化到[0,1]之间。 &emsp;&emsp;relation module是上述步骤的核心，因为使用relation module可以是整合Bbox，原始的appearance feature和分类分数，使整个目标检测框架仍然是一个端到端的模型。 &emsp;&emsp;紧接着面临的任务就是如何判断哪个detected object是正确，哪些是duplicate。作者首先设置了一个阈值$\eta$，输出大于该阈值的都会保留，然后在保留的detected object中，选择IOU最大的作为正确的保留，其余为duplicate。 Experiments&emsp;&emsp;实验部分数据集是具有80个类别的COCO数据集，CNN模型用的ResNet-50和ResNet101。 Relation for Instance Recognition&emsp;&emsp;首先看Instance Recognition的实验，首先比较了单纯2fc的Instance Recognition，和2fc+RM(relation module)，比较了RM的多种参数。 &emsp;&emsp;从上图可以看到，使用rank这个策略确实能使准确率提升，但作者没有解释这是为什么。 Relation for Duplicate Removal多种网络模型，多种参数比较 Relation module究竟学习到了什么&emsp;&emsp;作者提出的Relation module是一个很好的研究点，遗憾的是文中没有很好的解释Relation module学到了什么，作者说这个不在文章的讨论范围。为了对文章所提出的模型给出一个直观的解释，作者分析了Relation module中最后一个fc之后的RM中的关系权重，如下图所示，蓝色代表检测到的物体，橙色框和数值代表对该次检测有帮助的关联信息。 作者提出的问题 1.只有一个样本被划分为correct，会不会导致严重的正负样本不均衡？答案是否定，网络工作的很好，这是为什么呢？因为作者实际运行发现，大多数的object的$s_0$得分很低，因此$s_0$和$s_1$就很小，从而导致 $L=-log(1-s_0s_1)$ 和梯度 $\frac{\partial L}{\partial S_1}$ 都会比较小。2.设计的两个模块功能是否矛盾？因为instance recognition要尽可能多地识别出high scores的物体，而duplicate removal的目标是只选择一个正样本。作者认为这个矛盾是由$s_0$和$s_1$来解决的，instance recognition输出的高$s_0$可以通过较低的$s_1$来调节。3.duplicate removal是一个可以学习的模块，和NMS不同，在end2end训练中，instance recognition输出的变化会直接影响到该模块，是否会产生不稳定性？答案也是否定的，实际上，作者发现end2end的训练方式更好，作者认为这是由于不稳定的label某种程度上起到了平均正则化的作用。 欢迎关注我的公众号]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>CVPR</tag>
        <tag>目标检测</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Cascade R-CNN:Delving into High Quality Object Detection》论文笔记]]></title>
    <url>%2Fpost%2Fd76cc2d4.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;今年的图像领域的顶会CVPR收录了不少目标检测的论文，学习这些论文又可以让我忙活一阵了。&emsp;&emsp;Cascade R-CNN: Delving into High Quality Object Detection这篇文章关注的是IOU的最优选择，很小的trick，不过在效果上有不错的提升。 摘要&emsp;&emsp;目标检测中，需要确定IOU的阈值来区分正样本和负样本。低的阈值如0.5训练网络，易产生检测噪声，但随着IOU阈值的增加，检测性能会降低。两个主要的因素：1)在训练期间过拟合，导致正样本指数消失;2)检测算法最佳的IOU和假设的IOU之间不匹配。在多阶段目标检测架构中，为了解决以上两个问题，提出了级联R-CNN算法。该算法由一系列随着IOU阈值增加而训练的检测器组成，循序渐进的对close false positive更具选择性。检测器被阶段性的训练，如果检测器的输出是一个好的分布，则用于训练下一个阶段更好的检测器。对逐渐改进的假设进行重采样，保证所有的检测器由一组同等大小的正样本组成，缓解过拟合问题。在假设阶段应用同样的级联程序，使得每一个阶段的假设和检测器的质量有一更匹配的性能，级联R-CNN算法的简单实现，在COCO数据集上检测性能超过了所有单模型对象检测算法。实验还表明，级联R-CNN可广泛用于不同的检测架构，获得与基准检测器强度无关的增益。代码地址为：https://github.com/zhaoweicai/cascade-rcnn。 方法概述&emsp;&emsp;检测问题和分类问题有很大的不同，检测问题要通过IOU区分正负样本，因此IOU的选择对train和inference有很大的影响。作者做了一组实验 &emsp;&emsp;如图(a),阈值为0.5会有大量的噪声，这个阈值很难区分与正样本相似的误检测；而一个较好的阈值，检测算法基本上没有误报。图c中，横轴是proposal的IOU,纵轴是经过box reg训练后得到的新的IOU，不同的线条代表不同的阈值训练出的检测器。三条线的整体趋势都显示，IOU越高，检测器的回归性更好。在0.55~0.6之间时，0.5的阈值训练出的检测器性能最好，在0.6~0.75之间，0.6的阈值性能最好，而在0.75以上，0.7的阈值训练出的检测器性能最好。通过以上分析表明，当proposal自身的IOU阈值与训练时的阈值接近时，检测器的性能最好。然而这会带来一个问题：目标检测中训练和测试的数据集是大量的，若使用单一的IOU阈值，以常用的0.5为例。在proposal时，所有IOU大于0.5的都会被认为是正样本，在0.6到0.95之间的proposal上，0.5的阈值表现会比较糟糕。而如果选用0.7的阈值，单纯看图(c)0.7的阈值也有较好的表现，但是图(d)中，0.7的阈值表现是最糟糕的，原因是0.7的阈值会使训练样本大幅减少，过拟合的现象会非常严重。&emsp;&emsp;为了解决以上的问题，作者提出了级联R-CNN框架，用一个stage的输出训练下一个stage。还是图(c)中，三条线的大部分区域都在y=x这条线之上，说明proposal在经过box reg训练之后，IOU是增加的。因此，作者想到可以将多个检测器级联起来，例如将三个IOU分别为0.5.0.6和0.7的检测器串起来，对于IOu为0.55的proposal，在经过0.5的detector后，IOU变为了0.7；再经过0.6的检测器，IOU变为了0.85；再经过0.7的检测器，IOU变为了0.89。这种训练方式比任何一个单独的检测器的结果要好。除了改善IOU外，经过多个检测器，proposal的IOU变高，正样本的质量会更好，即使下一个检测器的IOU阈值设置的较高，也不会有太多的样本被舍弃，能有效的缓解过拟合现象。 相关工作及比较证明&emsp;&emsp;作者将自己的工作与其余的几种工作做了对比实验。 &emsp;&emsp;上图中，==H0==代表区域建议网络，==H1==代表ROI池化层，==C==代表分类得分，==B==代表边界框回归图(a)是经典的Faster R-CNN框架模型，这篇文章的网络模型 也是在Faster R-CNN模型的基础上进行扩展；图(b)的Interarive BBox采用了级联的结构对Box回归，但可以看到ROI检测网络部分是相同的结构“H1”，即采用的级联结构完全相同.其中边界框回归部分的公式与Faster R-CNN中相同，这里不再赘述。 &emsp;&emsp;上图中第一行横纵轴分别为回归目标中box的x和y方向的偏移量，第二行横纵轴分别为回归目标中box的宽高偏差量。可以看到在级联中从1st stage到2st stage，proposal的分布发生了很大的变化，有很多噪声在经过box reg训练之后提高了IOU,在2st stage和3st stage中那些红点属于异常值。因此需要在后续的级联结构中提高阈值去除噪声干扰。&emsp;&emsp;但前面提到提高阈值会减少正样本的数量，在前言部分对此给出了感性的解释，作者又给出了更相信的理论依据。 &emsp;&emsp;从上图可以看到在1st stage中大于0.5的部分，与2st stage中大于0.6的部分，还有3st stage中大于0.7的部分在数量上基本一致。 &emsp;&emsp;而Figure3中的图(c)的Interative Loss没有级联结构，该结构使用了不同阈值分类，然后融合多个结果进行分类推理，而且只取了一个结果做边界框回归。作者指出，Figure4的1st stage部分可以看出，当IOU越高，proposal占的比重越小，因此Interative Loss的结构不能从根本上克服过拟合的现象。 实验&emsp;&emsp;作者提出的级联结构在Figure 3的(d)中已经给出，作者采用的结构是一个RPN网络加上3个检测器（阈值分别为0.5/0.6/0.7）。在这三个级联检测器中，每个检测器的输入都是上一层检测器边界框回归之后的结果。在COCO数据集上实验结果如下： &emsp;&emsp;作者的方法效果还是很惊艳的，作者也比较了网络的耗时 &emsp;&emsp;可以看到使用级联虽然在一定程度上增加了参数，但对效率的影响还在可以接受的范围。 总结&emsp;&emsp;目标检测与分类任务很大的不同就在于数据集的样本没有明确的区分正负样本，在检测任务中是通过设置IOU的值区分正负样本，这就涉及到一个调参的问题，可以想象作者在调IOU参数做对比实验时时所受的折磨。作者做的对比实验中，也比较了一些关于优化IOU参数前人做的结构调整，Cascade R-CNN所提的方法，在前人的基础上更进了一步，做的理论分析也十分的合理。 欢迎关注我的公众号]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>CVPR</tag>
        <tag>目标检测</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从0到1:开启商业与未来的秘密]]></title>
    <url>%2Fpost%2Fef0c22d1.html</url>
    <content type="text"><![CDATA[写在前面&emsp;&emsp;去年师兄推荐我和另外一个同门去听在华中科技大学举办的“深度学习研究前沿研讨会”。因为做报告的都是在校的教授以及公司的技术主管，在谈论学术界和工业界在深度学习领域发挥的作用时，地平线机器人的联合创始人杨铭老师的回答给我留下了很深的印象，他说学术界做的事情，更像是从0到1，因为深度学习是学术界搞出来的，而工业界则是将这一方法应用到各种领域，并针对遇到的问题完善方法，这过程更像是从1到n。其中从0到1和从1到n的比喻让我记忆深刻，随后的有天看到了《从0到1：开启商业与未来的秘密》这本书，好奇心驱使我将这本书加入了书单，在快到春节的时候断断续续的读完了这本书。&emsp;&emsp;《从0到1》写的很精炼而且通俗，所举的例子都是耳熟能详的大公司和人物，无论是否有计算机背景，或是否准备创业，这本书都能给人很大的启发。作者在论述各个观点时，所举的例子中有关于社会、教育、地缘差异以及政治的思考，是一本值得反复翻看的书。写这篇博客时，刚好看完第二遍，以后等自己关于公司的运作和创业有了新的感悟，还会接着看。 创业圣经&emsp;&emsp;在看一本书之前，我有个习惯是去看下书评，关于这本书的评价看到最多的是创业圣经！在我读完这本书后，作者关于商业模式、对于竞争和成功企业的看法，使我受益匪浅。这本书的确是围绕一个成功的企业应该有哪些特质来论述，但往小了说，大多数人就算不创业也会和公司的运营息息相关，因此书中的很多观点对个人的发展也很有参考意义。&emsp;&emsp;在看完这本书之前，我只知道彼得•蒂尔是paypal的创始人，读完之后很好奇他的其他经历，便搜索了一番。彼得•蒂尔是国际象棋天才，12岁获得美国国际象棋大师资格（百度百科上介绍是全美第7，维基百科是全美第21），大学就读于斯坦福大学，主修哲学，1989年取得学士学位。而后进入斯坦福法学院，1992年取得法学博士学位。随后进入了一家纽约顶级律所。1996年创办Thiel资产管理公司（Thiel Capital Management）1999年创立了paypal，在2002年更名为Clarium Capital，管理资产总值超过55亿，2004年，投资了50万给刚刚起步的facebook，拿了近10.2%的股份。此外，他也是明星风险投资家，对冲基金经理、衍生品交易操盘手、畅销书作家、斯坦福商学院的老师、公共演讲者、硅谷思想家…向大佬低头。&emsp;&emsp;彼得•蒂尔在《从0到1》中谈及了自己的创业历程和心得。文中的开篇就对书名做了解释： 做大家都知道如何去做的事，只会使世界发生从1到n的改变，增添许多类似的东西。创业秘籍并不存在，因为任何创新都是新颖独特的，任何权威都不可能具体规定如何创新成功人士总能在意想不到的地方发现价值，他们遵循的是基本原则，而非秘籍。 从0到1：进步的未来 水平进步，即从1到n，照搬已有的经验。全球化 垂直进步：即从0到1，探索心的道路。科技 &emsp;&emsp;大部分人认为未来由全球化决定，但事实上，科技更具影响力。全球化加速了全球的发展，为各个国家带来了先进的实用技术和管理经验，但作者表明全球化只是墨守陈规的推动世界进步，而科技的创造性价值会更有影响力。 像1999年那样狂欢&emsp;&emsp;硅谷淘金热： 1998年9月~2000年3月，这期间是创业热潮，在硅谷每周都有数十家新企业竞相举办豪华开业派对。 &emsp;&emsp;paypal的创立也是在1999年的创业热潮中，paypal最初开发的电子邮件支付系统，然而用户增长缓慢，成本不断上涨。为了获得新用户，提出付钱让人注册。每位新用户一注册即可获得10美元，推荐一个朋友也能获得10美元。但这个战略并不持久，因为会带来高额的成本。但整体考虑到巨大的用户技术和交易中收取的小额手续费，肯定会盈利。幸运的是，paypal刚完成融资，网络的泡沫就破灭了。 &emsp;&emsp;随着网络公司的衰败，全球化代替科技成为未来的希望。&emsp;&emsp;企业家从硅谷之劫得到的四点经验： 逐渐超前，不能沉溺于宏大的愿景中，否则会泡沫膨胀。心存改变世界之雄心的人通常要更加谦逊，小幅循序渐进地成长是唯一安全前进的道路。 保持精简和灵活性。所有的公司都必须留出一定空间，不要事事都严格计划。 你应该做些尝试， 反复实践， 把创业当成未知的实验 改进竞争方式。不要贸然创造一个新市场。以现成的客户作为出发点创业才 更有保障，在成功者已经创造出的产品的基础上，将这些已经被认可的产品加以改进， 才是可取之道。 专注于产品， 而非销量。 如果你的产品需要广告或营销人员去推销， 说明你的产品还不够好：科技应用于商业应该主打产品开发，而不是销售物流。 &emsp;&emsp;但作者显然不是这样墨守陈规的人，因此他提出了这个观点的对立面： 大胆尝试胜过平庸保守。 坏计划也好过没有计划。 竞争性市场对收益有负面影响。 营销和产品同样重要。 我们仍然需要新科技，我们甚至还可能需要用1999年的那种狂热去寻求新科技。要想建立新一代企业，我们必须扔掉之前陈旧的法则。但这并非意味着那些法则的对立面就一定是正确的：因为就算你有心逃脱，大众洪流也会裹挟着你向前。相反要问自己： 你对企业的认识有多少基于对以往过错的错误反应形成的。 最反主流的行动不是抵制潮流， 而是在潮流中， 不丢弃自己的独立思考。 所有成功企业都是不同的&emsp;&emsp;将第二章最后的对立面应用在商业上，那就是：还有什么有价值的公司没有成立。创造价值还不够——还得抓住自己创造的部分价值。表明大公司也可能经营不善。（以航空公司和谷歌为例）航空公司是完全竞争，而谷歌是垄断企业。垄断公司可以自由决定供给量和价格，以实现利益最大化。但有趣的是无论是哪种企业，都会规避被扣上对应的帽子。 由于垄断会招致审核甚至打击，垄断企业不会宣称自己的垄断地位。往往会把他们的市场描述成并集，表明他们只占很小的一部分； 非垄断企业为了凸显自己企业的与众不同，会把市场描述成交集来夸大自己的独特性。 &emsp;&emsp;然而市场是冷酷无情的，要想将企业从每日的生存竞赛解脱出来，唯一的方法是获取垄断利润。 &emsp;&emsp;关于垄断一些看似矛盾的点： 垄断对于产业内的人来说是好事，但对于产业外的人是坏事。 富有创意的垄断者，能给消费者更多的选择。有创意的垄断企业是使社会更美好的推动力； 政府有的部门通过授予发明专利权创造垄断企业，但是有些监管部门使用反垄断措施扼杀垄断企业； 垄断企业能推动社会进步，因为高额的垄断利润，使垄断企业有了规划长远未来的资本，有能力投资新的研究项目； &emsp;&emsp;经济学家痴迷于企业间的竞争，实际上经济学家的想法来自物理和生物学的平衡态。然而如果你的公司处于竞争平衡中，那么和你公司相差无几的其他企业随时都准备取态你的位置。竞争是大多数企业的特点，但垄断才是成功企业的写照。&emsp;&emsp;以2-8法则为例子，表明很多东西的数学模型不是正态分布。特别是在风险投资这一领域。目前，12家大型科技公司的总价值，比所有其他科技公司加起来都要多。 &emsp;&emsp;垄断企业的特点： 专利技术：专利技术的某些方面要比相近的替代品好上10倍，相当于创造全新的事物； 网络效应：使一项产品随着越来越多的人使用而变得更加有用，如QQ的用户体量为QQ带来的价值，还有facebook等社交网站； 规模经济：这是互联网企业的优势，传统的行业很难扩大自己的体量，而互联网公司依靠其核心团队，就能为上百万顾客提供服务； 品牌优势：很多优秀的产品形成自己的品牌后，会带来高额的品牌溢价； &emsp;&emsp;建立垄断企业的方法： 占领小市场:在非常小的市场占主导地位，目标市场可以只针对一小群人，尽量避开其他的竞争者 扩大规模：循序渐进的发展市场，以亚马逊为例，以图书为起点，第一个目标是提供世界上所有的书，如今的目标是提供世界上所有的东西。 不要搞破坏：要去创造新的事物，而不要到处找麻烦。paypal抢走了visa的一部分生意，但是当papyl扩大整个支付市场后，给了visa更多的商业机会。突然想到了美团··· 后来居上：经商就像下棋，要想赢，首要工作就是研究残局。 竞争意识&emsp;&emsp;社会随处可见竞争，各式的竞赛和教育体系，促使我们去竞争。学校里的成绩也是对每个学生竞争力的测量，分数高的学生能得到老师的夸奖与证书，在教育里用同样的方法教授学生，不顾个人的天赋和爱好（作者关于教育的观点也同样犀利）。除非是真正醉心于学术或者在这方面有极高天赋的人，不然这种教育的弊端随着学历的增长会越来越严重，优秀的学生自信的“往高处走”，直到竞争激烈到把个人的梦想吞噬殆尽。&emsp;&emsp;作者的起初人生规划是在斯坦福法学院从数以万记的学生脱颖而出，成为最高法院的书记员，最后却在面试时失败了。不得不说，有时候选择另一条路的机会成本真高。竞争容易使人出现幻觉，徒劳的抓一些并不存在的机会。作者形象的解释了患有阿斯伯格综合症（社交障碍）的人在硅谷更有优势的原因在于他们对社会动态不敏感，不会盲目跟风。 后发优势&emsp;&emsp;关于一直亏损的科技公司为什么估值那么高这个问题，过去一直很疑惑，后来了解到企业的估值会考虑到企业的生意模式，发展前景和经营态势，科技公司没有实体经济，而正是这种商业模式和发展前景使科技公司拥有较高的估值。作者给出的解释更加好理解。 一个企业今天的价值是它以后创造利润的总和。传统公司有稳定的现金流，一般是盈利了才上市，但科技公司开始时通常会亏损，这是由于创造有价值的东西需要时间，所以收益延迟低。一家科技公司的价值往往来源于遥远的未来。 &emsp;&emsp;因此先行一步只是策略，真正重要的是从未来产生现金流。要实现这个目标需要先主导一个小的市场，在这个基础上扩大，知道达到预想的目标 成功不是中彩票&emsp;&emsp;作者提了一个很有争议的话题，即成功是靠运气还是技能。如果成功来源于运气，那么连续创业者也就不会存在了。但不可否认的是，由于一个企业只有一次生存机会，因此其成功究竟是偶然还是必然是没法考证的，而关于未来哪些企业能成功，更重要的议题是：未来是靠机遇还是计划。&emsp;&emsp;对未来是持乐观还是不乐观，明确还是不明确的态度可以组合成四种观点。 对未来不明确的悲观主义：可能由于经济学常常被看作一门忧郁的学问，认为几乎历史上所有民族都是悲观的。对未来不明确的悲观主义看到的未来是阴郁的，而又素手无策。如1970年后的欧元区。 对未来明确的悲观主义：这种观点认为未来是可知的，但却是暗淡的。作者以中国举例，因为中国有强烈的忧患意识。 对未来明确的乐观主义：在这种观点眼中如果计划缜密，工作努力，未来会比现在更好。 对未来不明确的乐观主义：这种观点，想在未来获利，而又想坐享其成。 &emsp;&emsp;当今的世界正是对未来不明确的乐观主义，表现在以下几点： 不明确的金融：企业家不知道将钱放在哪，索性存到了银行里—&gt;银行家不知道怎么投资，于是将钱交给机构投资人—&gt;投资人不知道拿钱做什么，就投资了股票—&gt;公司试图产生自由现金流来提升股票价格，做法是发放股息，或是回购股份； 不明确的政治：西方政客只在选举期间才对民众负责； 不明确的哲学：20世纪后半叶，迷茫的哲学成为前沿思想。对未来没有任何具体的规划，相信人和上帝能和平共处； 不明确的人生：医药企业有倒摩尔定律，从1950年起，10亿美元研发的新药物数量每9年就会减半。这是由于生物技术的企业，研究对象是不可控的有机体，背景环境是了解不足的自然界，研究方法是不确定且随机的。 &emsp;&emsp;作者在谈完全球的大环境后随即指出，长期规划是重要的，只有对未来精确的规划，才可以改变世界，而非复制其他人的成功。而且不能将成功当作中彩票，必须有明确的方向。 秘密&emsp;&emsp;如果世界上还有很多秘密，就还有可能出现很多有望改变世界的企业。以惠普为例，在1999年前，惠普不断发明创新，而1999后，成立了咨询服务集团的惠普放弃了对科技的探索，只关注会计部门是否一切正常。结果是到2005年，惠普的市值跌倒了5年前的一半。成功的企业建立于开放却未知的秘密之上，如airbnb，优步。解决了人们的痛点而且丰富了生活的方式。探索秘密的最佳处是无人关注的地方，而且不要轻易将秘密告诉其他人，所有成功企业都是基于鲜为人知的秘密而创立。 基础决定命运&emsp;&emsp;作为公司的初创人，首要工作是打好基础，在有缺陷的基础上建立一个伟大的企业是不可能的。突出在以下几点： 选择好的合伙人至关重要，突出在技术能力、才华互补和合作的默契； 理清公司的所有权，经营权和控制权，有助于公司的团结； 为了减少分歧，应避免雇佣非正式员工，甚至是远程办公； 股票报酬相比较于现金报酬更能让员工全力以赴； 有价值的公司应始终鼓励发明创造。 打造帮派文化 提供不可替代的工作机会：应聘者应更关注公司的使命和将要加入的团队，只有这些吸引一个人，公司选择的才是合适的人选； 每个员工都与众不同：从外貌来看，每个员工都有与众不同的气质，但初创公司的每个人在个性上应相似，有共同的爱好； 每名员工只关注于一件事：简化管理，分工明确可减少矛盾； 凝聚力，能让员工极度投入。 顾客不会自动送上门&emsp;&emsp;这一章主要是讲销售的重要性，在华为实习时就听说了研发不如狗，销售岗位有着更大的竞争压力，薪水也普遍比研发高不少，部门也有不少高管是在国外做过技术支持或者销售的。本科时，我很不理解销售人员人人都称为经理，而且薪水很高。之后更加了解了生产和销售的关系，公司内部的《华为人》杂志中同事分享的扩展海外业务的艰辛与不易，我开始明白顾客并不会因为公司生产了产品就去买，而东西卖出去了对公司才有价值，做到这一点，靠的就是销售人员。 技术与销售的区别&emsp;&emsp;在工程上，解决方案不是奏效就是失效。而销售恰恰相反，往往是精心设计活动来改变表象而不修改实质内容。所以我想技术人员觉得销售人员不该有那么高的薪水，一方面是来源于销售没有给产品带来实质性的价值，另一方面是销售在看起来“喝喝茶，吹吹牛”中就把工作完成了 Σ(っ °Д °;)っ。但也仅限于吐槽，销售对拓展业务，向用户传递产品价值还是很重要的ヽ(｡◕‿◕｡)ﾉﾟ。 销售是隐形的&emsp;&emsp;销售的第一要务是说服，而不是真诚。很有趣的是由于很少有人愿意被提醒自己正在被销售。因此推销广告的称为“业务经理”，推销客户的称为“业务开发”，推销公司的称为“投资银行家”，推销自己的成为“政客”。 人类和机器&emsp;&emsp;计算机目前能干越来越多的事情，1997年，IBM的深蓝打败了世界象棋冠军。而比象棋要复杂的多的围棋也在2016年被谷歌的Alphago和Alphago Zero横扫。很长一段时间，媒体的报道都是未来有哪些行业会被人工智能取代。很赞同作者的观点：计算机是辅助人类的工具，而非替代物。&emsp;&emsp;全球化的发展使每个地区和国家按照其自身优势发展各自的专长，人类和计算机之间也有显著的差异，人类擅长做决策，而不适合做大数据处理，这种互补会起到补充作用。 创始人的悖论&emsp;&emsp;看到这一章时，我想了下我所知道的牛X的人物，在我所观看的关于伟人的电影中无论是“飞行家”中的刻画的美国历史上拍电影、造飞机经营航空公司自己又是飞行员颇具传奇色彩的霍华德·休斯，“模仿游戏”中在后世被称为计算机之父，人工智能之父的艾伦·麦席森·图灵，还是“社交网络”中的创立facebook的扎克伯格都是集天才与怪异于一身的。这一章也用图表表明，创业家的与常态的“正常人”有完全相反的倒钟形曲线。 &emsp;&emsp;独树一帜的创始人能够做出权威决策，激发员工强烈的忠诚度，提前做出未来几十年的规划。而训练有素的专业人氏组成的毫无人情味的官僚机构虽能长久持续下去，却没有长远的目光。 关于未来&emsp;&emsp;哲学家尼克·博斯特罗姆描述了人类世界的四种模式： 兴衰交替 稳定发展 人类灭绝 加速腾飞 &emsp;&emsp;但无论是哪种模式，未来都不是自行发生的，不能理说当然的认为未来会更美好，而是要努力创造美好的未来。当下的任务是找到创新的独特方式，使得未来不仅仅与众不同，而且更加美好。 欢迎关注我的公众号]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何阅读一本书]]></title>
    <url>%2Fpost%2F43b16a18.html</url>
    <content type="text"><![CDATA[写在前面&emsp;&emsp;趁着五一的闲暇，在家里读完了《如何阅读一本书》。这本书很偏学术风，逻辑性很强，前两章引出了阅读的四个层次，往后的章节都是对前两章的补充。每一章节的结束基本都有对整章内容的总结。&emsp;&emsp;作者在开篇提到，收音机、电视取代了以往由书本所提供的部分功能，就像照片取代了图画或艺术设计的部分功能一样。但不可否认，书籍的作用是不可取代的。这本书的初版是在1940年出版，距今有70多年。相比较于这版书出版的年代，90后的一代可以称为随着互联网成长起来的一代，因此拥有而且能适应更多的资讯获取方式，各种公众号以及自媒体会将整理好的信息推送给用户。利用碎片时间获取信息很有必要，也是一个良性的发展。然而很多有用的知识，是需要潜下心来系统的去看书，才能获取的。10多年的教育使大多数人都具备了阅读的能力，并且对如何去阅读一本书有了自己的习惯。我常常觉得我的习惯不太好，为了能在有限的时间，用更优质的方法去选择和阅读一本值得读的书。因此在五一读的第一本书便是用来纠正自己阅读习惯的。下面分享一下这本书的读书笔记。 阅读的活力与艺术&emsp;&emsp;阅读是一件说简单也简单，说复杂也复杂的活动。单从阅读能为我们带来什么这个角度来说，无非有这几点：理解力、娱乐和资讯。在阅读之前带着好的目标能有事半功倍的效果， 主动的阅读：如何做一个自我要求的阅读者 阅读是一件可以主动的事 阅读越主动，效果越好&emsp;&emsp;以棒球赛为例，作者=投球手；读者=捕手。区别在于，读书不止有读的好与读的差两种状态。而更靠近读的好还是读得差，取决于阅读时多么主动，以及投入多少心思和阅读技巧。阅读的时候，让自己昏昏入睡比保持清醒要容易的多。在阅读时处于哪种状态，主要取决于一个人的阅读目标。作者指出，许多人都能清楚的区分想要从书中获益还是取乐，但最后仍然无法按目标阅读。原因在于不知道怎样做一个自我要求的阅读者。在阅读一本书时想打瞌睡，并不是不想努力，而是不知道如何努力。要学会主动阅读主动阅读的基础：阅读时提出问题，并尝试自己解答 这本书再谈论什么 哪些部分详细描述了 这本书的观点对吗 这本书的信息跟你有什么关系 &emsp;&emsp;如何让一本书真正属于自己：学会做笔记，阅读一本书像是与作者对话。而笔记能表达自己与作者之间相异或相同的观点。做笔记的方法： 画底线 标记重点符号 在空白处编号，理清作者的思路 圈出关键字或句子 在书的空白处做笔记 &emsp;&emsp;培养阅读习惯:习惯是第二天性，让阅读变得自然。任何原创性的东西都有规则和技巧可循，如绘画和雕塑。&emsp;&emsp;从许多规则中养成一个习惯：任何技能在不熟练的时候，都显得笨手笨脚，熟悉了后能将复杂的步骤连贯起来，变得优美且和谐。 阅读的目标：为了获得资讯而读，以及求得理解而读&emsp;&emsp;作者鼓励如果想要从一本书获取到有用的东西，而不是打发时间，最好是为了提升理解力和资讯。事实上，在获取到理解力与资讯的同时，就具有了消遣的效果。&emsp;&emsp;阅读这个词可以区分为两种含义： 阅读与自己理解力相近的读物，如报纸：能增加资讯，却不能增加理解力 阅读的书籍或听的演讲，对方水平远高于自己:理解更多的事情（知乎上关于知识的定义） 阅读就是学习&emsp;&emsp;法国文艺复兴时期的人文主义思想家蒙田说：初学者的无知在于未学，而学者的无知在学后。后者被英国诗人亚历山大.蒲伯称为书呆子，读的广却读不通。要避免都的多就是读的好的观点，要学会选择值得阅读的书籍，在读书时思考，还要运用感觉和想象力。 老师的出席与缺席&emsp;&emsp;阅读一本书只能靠自己，更像是跟着一位缺席的老师学习，因此阅读书籍时更需要主动，也要通过各种方式运用书籍和作者交流。 阅读的层次&emsp;&emsp;阅读的层次是递进的，而且的向下包含的。四种阅读层次： 初级阅读（elementary reading）:能认字，这是基础，能明白每个字的意思，才能明白句子背后的含义。 检视阅读（inspectional reading）略读，在很短时间读完一本书 分析阅读（analytical reading），全盘完整的阅读，寻求理解 主题阅读(syntopical reading)，在阅读时，比较很多相关的书对于中国的国情，每一个受过义务教育的人都能精通初级阅读。因此主要看作者分享的后续阅读方法。 检视阅读&emsp;&emsp;精通了初级阅读才能熟练的检视阅读，检视阅读一共有两种： 有系统的粗读：目标是快速发现一本书值不值得多花时间阅读。略读是以最小的时间代价了解作者为什么写这样一本书以及这本书是否对自己有用。关于略读的一些建议：1.先看书名页，看序 2.研究目录页 3.如果有索引，也要检阅下 4.出版者的介绍 5.挑几个跟主题相关的篇章看看 6.翻看几页 粗浅的阅读：并不是说草草的读完了事，而是遇到不懂的地方，记录下来，先略过。集中精神读完弄得懂的地方。享受读书的快乐，洞察全书的意义。稍后再专心研究不懂的地方。 &emsp;&emsp;阅读的速度很重要，不同的书籍用不同的速度阅读。训练阅读速度的方法：眼睛一次只能读一个字或句子，但大脑能在一瞥之间掌握一个句子或段落。要学会跟着大脑的快速运转看书，而不是眼部的慢动作。训练自己快速阅读的方式：用手指着读，手移动的速度稍比自己的阅读速度快，强迫自己以更快的速度阅读。 分析阅读书籍分类的重要性：在阅读前要知道自己读的是那一类书书名与内容的关系：阅读书名可以让读者在阅读之前，获得一些基本的资讯 &emsp;&emsp;分析阅读的规则： 分析阅读的第一阶段：找出一本书在谈些什么 依照书的种类与主题分析 使用一个单一的句子或最多几句话叙述整本书的内容。 将书中重要篇章列举出来，说明是如何按照顺序则称一个整体的架构后两条规则可以帮助写作，因为写作和阅读是一体两面的事情，一个作品应该有整体感，逻辑清晰，前后连贯。 找出作者要问的问题，或作者想要解决的问题 分析阅读的第二阶段：诠释一本书的内容规则 诠释作者使用的关键字，与作者达成共识 从最重要的句子中抓出作者的重要主旨 找出作者的论述 ，重新架构这些论述的前因后果，以明白作者的主张 确定作者已经解决了哪些问题，还有哪些是未解决的 分析阅读的第三阶段：评价一本书的规则 除非已经能诠释书的架构，否则不要轻易批评 证明作者知识的不足 证明作者错误的地方 证明作者的不合逻辑 证明作者的分析与理由是不完整的 主题阅读：阅读的最终目标&emsp;&emsp;主题阅读的准备阶段： 针对要研究的主题，参考图书馆目录以及书中的索引。找到与主题相关的书籍 浏览整理出的书单，确定哪些与要研究的主题相关。明确各个书与主题的相关程度。 &emsp;&emsp;在讨论某个主题的书时，所涉及到的往往不止是一本书。主题阅读的五个步骤： 找到相关章节，关心的重点应该在具体的内容，而不是整本书 分析阅读中说与作者达成共识。但面对不同的作者描述同样的观点，会有不同的字眼。需要找出他们之间的共识。 建立一个主旨，列出一连串的问题，从书中找答案 理清主要及次要的议题。将作者针对不同各个问题的不同意见整理在各个议题旁。 将问题与议题按顺序排列，以凸显主题。 阅读与心智的成长：&emsp;&emsp;好书能带来心智的成长，提升阅读能力，能教会你了解这个世界以及自己。不仅是读的更好，还更懂得生命；变得有智慧与知识，对人类生命中永恒的真理有更深刻的认识。在读书的时候不光要会阅读，也要能分辨哪些书能给自己带来成长。 总结 不要被书中的难点吓到，90%的书知道个大概就好，想要了解的地方再深究； 阅读时要系统的总结，梳理文章的内容 学会做笔记，包括画重点，记录章节，页数以及自己针对某段话的思考 选择优质的书籍，不要将时间浪费在不好的书上 阅读时有以下重点： 带着问题和目的主动的阅读 整理文章的脉络 与“书本”交流，时常问自己这本书这一章是解决什么问题，哪些观点对或者不对，值得借鉴和应该避免的错误 将书本引入自己的知识结构中 欢迎关注我的公众号]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe版Faster R-CNN可视化——网络模型,图像特征,Loss图,PR曲线]]></title>
    <url>%2Fpost%2F79a2296e.html</url>
    <content type="text"><![CDATA[可视化网络模型&emsp;&emsp;Caffe目前有两种常用的可视化模型方式： 使用Netscope在线可视化 Caffe代码包内置的draw_net.py文件可以可视化网络模型 Netscope&emsp;&emsp;Netscope能可视化神经网络体系结构（或技术上说，Netscope能可视化任何有向无环图）。目前Netscope能可视化Caffe的prototxt 文件。网址为：http://ethereon.github.io/netscope/#/editor&emsp;&emsp;Netscope的使用非常简单，只需要将prototxt的文件复制到Netscope的编辑框，再按快捷键Shift+Enter即可得到网络模型的可视化结构。Netscope的优点是显示的网络模型简洁，而且将鼠标放在右侧可视化的网络模型的任意模块上，会显示该模块的具体参数。图1以Faster R-CNN中ZF模型的train.prototxt文件为例 图1 Netscope可视化ZF网络模 draw_net.py&emsp;&emsp;draw_net.py同样是将prototxt绘制成网络模型，在绘制之前，需要安装两个依赖库： 1、安装ＧraphViz # sudo apt-get install GraphViz 注意，这里用的是apt-get来安装，而不是pip.2 、安装pydot # sudo pip install pydot 用的是pip来安装，而不是apt-get &emsp;&emsp;安装完毕后，即可调用draw_net.py绘制网络模型，如绘制caffe自带的LeNet网络模型： 1sudo python python/draw_net.py examples/mnist/lenet_train_test.prototxt netImage/lenet.png --rankdir=TB &emsp;&emsp;其中有三个参数，各自的含义为： 第一个参数：网络模型的prototxt文件第二个参数：保存的图片路径及名字第二个参数：–rankdir=x , x 有四种选项，分别是LR, RL, TB, BT 。用来表示网络的方向，分别是从左到右，从右到左，从上到小，从下到上。默认为ＬＲ。 &emsp;&emsp;可视化结果如下图所示： 图2 draw_net.py可视化LeNet网络模型 可视化图像特征&emsp;&emsp;关于图像的可视化，我也使用过两种两种方式： 修改demo.py代码输出中间层结果 使用可视化工具deep-visualization-toolbox 修改demo.py&emsp;&emsp;该部分是参考薛开宇的《caffe学习笔记》中的VOC为例，修改demo.py文件后，代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263``` stylus#!/usr/bin/env python#-*-coding:utf-8-*-import matplotlibmatplotlib.use(&apos;Agg&apos;)import _init_pathsfrom fast_rcnn.config import cfgfrom fast_rcnn.test import im_detectfrom fast_rcnn.nms_wrapper import nmsfrom utils.timer import Timerimport matplotlib.pyplot as pltimport numpy as npimport scipy.io as sioimport caffe, os, sys, cv2import argparseCLASSES = (&apos;__background__&apos;, &apos;aeroplane&apos;, &apos;bicycle&apos;, &apos;bird&apos;, &apos;boat&apos;, &apos;bottle&apos;, &apos;bus&apos;, &apos;car&apos;, &apos;cat&apos;, &apos;chair&apos;, &apos;cow&apos;, &apos;diningtable&apos;, &apos;dog&apos;, &apos;horse&apos;, &apos;motorbike&apos;, &apos;person&apos;, &apos;pottedplant&apos;, &apos;sheep&apos;, &apos;sofa&apos;, &apos;train&apos;, &apos;tvmonitor&apos;)NETS = &#123;&apos;vgg16&apos;: (&apos;VGG16&apos;, &apos;VGG16_faster_rcnn_final.caffemodel&apos;), &apos;zf&apos;: (&apos;ZF&apos;, &apos;zf_faster_rcnn_iter_2000.caffemodel&apos;)&#125;def vis_detections(im, class_name, dets, thresh=0.5): &quot;&quot;&quot;Draw detected bounding boxes.&quot;&quot;&quot; inds = np.where(dets[:, -1] &gt;= thresh)[0] if len(inds) == 0: return im = im[:, :, (2, 1, 0)] fig, ax = plt.subplots(figsize=(12, 12)) ax.imshow(im, aspect=&apos;equal&apos;) for i in inds: bbox = dets[i, :4] score = dets[i, -1] ax.add_patch( plt.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1], fill=False, edgecolor=&apos;red&apos;, linewidth=3.5) ) ax.text(bbox[0], bbox[1] - 2, &apos;&#123;:s&#125; &#123;:.3f&#125;&apos;.format(class_name, score), bbox=dict(facecolor=&apos;blue&apos;, alpha=0.5), fontsize=14, color=&apos;white&apos;) ax.set_title((&apos;&#123;&#125; detections with &apos; &apos;p(&#123;&#125; | box) &gt;= &#123;:.1f&#125;&apos;).format(class_name, class_name, thresh), fontsize=14) plt.axis(&apos;off&apos;) plt.tight_layout() plt.draw()def demo(net, image_name): &quot;&quot;&quot;Detect object classes in an image using pre-computed object proposals.&quot;&quot;&quot; # Load the demo image im_file = os.path.join(cfg.DATA_DIR, &apos;demo&apos;, image_name) im = cv2.imread(im_file) # Detect all object classes and regress object bounds timer = Timer() timer.tic() scores, boxes = im_detect(net, im) timer.toc() print (&apos;Detection took &#123;:.3f&#125;s for &apos; &apos;&#123;:d&#125; object proposals&apos;).format(timer.total_time, boxes.shape[0]) # Visualize detections for each class CONF_THRESH = 0.8 NMS_THRESH = 0.3 for cls_ind, cls in enumerate(CLASSES[1:]): cls_ind += 1 # because we skipped background cls_boxes = boxes[:, 4*cls_ind:4*(cls_ind + 1)] cls_scores = scores[:, cls_ind] dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])).astype(np.float32) keep = nms(dets, NMS_THRESH) dets = dets[keep, :] vis_detections(im, cls, dets, thresh=CONF_THRESH)def parse_args(): &quot;&quot;&quot;Parse input arguments.&quot;&quot;&quot; parser = argparse.ArgumentParser(description=&apos;Faster R-CNN demo&apos;) parser.add_argument(&apos;--gpu&apos;, dest=&apos;gpu_id&apos;, help=&apos;GPU device id to use [0]&apos;, default=0, type=int) parser.add_argument(&apos;--cpu&apos;, dest=&apos;cpu_mode&apos;, help=&apos;Use CPU mode (overrides --gpu)&apos;, action=&apos;store_true&apos;) parser.add_argument(&apos;--net&apos;, dest=&apos;demo_net&apos;, help=&apos;Network to use [zf]&apos;, choices=NETS.keys(), default=&apos;zf&apos;) args = parser.parse_args() return argsif __name__ == &apos;__main__&apos;: cfg.TEST.HAS_RPN = True # Use RPN for proposals args = parse_args() prototxt = os.path.join(cfg.MODELS_DIR, NETS[args.demo_net][0], &apos;faster_rcnn_alt_opt&apos;, &apos;faster_rcnn_test.pt&apos;) caffemodel = os.path.join(cfg.DATA_DIR, &apos;faster_rcnn_models&apos;, NETS[args.demo_net][1]) if not os.path.isfile(caffemodel): raise IOError((&apos;&#123;:s&#125; not found.\nDid you run ./data/script/&apos; &apos;fetch_faster_rcnn_models.sh?&apos;).format(caffemodel)) if args.cpu_mode: caffe.set_mode_cpu() else: caffe.set_mode_gpu() caffe.set_device(args.gpu_id) cfg.GPU_ID = args.gpu_id net = caffe.Net(prototxt, caffemodel, caffe.TEST) #指定caffe路径，以下是我的caffe路径 caffe_root=&apos;/home/ouyang/GitRepository/py-faster-rcnn/caffe-fast-rcnn/&apos; # import sys sys.path.insert(0, caffe_root+&apos;python&apos;) # import caffe # #显示的图表大小为 10,图形的插值是以最近为原则,图像颜色是灰色 plt.rcParams[&apos;figure.figsize&apos;] = (10, 10) plt.rcParams[&apos;image.interpolation&apos;] = &apos;nearest&apos; plt.rcParams[&apos;image.cmap&apos;] = &apos;gray&apos; image_file = caffe_root+&apos;examples/images/vehicle_0000015.jpg&apos; # 载入模型 npload = caffe_root+ &apos;python/caffe/imagenet/ilsvrc_2012_mean.npy&apos; transformer = caffe.io.Transformer(&#123;&apos;data&apos;: net.blobs[&apos;data&apos;].data.shape&#125;) transformer.set_transpose(&apos;data&apos;, (2,0,1)) transformer.set_mean(&apos;data&apos;, np.load(npload).mean(1).mean(1)) # 参考模型的灰度为0~255，而不是0~1 transformer.set_raw_scale(&apos;data&apos;, 255) # 由于参考模型色彩是BGR,需要将其转换为RGB transformer.set_channel_swap(&apos;data&apos;, (2,1,0)) im=caffe.io.load_image(image_file) net.blobs[&apos;data&apos;].reshape(1,3,224,224) net.blobs[&apos;data&apos;].data[...] = transformer.preprocess(&apos;data&apos;,im) # 显示出各层的参数和形状，第一个是批次，第二个是feature map数目，第三和第四是每个神经元中图片的长和宽 print [(k,v.data.shape) for k,v in net.blobs.items()] #输出网络参数 print [(k,v[0].data.shape) for k,v in net.params.items()] def show_image(im): if im.ndim==3: m=im[:,:,::-1] plt.imshow(im) #显示图片的方法 plt.axis(&apos;off&apos;) # 不显示坐标轴 plt.show() # 每个可视化的都是在一个由一个个网格组成 def vis_square(data,padsize=1,padval=0): data-=data.min() data/=data.max() # force the number of filters to be square n=int(np.ceil(np.sqrt(data.shape[0]))) padding=((0,n**2-data.shape[0]),(0,padsize),(0,padsize))+((0,0),)*(data.ndim-3) data=np.pad(data,padding,mode=&apos;constant&apos;,constant_values=(padval,padval)) # 对图像使用滤波器 data=data.reshape((n,n)+data.shape[1:]).transpose((0,2,1,3)+tuple(range( 4,data.ndim+1))) data=data.reshape((n*data.shape[1],n*data.shape[3])+data.shape[4:]) #show_image(data) plt.imshow(data) plt.show() # 设置图片的保存路径，此处是我的路径 plt.savefig(&quot;./tools/Vehicle_2000/fc6.jpg&quot;) out = net.forward() image=net.blobs[&apos;data&apos;].data[4].copy() image-=image.min() image/=image.max() # 显示原始图像 show_image(image.transpose(1,2,0)) #网络提取conv1的卷积核 filters = net.params[&apos;conv1&apos;][0].data vis_square(filters.transpose(0, 2, 3, 1)) #过滤后的输出,96 张 featuremap feat =net.blobs[&apos;conv1&apos;].data[0,:96] vis_square(feat,padval=1) #第二个卷积层,显示全部的96个滤波器,每一个滤波器为一行。 filters = net.params[&apos;conv2&apos;][0].data vis_square(filters[:96].reshape(96**2, 5, 5)) # #第二层输出 256 张 featuremap feat = net.blobs[&apos;conv2&apos;].data[0] vis_square(feat, padval=1) filters = net.params[&apos;conv3&apos;][0].data vis_square(filters[:256].reshape(256**2, 3, 3)) # 第三个卷积层:全部 384 个 feature map feat = net.blobs[&apos;conv3&apos;].data[0] vis_square(feat, padval=0.5) #第四个卷积层,我们只显示前面 48 个滤波器,每一个滤波器为一行。 filters = net.params[&apos;conv4&apos;][0].data vis_square(filters[:384].reshape(384**2, 3, 3)) # 第四个卷积层:全部 384 个 feature map feat = net.blobs[&apos;conv4&apos;].data[0] vis_square(feat, padval=0.5) # 第五个卷积层:全部 256 个 feature map filters = net.params[&apos;conv5&apos;][0].data vis_square(filters[:384].reshape(384**2, 3, 3)) feat = net.blobs[&apos;conv5&apos;].data[0] vis_square(feat, padval=0.5) #第五个 pooling 层 feat = net.blobs[&apos;fc6&apos;].data[0] vis_square(feat, padval=1) 第六层输出后的直方分布 feat=net.blobs[&apos;fc6&apos;].data[0] plt.subplot(2,1,1) plt.plot(feat.flat) plt.subplot(2,1,2) _=plt.hist(feat.flat[feat.flat&gt;0],bins=100) # #显示图片的方法 #plt.axis(&apos;off&apos;) # 不显示坐标轴 plt.show() plt.savefig(&quot;fc6_zhifangtu.jpg&quot;) # 第七层输出后的直方分布 feat=net.blobs[&apos;fc7&apos;].data[0] plt.subplot(2,1,1) plt.plot(feat.flat) plt.subplot(2,1,2) _=plt.hist(feat.flat[feat.flat&gt;0],bins=100) plt.show() plt.savefig(&quot;fc7_zhifangtu.jpg&quot;) #看标签 #执行测试 image_labels_filename=caffe_root+&apos;data/ilsvrc12/synset_words.txt&apos; #try: labels=np.loadtxt(image_labels_filename,str,delimiter=&apos;\t&apos;) top_k=net.blobs[&apos;prob&apos;].data[0].flatten().argsort()[-1:-6:-1] #print labels[top_k] for i in np.arange(top_k.size): print top_k[i], labels[top_k[i]] &emsp;&emsp;下面贴几张检测结果 图3 原始检测图片 图4 conv1参数可视化 图5 conv1特征可视化 deep-visualization-toolbox&emsp;&emsp;deep-visualization-toolbox是Jason Yosinsk出版在Computer Science上的一篇论文的源代码，改论文主要讲述的是卷积神经网络的可视化，感兴趣的朋友可以看看这篇论文（论文地址）。B站上有个讲怎么使用该工具的视频，这里附上链接https://www.bilibili.com/video/av7405645/。&emsp;&emsp;该工具的源码在github：https://github.com/yosinski/deep-visualization-toolbox。该github下有完整的安装配置步骤，还是以图2中的马为例，贴几张检测结果图。 图6 ToolBox conv1特征可视化 图7 ToolBox conv2特征可视化 &emsp;&emsp;从检测效果上看，还是挺简洁的。图片左侧的一列图片左上角是输入图片，中间部分是图片经过网络前向传播得到的特征图可视化，左下角是其特征可视化。 Loss可视化&emsp;&emsp;网络训练过程中Loss值的可视化可以帮助分析该网络模型的参数是否合适。在使用Faster R-CNN网络训练模型时，训练完成后的日志文件中保存了网络训练各个阶段的loss值，如图8所示。只用写简单的python程序，读取日志文件中的迭代次数，以及需要的损失值，再画图即可完成Loss的可视化。 图8 模型的训练日志 &emsp;&emsp;在下面贴出Loss可视化的代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#!/usr/bin/env python import os import sys import numpy as np import matplotlib.pyplot as plt import math import re import pylab from pylab import figure, show, legend from mpl_toolkits.axes_grid1 import host_subplot # 日志文件名fp = open('faster_rcnn_end2end_ZF_.txt.2018-04-13_19-46-23', 'r',encoding='UTF-8') train_iterations = [] train_loss = [] test_iterations = [] #test_accuracy = [] for ln in fp: # get train_iterations and train_loss if '] Iteration ' in ln and 'loss = ' in ln: arr = re.findall(r'ion \b\d+\b,',ln) train_iterations.append(int(arr[0].strip(',')[4:])) train_loss.append(float(ln.strip().split(' = ')[-1])) fp.close() host = host_subplot(111) plt.subplots_adjust(right=0.8) # ajust the right boundary of the plot window #par1 = host.twinx() # set labels host.set_xlabel("iterations") host.set_ylabel("RPN loss") #par1.set_ylabel("validation accuracy") # plot curves p1, = host.plot(train_iterations, train_loss, label="train RPN loss") . host.legend(loc=1) # set label color host.axis["left"].label.set_color(p1.get_color()) host.set_xlim([-1000, 60000]) host.set_ylim([0., 3.5]) plt.draw() plt.show() &emsp;&emsp;可视化效果如下图所示 ![图9 Loss可视化](https://myblog-1258449291.cos.ap-beijing.myqcloud.com/Blog/loss可视化.png-chuli) 图9 Loss可视化 画PR图&emsp;&emsp;Faster R-CNN训练网络在输出网络模型的同级文件夹里有每一类检测目标每张图片的准确率和召回率，可以绘制准确率召回率(Precision-recall, PR)曲线，PR曲线的面积即准确率的值。&emsp;&emsp;该文件存储在==output\faster_rcnn_end2end\voc_2007_test\zf_faster_rcnn_iter==下的.pkl文件下，需要将其转换为.txt文件。代码如下： 1234567891011#-*-coding:utf-8-*-import cPickle as pickleimport numpy as np np.set_printoptions(threshold=np.NaN) fr = open('./aeroplane_pr.pkl') #open的参数是pkl文件的路径inf = pickle.load(fr) #读取pkl文件的内容print inffo = open("aeroplane_pr.txt", "wb")fo.write(str(inf))fo.close()fr.close() #关闭文件 &emsp;&emsp;执行完这个程序后，会将.pkl文件转换为.txt文件保存。.txt文件能直观看到每张图片的检测准确率与召回率。用与画loss图相似的方法，即可完成PR曲线的绘制。效果图如图10所示。 图10 PR曲线 参考文献[1] 薛开宇，caffe学习笔记[2] Yosinski J, Clune J, Nguyen A, et al. Understanding Neural Networks Through Deep Visualization[J]. Computer Science, 2015. 欢迎关注我的公众号]]></content>
      <categories>
        <category>深度学习笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
        <tag>Faster R-CNN</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络模型解读汇总——LeNet5，AlexNet、ZFNet、VGG16、GoogLeNet和ResNet]]></title>
    <url>%2Fpost%2Fd86a012b.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;在我的个人博客上一篇博文中分析了卷积神经网络的结构与相关算法，知道了这些基本原理之后。这篇博文主要介绍在卷积神经网络的发展历程中一些经典的网络模型。 LeNet5&emsp;&emsp;LeCun等将BP算法应用到多层神经网络中，提出LeNet-5模型[1]（效果和paper见此处），并将其用于手写数字识别，卷积神经网络才算正式提出。LeNet-5的网络模型如图1所示。网络模型具体参数如图2所示。 图1 LeNet-5网络模型 表1 LeNet-5具体参数输入：3232的手写字体图片，这些手写字体包含0~9数字，也就是相当于10个类别的图片;*输出：分类结果，0~9之间。&emsp;&emsp;从输入输出可以知道，改网络解决的是一个十分类的问题，分类器使用的Softamx回归。 C1：卷积核参数如表所示。卷积的后的尺寸计算公式为：outHeight=(inHeight+2pad-filterHeight)/strides[1]+1outWidth=(inWidth+2pad-filterHidth)/strides[2] +1&emsp;&emsp;因此，经过C1卷积层后，每个特征图大小为32-5+1=28，这一层输出的神经元个数为28286=784。而这一层卷积操作的参数个数为5516+6=156，其中参数个数与神经元个数无关，只与卷积核大小（此处为55），卷积核数量（此处为6，上一层图像默认深度为1）； S2：输入为28286，该网络使用最大池化进行下采样，池化大小为22，经过池化操作后输出神经元个数为1414*6； C3：经过C3层后，输出为101016，参数个数为556*16+16=2416个参数； S4：输入为101016，参数与S2层一致，池化后输出神经元个数为5516； C5：经过C5层后，输出为11120，参数个数为5516120+120=48120个参数。（这一层的卷积大小为55，图像的输入大小也为5*5，可等效为全连接层）； F6：输出为1184，参数个数为11120*84+84=10164参数总量：60856 &emsp;&emsp;从表1的具体参数可以看出，LeNet的网络结构十分简单且单一，卷积层C1、C3和C5层除了输出维数外采用的是相同的参数，池化层S2和S4采用的也是相同的参数 AlexNet&emsp;&emsp;2012年Krizhevsky使用卷积神经网络在ILSVRC 2012图像分类大赛上夺冠，提出了AlexNet模型[2]（论文地址）。这篇文章凭借着诸多创新的方法，促使了之后的神经网络研究浪潮。AlexNet网络的提出对于卷积神经网络具有里程碑式的意义，相比较于LeNet5的改进有以下几点 数据增强 水平翻转 随机裁剪、平移变换 颜色光照变换 Dropout： Dropout方法和数据增强一样，都是防止过拟合的。简单的说，dropout能按照一定的概率将神经元从网络中丢弃。一个很形象的解释如图2所示，左图为dropout前，右图为dropout后。dropout能在一定程度上防止网络过拟合，并且能加快网络的训练速度。图2 Dropout示意图 ReLU激活函数：ReLu具有一些优良特性，在为网络引入非线性的同时，也能引入稀疏性。稀疏性可以选择性激活和分布式激活神经元，能学习到相对稀疏的特征，起到自动化解离的效果。此外，ReLu的导数曲线在输入大于0时，函数的导数为1，这种特性能保证在输入大于0时梯度不衰减，从而避免或抑制网络训练时的梯度消失现象，网络模型的收敛速度会相对稳定[10]。 Local Response Normalization：Local Response Normalization要硬翻译的话是局部响应归一化，简称LRN，实际就是利用临近的数据做归一化。这个策略贡献了1.2%的Top-5错误率。 Overlapping Pooling：Overlapping的意思是有重叠，即Pooling的步长比Pooling Kernel的对应边要小。这个策略贡献了0.3%的Top-5错误率。 多GPU并行：这个太重要了，入坑了后发现深度学习真是“炼丹”的学科。得益于计算机硬件的发展，在我自己训练时，Gpu大概能比Cpu快一个数量级以上。能极大的加快网络训练。AlextNet的网络结构如图3所示，具体参数如表2所示。图3 AlexNet网络模型表2 AlexNet具体参数输入：2242243（RGB图像），图像会经过预处理变为2272273;输出：使用的是ImageNet数据集，该数据集有1000个类，因此输出的类别也是1000个。&emsp;&emsp;从输入输出可以知道，改网络解决的是一个十分类的问题，分类器使用的Softamx回归。 conv1：输出为555596，参数个数为11113*96+96=34944 pool1：输出为272796； conv2：输出为2727256，参数个数为5596*256+256=614656 pool2：输出为1313256； conv3：输出为1313384，参数个数为33256*384+384=885120 conv4：输出为1313384，参数个数为33384*384+384=1327488 conv5：输出为1313256，参数个数为33384*256+256=884992 pool3：输出为66256； fc6：输出为114096，参数个数为11256*4096+4096=1052672 fc7：输出为114096，参数个数为114096*4096+4096=16781312参数总量：21581184 &emsp;&emsp;通过对比LeNet-5和AlexNet的网络结构可以看出，AlexNet具有更深的网络结构，更多的参数。 ZFNet&emsp;&emsp;ZFNet[3]（论文地址）是由纽约大学的Matthew Zeiler和Rob Fergus所设计，该网络在AlexNet上进行了微小的改进，但这篇文章主要贡献在于在一定程度上解释了卷积神经网络为什么有效，以及如何提高网络的性能。该网络的贡献在于： 使用了反卷积网络，可视化了特征图。通过特征图证明了浅层网络学习到了图像的边缘、颜色和纹理特征，高层网络学习到了图像的抽象特征； 根据特征可视化，提出AlexNet第一个卷积层卷积核太大，导致提取到的特征模糊； 通过几组遮挡实验，对比分析找出了图像的关键部位； 论证了更深的网络模型，具有更好的性能。 &emsp;&emsp;ZFNet的网络模型如图4所示，具体参数如表3所示。 图4 ZFNet网络模型 表3 ZFNet具体参数 &emsp;&emsp;ZFNet的网络模型与AlexNet十分相似，这里就不列举每一层的输入输出了。 VGG16&emsp;&emsp;VGGNet[4]是由牛津大学计算机视觉组和Google DeepMind项目的研究员共同研发的卷积神经网络模型，包含VGG16和VGG19两种模型，其网络模型如图5所示，也可以点击此处链接查看网络模型。图5 VGG16网络模型&emsp;&emsp;从网络模型可以看出，VGG16相比AlexNet类的模型具有较深的深度，通过反复堆叠33的卷积层和22的池化层，VGG16构建了较深层次的网络结构，整个网络的卷积核使用了一致的33的尺寸，最大池化层尺寸也一致为22。与AlexNet主要有以下不同： Vgg16有16层网络，AlexNet只有8层； 在训练和测试时使用了多尺度做数据增强。 GoogLeNet&emsp;&emsp;GoogLeNet[5]（论文地址）进一步增加了网络模型的深度和宽度，但是单纯的在VGG16的基础上增加网络的宽度深度会带来以下缺陷： 过多的参数容易引起过拟合； 层数的加深，容易引起梯度消失现象。 &emsp;&emsp;GoogLeNet的提出受到论文Network in Network（NIN）的启发，NIN有两个贡献： 提出多层感知卷积层：使用卷积层后加上多层感知机，增强网络提取特征的能力。普通的卷积层和多层感知卷积层的结构图如图6所示，Mlpconv相当于在一般的卷积层后加了一个1*1的卷积层；图6 普通卷积层和多层感知卷积层结构图 提出了全局平均池化替代全连接层，从上文计算的LeNet5，AlexNet网络各层的参数数量发现，全连接层具有大量的参数。使用全局平均池化替代全连接层，能很大程度减少参数空间，便于加深网络，还能防止过拟合。 &emsp;&emsp;GoogLeNet根据Mlpconv的思想提出了Inception结构，该结构有两个版本，图7是Inception的naive版。该结构巧妙的将11、33和5*5三种卷积核和最大池化层结合起来作为一层结构。 图7 Inception结构的naive版&emsp;&emsp;然而Inception的naive版中55的卷积核会带来很大的计算量，因此采用了与NIN类似的结构，在原始的卷积层之后加上了11卷积层，最终版本的Inception如图8所示。 图8 降维后的Inception模块&emsp;&emsp;GoogLeNet的模型结构如图9所示，详细参数如表4所示。 图9 GoogLeNet模型结构 表4 GoogLeNet具体参数 ResNet&emsp;&emsp;卷积神经网络模型的发展历程一次次证明加深网络的深度和宽度能得到更好的效果，但是后来的研究发现，网络层次较深的网络模型的效果反而会不如较浅层的网络，称为“退化”现象，如图10所示。 图10 退化现象&emsp;&emsp;退化现象产生的原因在于当模型的结构变得复杂时，随机梯度下降的优化变得更加困难，导致网络模型的效果反而不如浅层网络。针对这个问题，MSRA何凯明团队提出了Residual Networks6)。该网络具有Residual结构如图11所示。 图11 Residual 结构&emsp;&emsp;ResNet的基本思想是引入了能够跳过一层或多层的“shortcut connection”，即增加一个identity mapping（恒等映射），将原始所需要学的函数H(x)转换成F(x)+x，而作者认为这两种表达的效果相同，但是优化的难度却并不相同，作者假设F(x)的优化 会比H(x)简单的多。这一想法也是源于图像处理中的残差向量编码，通过一个reformulation，将一个问题分解成多个尺度直接的残差问题，能够很好的起到优化训练的效果。&emsp;&emsp;这个Residual block通过shortcut connection实现，通过shortcut将这个block的输入和输出进行一个element-wise的加叠，这个简单的加法并不会给网络增加额外的参数和计算量，同时却可以大大增加模型的训练速度、提高训练效果，并且当模型的层数加深时，这个简单的结构能够很好的解决退化问题。&emsp;&emsp;首先构建了一个18层和一个34层的plain网络，即将所有层进行简单的铺叠，然后构建了一个18层和一个34层的residual网络，仅仅是在plain上插入了shortcut，而且这两个网络的参数量、计算量相同，并且和之前有很好效果的VGG-19相比，计算量要小很多。（36亿FLOPs VS 196亿FLOPs，FLOPs即每秒浮点运算次数。）这也是作者反复强调的地方，也是这个模型最大的优势所在。&emsp;&emsp;模型构建好后进行实验，在plain上观测到明显的退化现象，而且ResNet上不仅没有退化，34层网络的效果反而比18层的更好，而且不仅如此，ResNet的收敛速度比plain的要快得多。对于shortcut的方式，作者提出了三个策略： 使用恒等映射，如果residual block的输入输出维度不一致，对增加的维度用0来填充； 在block输入输出维度一致时使用恒等映射，不一致时使用线性投影以保证维度一致； 对于所有的block均使用线性投影。ResNet论文的最后探讨了阻碍网络更深的瓶颈问题，如图12所示，论文中用三个1x1,3x3,1x1的卷积层代替前面说的两个3x3卷积层，第一个1x1用来降低维度，第三个1x1用来增加维度，这样可以保证中间的3x3卷积层拥有比较小的输入输出维度。图12 更深的residual block参考文献[1] Lecun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.[2] Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems. Curran Associates Inc. 2012:1097-1105.[3] Zeiler M D, Fergus R. Visualizing and Understanding Convolutional Networks[J]. 2013, 8689:818-833.[4] Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.[5] Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2015:1-9.[6] He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]// Computer Vision and Pattern Recognition. IEEE, 2016:770-778. 欢迎关注我的公众号]]></content>
      <categories>
        <category>深度学习笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
        <tag>LeNet5</tag>
        <tag>AlexNet</tag>
        <tag>ZFNet</tag>
        <tag>VGG16</tag>
        <tag>GoogLeNet</tag>
        <tag>ResNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络的结构与相关算法]]></title>
    <url>%2Fpost%2F4ec18e58.html</url>
    <content type="text"><![CDATA[引言&emsp;&emsp;早在上世纪50年代，美国神经生物学家David Hubel通过研究猫和猴子的瞳孔区域与大脑皮层神经元的对应关系就发现视觉系统的信息处理方式是分级的。这一发现，促成了神经网络在图像处理上的发展。&emsp;&emsp;神经网络的发展史可以分为三个阶段，第一个阶段是Frank Rosenblatt提出的感知机模型[1]，感知机模型的逻辑简单有效，但不能处理异或等非线性问题。第二个阶段是Rumelhart等提出的反向传播算法[2]，该算法使用梯度更新权值，使多层神经网络的训练成为可能。第三个阶段得益于计算机硬件的发展和大数据时代的到来，促进了深度神经网络的发展。在上一篇博客什么是深度学习中介绍了深度学习的三种网络模型，其中卷积神经网络在图像处理上有着诸多突破性的进展。由于我对卷积神经网络较为熟悉，下面将根据神经网络的三个发展阶段阶段分析讨论卷积神经网络的发展史。 传统神经网络感知机与多层网络&emsp;&emsp;感知机（Perceptron）由两层神经元组成，是一种二分类的线性分类模型，也是最简单的单层前馈神经网络（Feedforward Neural Network）模型。感知机的提出受到生物神经元的启发，神经元在处理突触传递而来的电信号后，若产生的刺激大于一定的阈值，则神经元被激活，感知机也具有类似的结构。假设输入空间是χ⊆R^n，输出空间是y={+1,-1}，x和y分别属于两个空间，则由感知机表示的由输入空间到输出空间的函数为：f(x)=sign(w∙x+b)&emsp;&emsp;其中w和b是感知机模型的参数，w∈R^n称为权重(Weight)，b∈R^n称为偏置(Bias)，w∙x表示两个向量间的内积。&emsp;&emsp;Minsky和Papert已证明若决策区域类型是线性可分的，则感知机一定会学习到收敛的参数权重w和偏置b，否则感知机会发生震荡[3]（fluctuation）。因此，感知机在线性可分数据中表现良好，如果设定足够的迭代次数，能很好的处理近似线性可分的数据。但如果对非线性可分的数据，如异或问题，单层感知机不能有效的解决。由于不能用一条直线划分样本空间，有学者想到用多条直线来划分样本，多层感知机就是这样一个模型。多层感知机结构如图1所示，相比较于单层感知机，多层感知机增加了隐藏层的层数。 图1 多层感知机结构图 &emsp;&emsp;随着隐藏层数的增加，感知机的分类能力如表1所示。 表1 感知机分类能力比较 &emsp;&emsp;由表可知，在异或问题中，无隐层的感知机不能解决异或问题，引入了隐层后，异或问题得到解决，而随着层数越多，对于异或问题的拟合会越来越好。这说明，在感知机中随着隐藏层层数的增多，决策区域可以拟合任意的区域，因此理论上多层感知机可以解决任何线性或非线性的分类问题。但是，Minsky和Papert提出隐藏层的权重和偏置参数无法训练，这是由于隐藏层不存在期望的输出，无法通过单层感知机的训练方式训练多层感知机[4]。 反向传播算法&emsp;&emsp;如何训练多层感知机的难点在很长一段时间没有得到解决，要训练多层网络，需要更有效的学习算法。反向传播（BackPropagation，BP）算法[1]是训练多层网络的常用方法，该方法用链式法则对网络中所有权重和偏置计算损失函数的梯度，将梯度反馈给随机梯度下降或其它最优化算法，用来更新权值以最小化损失函数。在网络中，正向传播和反向传播的过程如图2所示。在这个例子中输入图片经过网络正向传播后得到的分类是狗，与实际类别的人脸不符，此时会将误差逐层反向传播，修正各个层的权重和偏置参数后，再进行正向传播，反复迭代，直至网络的参数能正确的分类输入的图片。 图2 BP网络训练过程&emsp;&emsp;反向传播算法的主要步骤如下： 随机初始化多层网络的权重和偏置参数，将训练数据送入多层网络的输入层，经过隐藏层和输出层，得到输出结果。完成网络的前向传播过程； 计算输出层实际值和输出值间的偏差，根据反向传播算法中的链式法则，得到每个隐藏层的误差，根据每层的误差调整各层的参数。完成网络的反向传播过程； 不断迭代前两步中的正向传播和反向传播过程，直至网络收敛。 &emsp;&emsp;由于还不熟悉markdown的公式编辑，这里省去反向传播的推导过程，感兴趣的朋友可以阅读周志华教授的《机器学习》第五章神经网络里面有详尽的推导过程。 卷积神经网络的基本思想&emsp;&emsp;在BP神经网络中，每一层都是全连接的，参数数量随着网络宽度和深度增加会指数级增长。多层网络结合BP算法对输入数据虽然有强大的表示能力，但巨大的参数一方面限制了每层能够容纳的最大神经元数量，另一方面也限制了神经网络的深度。受到动物视觉皮层中感受野的启发，效仿这种结构的卷积神经网络具有局部连接和参数共享的特点，可以有效的减少网络的相关参数数量，优化网络的训练速度。 局部连接&emsp;&emsp;Hubel和Wiesel在二十世纪五十年代和六十年代的研究表明，猫和猴子的视觉皮层中的神经元只响应特定的某些区域的刺激。将这种视觉刺激影响单个神经元反应的区域称为感受野（receptive field），相邻神经元细胞具有相同或相似的感受野[5]。正是由于发现了感受野等功能在猫的视觉神经中枢中的作用，催生了日本学者福岛邦彦提出带卷积和下采样层的多层卷积神经网络[6-8]。&emsp;&emsp;当我们在处理一副图像时，其输入往往是高维的。传统的神经网络将下一层神经元连接到上一层所有神经元。这种方式随着网络层数的增加，参数数量会爆炸式增加，在实际运用中，会无法训练网络。卷积神经网络中采取的做法是将每个神经元连接到上一层的部分神经元。这种连接的空间范围是一个超参数，称为神经元的感受野，感受野实际上是神经元映射到输入图像矩阵空间的大小。&emsp;&emsp;局部连接的实现方式是引入卷积层，通过卷积层对应局部的图像，每一层的神经元组合在一起对应图像的全局信息。如图3所示，在网络的第m层，每个神经元感受野大小为3，能连接到上一层的3个神经元。m+1层与m层类似。随着层数增加，神经元相对于输入层的感受野会越来越大。每个神经元不会响应感受野以外神经元的变化。受启发于动物的视觉神经元只响应局部信息，这样的结构确保了卷积神经网络只响应上一层局部神经元的变化，起到过滤作用的同时，减少了网络参数。而且随着层数的增加，这种过滤作用会越来越全局。 图3 局部连接 权值共享&emsp;&emsp;卷积的优点除了局部连接外还有权值共享。如图4所示，假设第m-1层有5个神经元，m层有3个神经元，对第m-1层的特征进行卷积，得到第m层共有3个单元的输出特征图。虽然第m层每个神经元都与第m-1层中的3个神经元连接，但同一组卷积操作的权重参数相同。在这个例子中，通过权值共享，将9个参数较少到了3个。 图4 权值共享&emsp;&emsp;卷积神经网络中权值共享的实现方式是让同一个卷积核去卷积整张图像，生成一整张特征图[9]。在卷积操作中，同一个卷积核内，所有神经元共享相同权值，权值共享的策略可以很大程度上降低网络需要计算的参数数量。通过权值共享，不仅大大增加了参数的训练效率，而且提取的特征在一定程度上具有位置不变性，加强了特征对输入图像的表达能力。 卷积神经网络结构&emsp;&emsp;卷积神经网络是一种层次模型（Hierarchical Model），其输入是RGB图像，视频，音频等数据。卷积神经网络通过一系列卷积（Convolution）操作，非线性激活函数（Non-linear Activation Function），池化（Pooling）操作层层堆叠，逐层从原始数据获取高层语义信息[10]。如图5所示，在结构上，卷积神经网络分类器有四种类型的网络层：卷积层、池化层、全连接层和分类器。各层次之间的有如下约束：（1）多个卷积（C）和池化（S）层，将上一层的输出图像与本层权重W做卷积得到各个C层，然后经过下采样得到S层。（2）全连接层：全连接层的输入是最后一个卷积池化层的输出，其输出是一个N维的列向量，维度对应类别的个数。（3）分类器：p_1，p_2，p_n的具体数值代表输入图像属于各类别的概率，分类器根据提取到的特征向量将检测目标划分到合适的类中。 图5 卷积神经网络分类器 卷积&emsp;&emsp;图片有着固有的特性，这意味着，图像的一部分特征与其他部分相似，对一张图片学习到的一部分特征可以用于其他部分。卷积操作受启发于这种特性，具体操作如图6所示，输入图片大小为5×5，经过卷积核大小为3×3的卷积后，原来的输入空间映射到3×3的区域。再经过一次相同大小的卷积核后，图片大小变为了1×1。可见，卷积层逐层提取特征的方式，能从庞大的像素矩阵中，提取到对图像更有代表性的特征。卷积层最重要的是卷积核的设计，卷积核有几个参数:大小、步长、数目、边界填充。这些参数会对卷积的效果带来很大的影响。若卷积核设计的较大，如AlexNet[11]中使用的11×11和5×5的卷积核，其感受野很大，能覆盖图像更大的区域，对图像的“抽象”能力会较好，但较大的卷积核也会带来参数过多的负面影响。卷积核的步长指卷积每次滑动的距离，在一定程度上影响了特征提取的好坏。每一层网络的多个卷积核保证了提取到的特征是图像的多个方面，但卷积核的数量也不是越多越好，过多的卷积核会增加参数数量，计算复杂的同时容易过拟合。边界填充可用于卷积核与图像尺寸不匹配时，填充图像缺失区域。 图6 卷积示意图 池化&emsp;&emsp;卷积后的特征依然十分巨大，不仅带来计算性能的下降，也会产生过拟合。于是产生了对一块区域特征进行聚合统计的想法.例如，可以计算图像在某一块区域内的最大值或平均值代替这一块区域的特征，在降低特征维度的同时能使提取到的特征更具有代表性，还会使得处理过后的特征图谱拥有更大的感受野，这种用部分特征代替整体特征的操作称为池化[10]（Pooling）。常用的池化方法如下：最大值池化（Max Pooling）；均值池化（Mean Pooling）；随机池化（Random Pooling）。池化操作具有以下优良特性：（1）平移不变性（Translation Invariant）。无论是哪种池化方式，提取的都是局部特征。池化操作会模糊特征的具体位置，图像发生了平移后，依然能产生相同的特征。（2）特征降维（Feature Dimension Reduction），池化操作将一个局部区域的特征进一步抽象，池化中的一个元素对应输入数据中的一个区域，可以减少参数数量，降低维度。&emsp;&emsp;池化操作的功能是减小特征空间的大小，以减小网络中的参数和计算量，从而避免过度拟合。如图7所示，224×224×64经过大小为2×2，步长为2的池化核，变成了112×112×64，使得特征图谱减少为原来的1/2。图7中池化方式是最大池化，即将一个区域内的最大值表示为这个区域的池化结果。 图7 池化示意图 全连接层&emsp;&emsp;前面讨论的卷积层，池化层等操作是将原始数据映射到特征空间，使得到的特征矩阵越来越抽象并对特征有良好的表达能力。Softmax分类器要求输入是列向量，需要全连接层将卷积和池化的输出映射到线性可分空间。全连接层可以聚合卷积和池化操作得到的高阶特征，并且可以简化参数模型，一定程度的减少神经元数量和训练参数。为了能用反向传播算法训练网络，全连接层要求图片有固定的输入尺寸。因此早期网络中，需要对不同尺寸的图片进行裁剪或拉伸，这种操作会带来图片信息的失真和损失。在第三章讨论的感兴趣（Region of Interest）池化方法，可以很好的解决这一问题。&emsp;&emsp;卷积层是由全连接层发展而来，全连接层可以用特殊的卷积层表示，对于前一层全连接的全连接层可以用卷积核大小为1×1的卷积层替代，而对于前一层是卷积的全连接层可以用对上一层所有输入全局卷积的卷积层替代。在全连接层中可以认为每个神经元的感受野是整个图像。全连接层隐藏层节点数越多，模型拟合能力越强，但参数冗余会带来过拟合的风险而且会降低效率。对于这个问题，一般的做法是采用正则化（Regularization）技术，如L1、L2范式。还有通过Dropout随机舍弃一些神经元，来减少权重连接，然后增强网络模型在缺失个体连接信息情况下的鲁棒性[10]。 分类器&emsp;&emsp;经过全连接层将特征映射到线性空间后，最后还需要将实例数据划分到合适的分类中。分类器有多种，常用的有支持向量机和Softmax回归，此处以Softmax为例子。Softmax函数用于将多个神经元的输出映射到(0,1)之间，转化为概率问题，从而处理多分类问题。如图8所示，Softmax层的输入分别是3、1和-3，在经过Softmax层后分别映射为0.85、0.12和0.03，三个值的累加和为1，其数值可以理解为概率，则属于y_1类的概率最大为0.85。这幅图是Softmax的通俗理解，具体推导过程可以参考这篇文章。 图8 Softmax层 参考文献[1] Rumelhart D E, Hinton G E, Williams R J. Learning representations by back-propagating errors [J]. Nature, 1986, 323(6088): 533-536.[2] Hinton G E, Osindero S, Teh Y W. A fast learning algorithm for deep belief nets [J]. Neural Computation, 2006, 18(7): 1527-1554.[3] Minsky M, Papert S. Perceptrons: An introduction to computational geometry [J]. 1969, 75(3): 3356-3362.[4] Minsky M L, Papert S A. Perceptrons (expanded edition) mit press [J]. 1988.[5] 刘建立, 沈菁, 王蕾, 等. 织物纹理的简单视神经细胞感受野的选择特性 [J]. 计算机工程与应用, 2014, 50(1): 185-190.[6] Fukushima K. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position [J]. Biological Cybernetics, 1980, 36(4): 193-202.[7] Fukushima K. Neocognitron: A hierarchical neural network capable of visual pattern recognition [J]. Neural Networks, 1988, 1(2): 119-130.[8] Fukushima K, Miyake S. Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position [J]. Pattern Recognition, 1982, 15(6): 455-469.[9] 曹婷. 一种基于改进卷积神经网络的目标识别方法研究 [D]. 湖南大学, 2016.[10] LeCun Y, Boser B, Denker J S, et al. Backpropagation applied to handwritten zip code recognition [J]. Neural computation, 1989, 1(4): 541-551.[11] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks [C]. Proceedings of 26th Annual Conference on Neural Information Processing Systems, Nevada:NIPS, 2012: 1097-1105. 欢迎关注我的公众号]]></content>
      <categories>
        <category>深度学习笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
        <tag>BP算法</tag>
        <tag>感知机</tag>
        <tag>卷积</tag>
        <tag>池化</tag>
        <tag>全连接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是深度学习]]></title>
    <url>%2Fpost%2F309a28b1.html</url>
    <content type="text"><![CDATA[机器学习&emsp;&emsp;比起深度学习，“机器学习”一词更耳熟能详。机器学习是人工智能的一个分支，它致力于研究如何通过计算的手段，利用经验来改善计算机系统自身的性能。通过从经验中获取知识，机器学习算法摒弃了人为向机器输入知识的操作，转而凭借算法自身来学到所需所有知识。对于传统机器学习算法而言，“经验”往往对应以“特征”形式存储的“数据”，传统机器学习算法所做的事情便是依靠这些数据产生“模型”。&emsp;&emsp;特征的意义是找一个更好的空间去重构表达数据，把原始数据映射到高维空间，更便于划分不同类的数据。特征的选取是机器学习的核心，通常线性可分的数据用最简单的感知机即可划分，而现实应用中的数据往往是高维复杂的，传统的特征提取的方式可以归纳为以下几种： 依据经验人工挑选：如关于天气的数据集，如果是预测是否下雨，可以挑选与降雨密切相关的特征：季节、紫外线指数、温度、湿度、是否有云、风向和风速等属性。 线性特征选择：假设特征之间相互独立，不存在交互，那么可以使用卡方检验、信息增益、互信息等方法逐个检验特征与结果之间的相关程度。更为简便的方法是使用LR等线性模型，先做一次预训练，根据特征对应的线性模型权值的绝对值大小来对特征的重要程度进行排序。 非线性特征选择：如果属性之间不是相互独立，可以使用随机森林来进行特征选择，概括来说就是将想要检验重要性的特征在样本上进行permutation，然后观察OOB错误的上升程度，上升越大，说明这个特征越重要。 &emsp;&emsp;以上介绍的都是传统的特征提取方式，而随着机器学习任务的复杂多变，现有的特征提取方法表现出了诸多弊端，针对一个数据集设计特征提取方法不仅费时费力，而且还十分敏感，换成其他的任务，表现往往不尽人意。得益于计算机硬件的发展和大数据时代的到来，计算机拥有了能处理大量数据的前提和能力，促进了深度学习的发展。 深度学习的实质&emsp;&emsp;深度学习以原始数据作为输入，经过算法层层的将数据抽象为自身任务所需要的最终特征表示。通过大量的数据逐层学习特征，免去了传统特征提取过程中人类先验知识的影响。通过数据自主的学习特征，以获取输入信息更本质的特征[1, 2]。&emsp;&emsp;深度学习的实质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。因此，“深度模型”是手段，“特征学习”是目的。区别于传统的浅层学习，深度学习的不同在于： 强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点； 明确突出了特征学习的重要性，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。 与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。 深度学习的网络模型&emsp;&emsp;相比较于传统的机器学习算法，深度学习除了模型学习，还有特征学习、特征抽象等任务模块的参与，借助多层任务模块完成最终学习任务，故称为“深度”学习。深度学习发展到如今已经有了多种结构的深度神经网络模型，如： 由多个受限玻尔兹曼机组成的深度信念网络（Deep Belief Network，DBN）[3]; 应用于自然语言处理的循环神经网络（Recurrent Neural Network，RNN）[4]; 具有局部连接和权值共享等优点的卷积神经网络（Convolutional Neural Network，CNN）[5]。参考文献[1] Bottou L, Chapelle O, Decoste D, et al. Scaling learning algorithms towards AI[J]. Large-scale kernel machines,2007,34(5): 321-359.[2] Bengio Y, Delalleau O. On the expressive power of deep architectures [C]. Proceedings of International Conference on Algorithmic Learning Theory, Springer-Verlag, 2011: 18-36.[3] Mikolov T, Karafiát M, Burget L, et al. Recurrent neural network based language model [C]. Proceedings of 11th Annual Conference of the International Speech Communication Association, Chiba: Interspeech, 2010: 1045-1048.[4] LeCun Y, Boser B, Denker J S, et al. Backpropagation applied to handwritten zip code recognition [J]. Neural computation, 1989, 1(4): 541-551. 欢迎关注我的公众号]]></content>
      <categories>
        <category>深度学习笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>特征提取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Github和Hexo的个人博客搭建]]></title>
    <url>%2Fpost%2Fc57581c4.html</url>
    <content type="text"><![CDATA[写在前面&emsp;&emsp;去年起看了很多大牛的博客，也萌生了搭建个人博客的想法，为什么搭建博客，总结下来有以下好处： 书写是为了更好的思考 激励自己持续学习 尝试持之以恒的去做一些事情 &emsp;&emsp;这次趁着大论文盲审结果还没出来和导师还没反馈小论文修改意见的间隙，花了一个周末的时间搭建好了自己的个人博客。在此将搭建过程作为自己的第一篇博客记录下来。 Hexo简介&emsp;&emsp;Hexo 是一个基于 Node.js 的静态博客程序，可以方便的生成静态网页托管在github和Heroku上。Hexo有着丰富的主题，可以定制多种样式。hexo特性： 速度快：Hexo基于Node.js，支持多进程，几百篇文章也可以秒生成； 撰写工具丰富：支持GitHub Flavored Markdown和所有Octopress的插件； 扩展性强： Hexo支持EJS、Swig和Stylus。通过插件支持Haml、Jade和Less。 &emsp;&emsp;使用hexo时，有以下常用命令：12345678910npm install hexo -g #安装npm install hexo -g #安装 npm update hexo -g #升级 hexo init #初始化hexo n "我的博客" == hexo new "我的博客" #新建文章hexo p == hexo publishhexo g == hexo generate#生成hexo s == hexo server #启动服务预览hexo d == hexo deploy#部署hexo new 创建文章 环境搭建(node,hexo,git)&emsp;&emsp;前面说过hexo是基于node.js，因此需要先安装node.js。git的配置这里就不再赘述，可以参考廖雪峰老师的Git教程。安装好Git后，需要将其与自己的GitHub账号关联上。安装好node.js后，仅需一步即可安装hexo的相关套件。在命令行输入: npm install hexo -g hexo-cli &emsp;&emsp;到这一步就安装好了所需的所用环境。 设置&emsp;&emsp;在搭建自己的博客前，需要设置一个博客的根目录。使用命令行切换到该根目录下，输入： hexo init blog &emsp;&emsp;等待片刻，成功后会提示INFO Start blogging with Hexo!初始化成功后，目录如下： . ├── _config.yml ├── package.json ├── scaffolds ├── source | ├── _drafts | └── _posts └── themes &emsp;&emsp;source的_posts目录下会自带一篇题为“Hello World”的示例文章，直接执行以下操作可以看到网站初步的模样: $ hexo generate # 启动本地服务器 $ hexo server # 在浏览器输入 http://localhost:4000/就可以看见网页和模板了,若端口号被占用，可输入hexo server -p 4001改为其他端口号。 &emsp;&emsp;访问http://localhost:4000/，界面如下： 部署及配置博客配置SSH&emsp;&emsp;在上一步看到了网站的默认效果，此时需要将该博客部署到Github上，登陆Github，创建名为your_name.github.io(your_name替换成你的用户名)的仓库。重新打开CMD,输入： ssh-keygen -t rsa -C &quot;Github的注册邮箱地址&quot; &emsp;&emsp;一路Enter，得到信息：Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub.根据保存的路径找到id_rsa.pub文件，用编辑器打开，复制所有内容，然后Sign in to GitHub，按以下步骤配置SSH：&emsp;&emsp;New SSH key ——&gt;Title：blog ——&gt; Key：输入刚才复制的—— &gt;Add SSH key 配置博客&emsp;&emsp;在blog目录下，用编辑器打开_config.yml，修改其中的配置信息。&emsp;&emsp;修改网站中的相关信息 ：1234567title: #标题subtitle: #副标题description: #站点描述author: #作者language: zh-Hansemail: #电子邮箱timezone: Asia/Shanghai &emsp;&emsp;配置部署仓库 1234deploy: type: git repo: 刚刚github创库地址.git branch: master &emsp;&emsp;特别提醒，在每个参数的：后都要加一个空格。以上操作完成后，执行： 1234hexo clean #清除缓存 网页正常情况下可以忽略此条命令hexo generate #生成hexo server #启动服务预览，非必要，可本地浏览网页hexo deploy #部署发布 &emsp;&emsp;得到提示信息INFO Deploy done: git表示成功发布到Github上。然后在浏览器里输入your_name.github.io就可以访问刚刚配置好的博客了。 后记&emsp;&emsp;到此为止，最基本的hexo+github搭建博客完结。hexo有许多优美简洁的主题，网上也有许多关于主题美化的教程，可以根据自己的喜好添加各种或实用或酷炫的功能。 欢迎关注我的公众号]]></content>
      <categories>
        <category>Hexo博客搭建教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>Github</tag>
      </tags>
  </entry>
</search>
