<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[卷积神经网络模型解读汇总——LeNet5，AlexNet、ZFNet、VGG16、GoogLeNet和ResNet]]></title>
    <url>%2F2018%2F04%2F26%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%A7%A3%E8%AF%BB%E6%B1%87%E6%80%BB%E2%80%94%E2%80%94LeNet5%EF%BC%8CAlexNet%E3%80%81ZFNet%E3%80%81VGG16%E3%80%81GoogLeNet%E5%92%8CResNet%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;在我的个人博客上一篇博文中分析了卷积神经网络的结构与相关算法，知道了这些基本原理之后。这篇博文主要介绍在卷积神经网络的发展历程中一些经典的网络模型。 LeNet5&emsp;&emsp;LeCun等将BP算法应用到多层神经网络中，提出LeNet-5模型[1]（效果和paper见此处），并将其用于手写数字识别，卷积神经网络才算正式提出。LeNet-5的网络模型如图1所示。网络模型具体参数如图2所示。 图1 LeNet-5网络模型 表1 LeNet-5具体参数输入：32*32的手写字体图片，这些手写字体包含0~9数字，也就是相当于10个类别的图片;输出：分类结果，0~9之间。&emsp;&emsp;从输入输出可以知道，改网络解决的是一个十分类的问题，分类器使用的Softamx回归。 C1：卷积核参数如表所示。卷积的后的尺寸计算公式为：$$outHeight=(inHeight+2pad-filterHeight)/strides[1]+1 $$$$outWidth=(inWidth+2pad-filterHidth)/strides[2] +1 $$&emsp;&emsp;因此，经过C1卷积层后，每个特征图大小为32-5+1=28，这一层输出的神经元个数为28286=784。而这一层卷积操作的参数个数为5516+6=156，其中参数个数与神经元个数无关，只与卷积核大小（此处为55），卷积核数量（此处为6，上一层图像默认深度为1）； S2：输入为28286，该网络使用最大池化进行下采样，池化大小为22，经过池化操作后输出神经元个数为1414*6； C3：经过C3层后，输出为101016，参数个数为556*16+16=2416个参数； S4：输入为101016，参数与S2层一致，池化后输出神经元个数为5516； C5：经过C5层后，输出为11120，参数个数为5516120+120=48120个参数。（这一层的卷积大小为55，图像的输入大小也为5*5，可等效为全连接层）； F6：输出为1184，参数个数为11120*84+84=10164参数总量：60856 &emsp;&emsp;从表1的具体参数可以看出，LeNet的网络结构十分简单且单一，卷积层C1、C3和C5层除了输出维数外采用的是相同的参数，池化层S2和S4采用的也是相同的参数 AlexNet&emsp;&emsp;2012年Krizhevsky使用卷积神经网络在ILSVRC 2012图像分类大赛上夺冠，提出了AlexNet模型[2]（论文地址）。这篇文章凭借着诸多创新的方法，促使了之后的神经网络研究浪潮。AlexNet网络的提出对于卷积神经网络具有里程碑式的意义，相比较于LeNet5的改进有以下几点 数据增强 水平翻转 随机裁剪、平移变换 颜色光照变换 Dropout： Dropout方法和数据增强一样，都是防止过拟合的。简单的说，dropout能按照一定的概率将神经元从网络中丢弃。一个很形象的解释如图2所示，左图为dropout前，右图为dropout后。dropout能在一定程度上防止网络过拟合，并且能加快网络的训练速度。图2 Dropout示意图 ReLU激活函数：ReLu具有一些优良特性，在为网络引入非线性的同时，也能引入稀疏性。稀疏性可以选择性激活和分布式激活神经元，能学习到相对稀疏的特征，起到自动化解离的效果。此外，ReLu的导数曲线在输入大于0时，函数的导数为1，这种特性能保证在输入大于0时梯度不衰减，从而避免或抑制网络训练时的梯度消失现象，网络模型的收敛速度会相对稳定[10]。 Local Response Normalization：Local Response Normalization要硬翻译的话是局部响应归一化，简称LRN，实际就是利用临近的数据做归一化。这个策略贡献了1.2%的Top-5错误率。 Overlapping Pooling：Overlapping的意思是有重叠，即Pooling的步长比Pooling Kernel的对应边要小。这个策略贡献了0.3%的Top-5错误率。 多GPU并行：这个太重要了，入坑了后发现深度学习真是“炼丹”的学科。得益于计算机硬件的发展，在我自己训练时，Gpu大概能比Cpu快一个数量级以上。能极大的加快网络训练。AlextNet的网络结构如图3所示，具体参数如表2所示。图3 AlexNet网络模型表2 AlexNet具体参数输入：2242243（RGB图像），图像会经过预处理变为2272273;输出：使用的是ImageNet数据集，该数据集有1000个类，因此输出的类别也是1000个。&emsp;&emsp;从输入输出可以知道，改网络解决的是一个十分类的问题，分类器使用的Softamx回归。 conv1：输出为555596，参数个数为11113*96+96=34944 pool1：输出为272796； conv2：输出为2727256，参数个数为5596*256+256=614656 pool2：输出为1313256； conv3：输出为1313384，参数个数为33256*384+384=885120 conv4：输出为1313384，参数个数为33384*384+384=1327488 conv5：输出为1313256，参数个数为33384*256+256=884992 pool3：输出为66256； fc6：输出为114096，参数个数为11256*4096+4096=1052672 fc7：输出为114096，参数个数为114096*4096+4096=16781312参数总量：21581184 &emsp;&emsp;通过对比LeNet-5和AlexNet的网络结构可以看出，AlexNet具有更深的网络结构，更多的参数。 ZFNet&emsp;&emsp;ZFNet[3]（论文地址）是由纽约大学的Matthew Zeiler和Rob Fergus所设计，该网络在AlexNet上进行了微小的改进，但这篇文章主要贡献在于在一定程度上解释了卷积神经网络为什么有效，以及如何提高网络的性能。该网络的贡献在于： 使用了反卷积网络，可视化了特征图。通过特征图证明了浅层网络学习到了图像的边缘、颜色和纹理特征，高层网络学习到了图像的抽象特征； 根据特征可视化，提出AlexNet第一个卷积层卷积核太大，导致提取到的特征模糊； 通过几组遮挡实验，对比分析找出了图像的关键部位； 论证了更深的网络模型，具有更好的性能。 &emsp;&emsp;ZFNet的网络模型如图4所示，具体参数如表3所示。 图4 ZFNet网络模型 表3 ZFNet具体参数 &emsp;&emsp;ZFNet的网络模型与AlexNet十分相似，这里就不列举每一层的输入输出了。 VGG16&emsp;&emsp;VGGNet[4]是由牛津大学计算机视觉组和Google DeepMind项目的研究员共同研发的卷积神经网络模型，包含VGG16和VGG19两种模型，其网络模型如图5所示，也可以点击此处链接查看网络模型。图5 VGG16网络模型&emsp;&emsp;从网络模型可以看出，VGG16相比AlexNet类的模型具有较深的深度，通过反复堆叠33的卷积层和22的池化层，VGG16构建了较深层次的网络结构，整个网络的卷积核使用了一致的33的尺寸，最大池化层尺寸也一致为22。与AlexNet主要有以下不同： Vgg16有16层网络，AlexNet只有8层； 在训练和测试时使用了多尺度做数据增强。 GoogLeNet&emsp;&emsp;GoogLeNet[5]（论文地址）进一步增加了网络模型的深度和宽度，但是单纯的在VGG16的基础上增加网络的宽度深度会带来以下缺陷： 过多的参数容易引起过拟合； 层数的加深，容易引起梯度消失现象。 &emsp;&emsp;GoogLeNet的提出受到论文Network in Network（NIN）的启发，NIN有两个贡献： 提出多层感知卷积层：使用卷积层后加上多层感知机，增强网络提取特征的能力。普通的卷积层和多层感知卷积层的结构图如图6所示，Mlpconv相当于在一般的卷积层后加了一个1*1的卷积层；图6 普通卷积层和多层感知卷积层结构图 提出了全局平均池化替代全连接层，从上文计算的LeNet5，AlexNet网络各层的参数数量发现，全连接层具有大量的参数。使用全局平均池化替代全连接层，能很大程度减少参数空间，便于加深网络，还能防止过拟合。 &emsp;&emsp;GoogLeNet根据Mlpconv的思想提出了Inception结构，该结构有两个版本，图7是Inception的naive版。该结构巧妙的将11、33和5*5三种卷积核和最大池化层结合起来作为一层结构。 图7 Inception结构的naive版&emsp;&emsp;然而Inception的naive版中55的卷积核会带来很大的计算量，因此采用了与NIN类似的结构，在原始的卷积层之后加上了11卷积层，最终版本的Inception如图8所示。 图8 降维后的Inception模块&emsp;&emsp;GoogLeNet的模型结构如图9所示，详细参数如表4所示。 图9 GoogLeNet模型结构 表4 GoogLeNet具体参数 ResNet&emsp;&emsp;卷积神经网络模型的发展历程一次次证明加深网络的深度和宽度能得到更好的效果，但是后来的研究发现，网络层次较深的网络模型的效果反而会不如较浅层的网络，称为“退化”现象，如图10所示。 图10 退化现象&emsp;&emsp;退化现象产生的原因在于当模型的结构变得复杂时，随机梯度下降的优化变得更加困难，导致网络模型的效果反而不如浅层网络。针对这个问题，MSRA何凯明团队提出了Residual Networks6)。该网络具有Residual结构如图11所示。 图11 Residual 结构&emsp;&emsp;ResNet的基本思想是引入了能够跳过一层或多层的“shortcut connection”，即增加一个identity mapping（恒等映射），将原始所需要学的函数H(x)转换成F(x)+x，而作者认为这两种表达的效果相同，但是优化的难度却并不相同，作者假设F(x)的优化 会比H(x)简单的多。这一想法也是源于图像处理中的残差向量编码，通过一个reformulation，将一个问题分解成多个尺度直接的残差问题，能够很好的起到优化训练的效果。&emsp;&emsp;这个Residual block通过shortcut connection实现，通过shortcut将这个block的输入和输出进行一个element-wise的加叠，这个简单的加法并不会给网络增加额外的参数和计算量，同时却可以大大增加模型的训练速度、提高训练效果，并且当模型的层数加深时，这个简单的结构能够很好的解决退化问题。&emsp;&emsp;首先构建了一个18层和一个34层的plain网络，即将所有层进行简单的铺叠，然后构建了一个18层和一个34层的residual网络，仅仅是在plain上插入了shortcut，而且这两个网络的参数量、计算量相同，并且和之前有很好效果的VGG-19相比，计算量要小很多。（36亿FLOPs VS 196亿FLOPs，FLOPs即每秒浮点运算次数。）这也是作者反复强调的地方，也是这个模型最大的优势所在。&emsp;&emsp;模型构建好后进行实验，在plain上观测到明显的退化现象，而且ResNet上不仅没有退化，34层网络的效果反而比18层的更好，而且不仅如此，ResNet的收敛速度比plain的要快得多。对于shortcut的方式，作者提出了三个策略： 使用恒等映射，如果residual block的输入输出维度不一致，对增加的维度用0来填充； 在block输入输出维度一致时使用恒等映射，不一致时使用线性投影以保证维度一致； 对于所有的block均使用线性投影。ResNet论文的最后探讨了阻碍网络更深的瓶颈问题，如图12所示，论文中用三个1x1,3x3,1x1的卷积层代替前面说的两个3x3卷积层，第一个1x1用来降低维度，第三个1x1用来增加维度，这样可以保证中间的3x3卷积层拥有比较小的输入输出维度。图12 更深的residual block参考文献[1] Lecun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.[2] Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems. Curran Associates Inc. 2012:1097-1105.[3] Zeiler M D, Fergus R. Visualizing and Understanding Convolutional Networks[J]. 2013, 8689:818-833.[4] Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.[5] Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2015:1-9.[6] He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]// Computer Vision and Pattern Recognition. IEEE, 2016:770-778.]]></content>
      <categories>
        <category>深度学习笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
        <tag>网络模型</tag>
        <tag>卷积</tag>
        <tag>池化</tag>
        <tag>LeNet5</tag>
        <tag>AlexNet</tag>
        <tag>ZFNet</tag>
        <tag>VGG16</tag>
        <tag>GoogLeNet</tag>
        <tag>ResNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络的结构与相关算法]]></title>
    <url>%2F2018%2F04%2F25%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[引言&emsp;&emsp;早在上世纪50年代，美国神经生物学家David Hubel通过研究猫和猴子的瞳孔区域与大脑皮层神经元的对应关系就发现视觉系统的信息处理方式是分级的。这一发现，促成了神经网络在图像处理上的发展。&emsp;&emsp;神经网络的发展史可以分为三个阶段，第一个阶段是Frank Rosenblatt提出的感知机模型[1]，感知机模型的逻辑简单有效，但不能处理异或等非线性问题。第二个阶段是Rumelhart等提出的反向传播算法[2]，该算法使用梯度更新权值，使多层神经网络的训练成为可能。第三个阶段得益于计算机硬件的发展和大数据时代的到来，促进了深度神经网络的发展。在上一篇博客什么是深度学习中介绍了深度学习的三种网络模型，其中卷积神经网络在图像处理上有着诸多突破性的进展。由于我对卷积神经网络较为熟悉，下面将根据神经网络的三个发展阶段阶段分析讨论卷积神经网络的发展史。 传统神经网络感知机与多层网络&emsp;&emsp;感知机（Perceptron）由两层神经元组成，是一种二分类的线性分类模型，也是最简单的单层前馈神经网络（Feedforward Neural Network）模型。感知机的提出受到生物神经元的启发，神经元在处理突触传递而来的电信号后，若产生的刺激大于一定的阈值，则神经元被激活，感知机也具有类似的结构。假设输入空间是χ⊆R^n，输出空间是y={+1,-1}，x和y分别属于两个空间，则由感知机表示的由输入空间到输出空间的函数为：f(x)=sign(w∙x+b)&emsp;&emsp;其中w和b是感知机模型的参数，w∈R^n称为权重(Weight)，b∈R^n称为偏置(Bias)，w∙x表示两个向量间的内积。&emsp;&emsp;Minsky和Papert已证明若决策区域类型是线性可分的，则感知机一定会学习到收敛的参数权重w和偏置b，否则感知机会发生震荡[3]（fluctuation）。因此，感知机在线性可分数据中表现良好，如果设定足够的迭代次数，能很好的处理近似线性可分的数据。但如果对非线性可分的数据，如异或问题，单层感知机不能有效的解决。由于不能用一条直线划分样本空间，有学者想到用多条直线来划分样本，多层感知机就是这样一个模型。多层感知机结构如图1所示，相比较于单层感知机，多层感知机增加了隐藏层的层数。 图1 多层感知机结构图&emsp;&emsp;随着隐藏层数的增加，感知机的分类能力如表1所示。 表1 感知机分类能力比较 &emsp;&emsp;由表可知，在异或问题中，无隐层的感知机不能解决异或问题，引入了隐层后，异或问题得到解决，而随着层数越多，对于异或问题的拟合会越来越好。这说明，在感知机中随着隐藏层层数的增多，决策区域可以拟合任意的区域，因此理论上多层感知机可以解决任何线性或非线性的分类问题。但是，Minsky和Papert提出隐藏层的权重和偏置参数无法训练，这是由于隐藏层不存在期望的输出，无法通过单层感知机的训练方式训练多层感知机[4]。 反向传播算法&emsp;&emsp;如何训练多层感知机的难点在很长一段时间没有得到解决，要训练多层网络，需要更有效的学习算法。反向传播（BackPropagation，BP）算法[1]是训练多层网络的常用方法，该方法用链式法则对网络中所有权重和偏置计算损失函数的梯度，将梯度反馈给随机梯度下降或其它最优化算法，用来更新权值以最小化损失函数。在网络中，正向传播和反向传播的过程如图2所示。在这个例子中输入图片经过网络正向传播后得到的分类是狗，与实际类别的人脸不符，此时会将误差逐层反向传播，修正各个层的权重和偏置参数后，再进行正向传播，反复迭代，直至网络的参数能正确的分类输入的图片。 图2 BP网络训练过程&emsp;&emsp;反向传播算法的主要步骤如下： 随机初始化多层网络的权重和偏置参数，将训练数据送入多层网络的输入层，经过隐藏层和输出层，得到输出结果。完成网络的前向传播过程； 计算输出层实际值和输出值间的偏差，根据反向传播算法中的链式法则，得到每个隐藏层的误差，根据每层的误差调整各层的参数。完成网络的反向传播过程； 不断迭代前两步中的正向传播和反向传播过程，直至网络收敛。 &emsp;&emsp;由于还不熟悉markdown的公式编辑，这里省去反向传播的推导过程，感兴趣的朋友可以阅读周志华教授的《机器学习》第五章神经网络里面有详尽的推导过程。 卷积神经网络的基本思想&emsp;&emsp;在BP神经网络中，每一层都是全连接的，参数数量随着网络宽度和深度增加会指数级增长。多层网络结合BP算法对输入数据虽然有强大的表示能力，但巨大的参数一方面限制了每层能够容纳的最大神经元数量，另一方面也限制了神经网络的深度。受到动物视觉皮层中感受野的启发，效仿这种结构的卷积神经网络具有局部连接和参数共享的特点，可以有效的减少网络的相关参数数量，优化网络的训练速度。 局部连接&emsp;&emsp;Hubel和Wiesel在二十世纪五十年代和六十年代的研究表明，猫和猴子的视觉皮层中的神经元只响应特定的某些区域的刺激。将这种视觉刺激影响单个神经元反应的区域称为感受野（receptive field），相邻神经元细胞具有相同或相似的感受野[5]。正是由于发现了感受野等功能在猫的视觉神经中枢中的作用，催生了日本学者福岛邦彦提出带卷积和下采样层的多层卷积神经网络[6-8]。&emsp;&emsp;当我们在处理一副图像时，其输入往往是高维的。传统的神经网络将下一层神经元连接到上一层所有神经元。这种方式随着网络层数的增加，参数数量会爆炸式增加，在实际运用中，会无法训练网络。卷积神经网络中采取的做法是将每个神经元连接到上一层的部分神经元。这种连接的空间范围是一个超参数，称为神经元的感受野，感受野实际上是神经元映射到输入图像矩阵空间的大小。&emsp;&emsp;局部连接的实现方式是引入卷积层，通过卷积层对应局部的图像，每一层的神经元组合在一起对应图像的全局信息。如图3所示，在网络的第m层，每个神经元感受野大小为3，能连接到上一层的3个神经元。m+1层与m层类似。随着层数增加，神经元相对于输入层的感受野会越来越大。每个神经元不会响应感受野以外神经元的变化。受启发于动物的视觉神经元只响应局部信息，这样的结构确保了卷积神经网络只响应上一层局部神经元的变化，起到过滤作用的同时，减少了网络参数。而且随着层数的增加，这种过滤作用会越来越全局。 图3 局部连接 权值共享&emsp;&emsp;卷积的优点除了局部连接外还有权值共享。如图4所示，假设第m-1层有5个神经元，m层有3个神经元，对第m-1层的特征进行卷积，得到第m层共有3个单元的输出特征图。虽然第m层每个神经元都与第m-1层中的3个神经元连接，但同一组卷积操作的权重参数相同。在这个例子中，通过权值共享，将9个参数较少到了3个。 图4 权值共享&emsp;&emsp;卷积神经网络中权值共享的实现方式是让同一个卷积核去卷积整张图像，生成一整张特征图[9]。在卷积操作中，同一个卷积核内，所有神经元共享相同权值，权值共享的策略可以很大程度上降低网络需要计算的参数数量。通过权值共享，不仅大大增加了参数的训练效率，而且提取的特征在一定程度上具有位置不变性，加强了特征对输入图像的表达能力。 卷积神经网络结构&emsp;&emsp;卷积神经网络是一种层次模型（Hierarchical Model），其输入是RGB图像，视频，音频等数据。卷积神经网络通过一系列卷积（Convolution）操作，非线性激活函数（Non-linear Activation Function），池化（Pooling）操作层层堆叠，逐层从原始数据获取高层语义信息[10]。如图5所示，在结构上，卷积神经网络分类器有四种类型的网络层：卷积层、池化层、全连接层和分类器。各层次之间的有如下约束：（1）多个卷积（C）和池化（S）层，将上一层的输出图像与本层权重W做卷积得到各个C层，然后经过下采样得到S层。（2）全连接层：全连接层的输入是最后一个卷积池化层的输出，其输出是一个N维的列向量，维度对应类别的个数。（3）分类器：p_1，p_2，p_n的具体数值代表输入图像属于各类别的概率，分类器根据提取到的特征向量将检测目标划分到合适的类中。 图5 卷积神经网络分类器 卷积&emsp;&emsp;图片有着固有的特性，这意味着，图像的一部分特征与其他部分相似，对一张图片学习到的一部分特征可以用于其他部分。卷积操作受启发于这种特性，具体操作如图6所示，输入图片大小为5×5，经过卷积核大小为3×3的卷积后，原来的输入空间映射到3×3的区域。再经过一次相同大小的卷积核后，图片大小变为了1×1。可见，卷积层逐层提取特征的方式，能从庞大的像素矩阵中，提取到对图像更有代表性的特征。卷积层最重要的是卷积核的设计，卷积核有几个参数:大小、步长、数目、边界填充。这些参数会对卷积的效果带来很大的影响。若卷积核设计的较大，如AlexNet[11]中使用的11×11和5×5的卷积核，其感受野很大，能覆盖图像更大的区域，对图像的“抽象”能力会较好，但较大的卷积核也会带来参数过多的负面影响。卷积核的步长指卷积每次滑动的距离，在一定程度上影响了特征提取的好坏。每一层网络的多个卷积核保证了提取到的特征是图像的多个方面，但卷积核的数量也不是越多越好，过多的卷积核会增加参数数量，计算复杂的同时容易过拟合。边界填充可用于卷积核与图像尺寸不匹配时，填充图像缺失区域。 图6 卷积示意图 池化&emsp;&emsp;卷积后的特征依然十分巨大，不仅带来计算性能的下降，也会产生过拟合。于是产生了对一块区域特征进行聚合统计的想法.例如，可以计算图像在某一块区域内的最大值或平均值代替这一块区域的特征，在降低特征维度的同时能使提取到的特征更具有代表性，还会使得处理过后的特征图谱拥有更大的感受野，这种用部分特征代替整体特征的操作称为池化[10]（Pooling）。常用的池化方法如下：最大值池化（Max Pooling）；均值池化（Mean Pooling）；随机池化（Random Pooling）。池化操作具有以下优良特性：（1）平移不变性（Translation Invariant）。无论是哪种池化方式，提取的都是局部特征。池化操作会模糊特征的具体位置，图像发生了平移后，依然能产生相同的特征。（2）特征降维（Feature Dimension Reduction），池化操作将一个局部区域的特征进一步抽象，池化中的一个元素对应输入数据中的一个区域，可以减少参数数量，降低维度。&emsp;&emsp;池化操作的功能是减小特征空间的大小，以减小网络中的参数和计算量，从而避免过度拟合。如图7所示，224×224×64经过大小为2×2，步长为2的池化核，变成了112×112×64，使得特征图谱减少为原来的1/2。图7中池化方式是最大池化，即将一个区域内的最大值表示为这个区域的池化结果。 图7 池化示意图 全连接层&emsp;&emsp;前面讨论的卷积层，池化层等操作是将原始数据映射到特征空间，使得到的特征矩阵越来越抽象并对特征有良好的表达能力。Softmax分类器要求输入是列向量，需要全连接层将卷积和池化的输出映射到线性可分空间。全连接层可以聚合卷积和池化操作得到的高阶特征，并且可以简化参数模型，一定程度的减少神经元数量和训练参数。为了能用反向传播算法训练网络，全连接层要求图片有固定的输入尺寸。因此早期网络中，需要对不同尺寸的图片进行裁剪或拉伸，这种操作会带来图片信息的失真和损失。在第三章讨论的感兴趣（Region of Interest）池化方法，可以很好的解决这一问题。&emsp;&emsp;卷积层是由全连接层发展而来，全连接层可以用特殊的卷积层表示，对于前一层全连接的全连接层可以用卷积核大小为1×1的卷积层替代，而对于前一层是卷积的全连接层可以用对上一层所有输入全局卷积的卷积层替代。在全连接层中可以认为每个神经元的感受野是整个图像。全连接层隐藏层节点数越多，模型拟合能力越强，但参数冗余会带来过拟合的风险而且会降低效率。对于这个问题，一般的做法是采用正则化（Regularization）技术，如L1、L2范式。还有通过Dropout随机舍弃一些神经元，来减少权重连接，然后增强网络模型在缺失个体连接信息情况下的鲁棒性[10]。 分类器&emsp;&emsp;经过全连接层将特征映射到线性空间后，最后还需要将实例数据划分到合适的分类中。分类器有多种，常用的有支持向量机和Softmax回归，此处以Softmax为例子。Softmax函数用于将多个神经元的输出映射到(0,1)之间，转化为概率问题，从而处理多分类问题。如图8所示，Softmax层的输入分别是3、1和-3，在经过Softmax层后分别映射为0.85、0.12和0.03，三个值的累加和为1，其数值可以理解为概率，则属于y_1类的概率最大为0.85。这幅图是Softmax的通俗理解，具体推导过程可以参考这篇文章。 图8 Softmax层 参考文献[1] Rumelhart D E, Hinton G E, Williams R J. Learning representations by back-propagating errors [J]. Nature, 1986, 323(6088): 533-536.[2] Hinton G E, Osindero S, Teh Y W. A fast learning algorithm for deep belief nets [J]. Neural Computation, 2006, 18(7): 1527-1554.[3] Minsky M, Papert S. Perceptrons: An introduction to computational geometry [J]. 1969, 75(3): 3356-3362.[4] Minsky M L, Papert S A. Perceptrons (expanded edition) mit press [J]. 1988.[5] 刘建立, 沈菁, 王蕾, 等. 织物纹理的简单视神经细胞感受野的选择特性 [J]. 计算机工程与应用, 2014, 50(1): 185-190.[6] Fukushima K. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position [J]. Biological Cybernetics, 1980, 36(4): 193-202.[7] Fukushima K. Neocognitron: A hierarchical neural network capable of visual pattern recognition [J]. Neural Networks, 1988, 1(2): 119-130.[8] Fukushima K, Miyake S. Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position [J]. Pattern Recognition, 1982, 15(6): 455-469.[9] 曹婷. 一种基于改进卷积神经网络的目标识别方法研究 [D]. 湖南大学, 2016.[10] LeCun Y, Boser B, Denker J S, et al. Backpropagation applied to handwritten zip code recognition [J]. Neural computation, 1989, 1(4): 541-551.[11] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks [C]. Proceedings of 26th Annual Conference on Neural Information Processing Systems, Nevada:NIPS, 2012: 1097-1105.]]></content>
      <categories>
        <category>深度学习笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
        <tag>卷积</tag>
        <tag>池化</tag>
        <tag>BP算法</tag>
        <tag>感知机</tag>
        <tag>全连接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是深度学习]]></title>
    <url>%2F2018%2F04%2F24%2F%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[机器学习&emsp;&emsp;比起深度学习，“机器学习”一词更耳熟能详。机器学习是人工智能的一个分支，它致力于研究如何通过计算的手段，利用经验来改善计算机系统自身的性能。通过从经验中获取知识，机器学习算法摒弃了人为向机器输入知识的操作，转而凭借算法自身来学到所需所有知识。对于传统机器学习算法而言，“经验”往往对应以“特征”形式存储的“数据”，传统机器学习算法所做的事情便是依靠这些数据产生“模型”。&emsp;&emsp;特征的意义是找一个更好的空间去重构表达数据，把原始数据映射到高维空间，更便于划分不同类的数据。特征的选取是机器学习的核心，通常线性可分的数据用最简单的感知机即可划分，而现实应用中的数据往往是高维复杂的，传统的特征提取的方式可以归纳为以下几种： 依据经验人工挑选：如关于天气的数据集，如果是预测是否下雨，可以挑选与降雨密切相关的特征：季节、紫外线指数、温度、湿度、是否有云、风向和风速等属性。 线性特征选择：假设特征之间相互独立，不存在交互，那么可以使用卡方检验、信息增益、互信息等方法逐个检验特征与结果之间的相关程度。更为简便的方法是使用LR等线性模型，先做一次预训练，根据特征对应的线性模型权值的绝对值大小来对特征的重要程度进行排序。 非线性特征选择：如果属性之间不是相互独立，可以使用随机森林来进行特征选择，概括来说就是将想要检验重要性的特征在样本上进行permutation，然后观察OOB错误的上升程度，上升越大，说明这个特征越重要。 &emsp;&emsp;以上介绍的都是传统的特征提取方式，而随着机器学习任务的复杂多变，现有的特征提取方法表现出了诸多弊端，针对一个数据集设计特征提取方法不仅费时费力，而且还十分敏感，换成其他的任务，表现往往不尽人意。得益于计算机硬件的发展和大数据时代的到来，计算机拥有了能处理大量数据的前提和能力，促进了深度学习的发展。 深度学习的实质&emsp;&emsp;深度学习以原始数据作为输入，经过算法层层的将数据抽象为自身任务所需要的最终特征表示。通过大量的数据逐层学习特征，免去了传统特征提取过程中人类先验知识的影响。通过数据自主的学习特征，以获取输入信息更本质的特征[1, 2]。&emsp;&emsp;深度学习的实质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。因此，“深度模型”是手段，“特征学习”是目的。区别于传统的浅层学习，深度学习的不同在于： 强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点； 明确突出了特征学习的重要性，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。 与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。 深度学习的网络模型&emsp;&emsp;相比较于传统的机器学习算法，深度学习除了模型学习，还有特征学习、特征抽象等任务模块的参与，借助多层任务模块完成最终学习任务，故称为“深度”学习。深度学习发展到如今已经有了多种结构的深度神经网络模型，如： 由多个受限玻尔兹曼机组成的深度信念网络（Deep Belief Network，DBN）[3]; 应用于自然语言处理的循环神经网络（Recurrent Neural Network，RNN）[4]; 具有局部连接和权值共享等优点的卷积神经网络（Convolutional Neural Network，CNN）[5]。参考文献[1] Bottou L, Chapelle O, Decoste D, et al. Scaling learning algorithms towards AI[J]. Large-scale kernel machines,2007,34(5): 321-359.[2] Bengio Y, Delalleau O. On the expressive power of deep architectures [C]. Proceedings of International Conference on Algorithmic Learning Theory, Springer-Verlag, 2011: 18-36.[3] Mikolov T, Karafiát M, Burget L, et al. Recurrent neural network based language model [C]. Proceedings of 11th Annual Conference of the International Speech Communication Association, Chiba: Interspeech, 2010: 1045-1048.[4] LeCun Y, Boser B, Denker J S, et al. Backpropagation applied to handwritten zip code recognition [J]. Neural computation, 1989, 1(4): 541-551.]]></content>
      <categories>
        <category>深度学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>特征提取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Github和Hexo的个人博客搭建]]></title>
    <url>%2F2018%2F04%2F23%2F%E5%9F%BA%E4%BA%8EGithub%E5%92%8CHexo%E7%9A%84%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[写在前面&emsp;&emsp;去年起看了很多大牛的博客，也萌生了搭建个人博客的想法，为什么搭建博客，总结下来有以下好处： 书写是为了更好的思考 激励自己持续学习 尝试持之以恒的去做一些事情 &emsp;&emsp;这次趁着大论文盲审结果还没出来和导师还没反馈小论文修改意见的间隙，花了一个周末的时间搭建好了自己的个人博客。在此将搭建过程作为自己的第一篇博客记录下来。 Hexo简介&emsp;&emsp;Hexo 是一个基于 Node.js 的静态博客程序，可以方便的生成静态网页托管在github和Heroku上。Hexo有着丰富的主题，可以定制多种样式。hexo特性： 速度快：Hexo基于Node.js，支持多进程，几百篇文章也可以秒生成； 撰写工具丰富：支持GitHub Flavored Markdown和所有Octopress的插件； 扩展性强： Hexo支持EJS、Swig和Stylus。通过插件支持Haml、Jade和Less。 &emsp;&emsp;使用hexo时，有以下常用命令：12345678910npm install hexo -g #安装npm install hexo -g #安装 npm update hexo -g #升级 hexo init #初始化hexo n &quot;我的博客&quot; == hexo new &quot;我的博客&quot; #新建文章hexo p == hexo publishhexo g == hexo generate#生成hexo s == hexo server #启动服务预览hexo d == hexo deploy#部署hexo new 创建文章 环境搭建(node,hexo,git)&emsp;&emsp;前面说过hexo是基于node.js，因此需要先安装node.js。git的配置这里就不再赘述，可以参考廖雪峰老师的Git教程。安装好Git后，需要将其与自己的GitHub账号关联上。安装好node.js后，仅需一步即可安装hexo的相关套件。在命令行输入: npm install hexo -g hexo-cli &emsp;&emsp;到这一步就安装好了所需的所用环境。 设置&emsp;&emsp;在搭建自己的博客前，需要设置一个博客的根目录。使用命令行切换到该根目录下，输入： hexo init blog &emsp;&emsp;等待片刻，成功后会提示INFO Start blogging with Hexo!初始化成功后，目录如下： . ├── _config.yml ├── package.json ├── scaffolds ├── source | ├── _drafts | └── _posts └── themes &emsp;&emsp;source的_posts目录下会自带一篇题为“Hello World”的示例文章，直接执行以下操作可以看到网站初步的模样: $ hexo generate # 启动本地服务器 $ hexo server # 在浏览器输入 http://localhost:4000/就可以看见网页和模板了,若端口号被占用，可输入hexo server -p 4001改为其他端口号。 &emsp;&emsp;访问http://localhost:4000/，界面如下： 部署及配置博客配置SSH&emsp;&emsp;在上一步看到了网站的默认效果，此时需要将该博客部署到Github上，登陆Github，创建名为your_name.github.io(your_name替换成你的用户名)的仓库。重新打开CMD,输入： ssh-keygen -t rsa -C &quot;Github的注册邮箱地址&quot; &emsp;&emsp;一路Enter，得到信息：Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub.根据保存的路径找到id_rsa.pub文件，用编辑器打开，复制所有内容，然后Sign in to GitHub，按以下步骤配置SSH：&emsp;&emsp;New SSH key ——&gt;Title：blog ——&gt; Key：输入刚才复制的—— &gt;Add SSH key 配置博客&emsp;&emsp;在blog目录下，用编辑器打开_config.yml，修改其中的配置信息。&emsp;&emsp;修改网站中的相关信息 ：1234567title: #标题subtitle: #副标题description: #站点描述author: #作者language: zh-Hansemail: #电子邮箱timezone: Asia/Shanghai &emsp;&emsp;配置部署仓库 1234deploy: type: git repo: 刚刚github创库地址.git branch: master &emsp;&emsp;特别提醒，在每个参数的：后都要加一个空格。以上操作完成后，执行： 1234hexo clean #清除缓存 网页正常情况下可以忽略此条命令hexo generate #生成hexo server #启动服务预览，非必要，可本地浏览网页hexo deploy #部署发布 &emsp;&emsp;得到提示信息INFO Deploy done: git表示成功发布到Github上。然后在浏览器里输入your_name.github.io就可以访问刚刚配置好的博客了。 后记&emsp;&emsp;到此为止，最基本的hexo+github搭建博客完结。hexo有许多优美简洁的主题，网上也有许多关于主题美化的教程，可以根据自己的喜好添加各种或实用或酷炫的功能。]]></content>
      <categories>
        <category>Hexo博客搭建教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
