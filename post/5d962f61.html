<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script></script><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>!function(e,t,o,c,i,a,n){e.DaoVoiceObject=i,e[i]=e[i]||function(){(e[i].q=e[i].q||[]).push(arguments)},e[i].l=1*new Date,a=t.createElement(o),n=t.getElementsByTagName(o)[0],a.async=1,a.src=c,a.charset="utf-8",n.parentNode.insertBefore(a,n)}(window,document,"script",("https:"==document.location.protocol?"https:":"http:")+"//widget.daovoice.io/widget/0f81ff2f.js","daovoice"),daovoice("init",{app_id:"116d2e76"}),daovoice("update")</script><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="http://p7jiixmp8.bkt.clouddn.com/shanghai.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="http://p7jiixmp8.bkt.clouddn.com/32_32_shanghai.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="http://p7jiixmp8.bkt.clouddn.com/16_16_shanghai.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="CVPR,深度学习,FER,"><link rel="alternate" href="/atom.xml" title="冲弱's Blog" type="application/atom+xml"><meta name="keywords" content="CVPR,深度学习,FER"><meta property="og:type" content="article"><meta property="og:title" content="《Deep Facial Expression Recognition:A Survey》论文笔记"><meta property="og:url" content="https://oysz2016.github.io/post/5d962f61.html"><meta property="og:site_name" content="冲弱&#39;s Blog"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://p7jiixmp8.bkt.clouddn.com/Blog/1527822068735.jpg"><meta property="og:updated_time" content="2018-06-03T15:58:25.725Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="《Deep Facial Expression Recognition:A Survey》论文笔记"><meta name="twitter:image" content="http://p7jiixmp8.bkt.clouddn.com/Blog/1527822068735.jpg"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!0},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://oysz2016.github.io/post/5d962f61.html"><title>《Deep Facial Expression Recognition:A Survey》论文笔记 | 冲弱's Blog</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">冲弱's Blog</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">人生在勤，不索何获</p></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> 首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> 归档</a></li><li class="menu-item menu-item-comment"><a href="/comment/" rel="section"><i class="menu-item-icon fa fa-fw fa-comment"></i><br> 留言</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="st-search-show-outputs"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><form class="site-search-form"> <input type="text" id="st-search-input" class="st-search-input st-default-search-input"></form><script type="text/javascript">!function(e,t,n,s,c,i,o){e.SwiftypeObject=c,e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},i=t.createElement(n),o=t.getElementsByTagName(n)[0],i.async=1,i.src="//s.swiftypecdn.com/install/v2/st.js",o.parentNode.insertBefore(i,o)}(window,document,"script",0,"_st"),_st("install","_fC1jT9UoSk9vQ72-ezz","2.0.0")</script></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://oysz2016.github.io/post/5d962f61.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="冲弱"><meta itemprop="description" content=""><meta itemprop="image" content="http://p7jiixmp8.bkt.clouddn.com/%E6%B4%BE%E5%A4%A7%E6%98%9F.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="冲弱's Blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">《Deep Facial Expression Recognition:A Survey》论文笔记</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-01T17:22:25+08:00">2018-06-01</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/论文笔记/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a></span></span> <span id="/post/5d962f61.html" class="leancloud_visitors" data-flag-title="《Deep Facial Expression Recognition:A Survey》论文笔记"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数&#58;</span><span class="leancloud-visitors-count"></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">4,542 字</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">17 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><div class="post-gallery" itemscope itemtype="http://schema.org/ImageGallery"><div class="post-gallery-row"> <a class="post-gallery-img fancybox" href="http://p7jiixmp8.bkt.clouddn.com/Blog/1527822068735.jpg" rel="gallery_cji1enz0r0008xotpwpd7zwqi" itemscope itemtype="http://schema.org/ImageObject" itemprop="url"><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1527822068735.jpg" itemprop="contentUrl"></a></div></div><script src="\assets\js\APlayer.min.js"></script><p class="description"></p><a id="more"></a><p>&emsp;&emsp;论文链接：<a href="https://arxiv.org/abs/1804.08348" target="_blank" rel="noopener">https://arxiv.org/abs/1804.08348</a><br>&emsp;&emsp;这篇文章<sup><a href="#fn_1" id="reffn_1">1</a></sup>是北邮的邓伟洪教授关于<strong>深度人脸表情识别(Deep Facial Expression Recognition,DFER)</strong> (<strong>情感识别</strong>)的一篇综述性文章，该文章被<strong>计算机视觉顶会CVPR</strong>收录。对于像我这样对情感识别感兴趣，但又没做过具体应用的小白来说研读这篇文章再合适不过了。</p><hr><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>&emsp;&emsp;表情可以说是一门世界语，不分国界、种族以及性别，可以说所有人都有着通用的表情。FEP在机器人、医疗、驾驶员驾驶疲劳检测和人机交互系统中都有广泛应用，最早在20世纪，Ekman和Friesen通过跨文化研究，定义了6种<strong>基础表情</strong>：生气、害怕、厌恶、开心、悲伤和吃惊，随后又加入了“蔑视” 这一表情。开创性的工作和直观的定义，使该模型在自动人脸表情识别(automatic facial expression analysis, AFEA)中依然很流行。<br>&emsp;&emsp;根据特征表示，FER系统可以划分为图片FER和视频FER两类。图片FER只提取当前图片的特征，而视频需要考虑相邻帧之间的关系。实际上所有计算机视觉的任务的处理对象都可以划分为图片和视频两类。<br>&emsp;&emsp;FER传统的方式使用手工提取的特征和浅层学习，这种方式的弊端就不多赘述了。得益于深度学习的发展和更具有挑战性的数据集FER2013的出现，越来越多的研究者将深度学习技术运用到FER中。</p><h2 id="深度人脸表情识别"><a href="#深度人脸表情识别" class="headerlink" title="深度人脸表情识别"></a>深度人脸表情识别</h2><p>&emsp;&emsp;这一节讨论了深度学习在人脸表情识别应用上的三个步骤。分别是预处理、特征提取和特征分类，简述了每一步的具体做法，并引用了相关论文。</p><h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><h4 id="人脸对齐"><a href="#人脸对齐" class="headerlink" title="人脸对齐"></a>人脸对齐</h4><p>&emsp;&emsp;给定一个数据集，第一步是移除与人脸不相关的背景和非人脸区域。ViolaJones(V&amp;J)人脸检测器<sup><a href="#fn_2" id="reffn_2">2</a></sup> (在OpenCV和Matlab中都有实现)，该检测器能将原始图片裁剪以获得人脸区域，<br>&emsp;&emsp;第二步是面对齐，这一步至关重要，因为可以减少人脸尺度改变和旋转产生的影响。最常用的面部对齐的实现是IntraFace<sup><a href="#fn_3" id="reffn_3">3</a></sup>,IntraFace采用SDM算法，定位出49个面部特征点（双眼、两个眉毛、鼻子和嘴巴）</p><h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><p>&emsp;&emsp;数据增强包括在线和离线两种方式：</p><ul><li>离线方式：随机扰动，图像变换（旋转、评议、翻转、缩放和对齐），添加噪声（椒盐噪声和斑点噪声），以及调整亮度和饱和度，以及在眼睛之间添加2维高斯分布的噪声。此外，还有用对抗神经网络GAN<sup><a href="#fn_4" id="reffn_4">4</a></sup>生成脸，3DCNN辅助AUs生成表情。使用GAN生成脸对网络性能是否有提升还没有验证过。</li><li>在线方式：包含在训练时，裁剪图片，水平翻转。主要是通过随机扰动训练模型。<h4 id="人脸归一化"><a href="#人脸归一化" class="headerlink" title="人脸归一化"></a>人脸归一化</h4>&emsp;&emsp;人脸的光照和头部姿势变化会削弱训练模型的性能，有两种脸部归一化的策略削弱影响，分别是亮度归一化和姿态归一化。</li><li>亮度归一化:Inface 工具箱<sup><a href="#fn_5" id="reffn_5">5</a></sup>是最常用的光照不变人脸检测箱。除了直观的调整亮度以外，还有对比度调整。常见的对比度调整方法有直方图归一化、DCT归一化、Dog归一化。</li><li>姿态归一化：这是一个棘手的问题，目前的方法都不太理想。有2D的landmark对齐,3Dlandmark对齐，有通过图像和相机参数估计，也有通过深度传感器测量然后计算出来。比较新的模型都是基于GAN的，有FF-GAN、TP-GAN和DR-GAN。</li></ul><h3 id="深度特征学习"><a href="#深度特征学习" class="headerlink" title="深度特征学习"></a>深度特征学习</h3><p>&emsp;&emsp;这一部分主要讲的是使用深度学习模型提取特征，包括卷积神经网络(Convolutional neural network，CNN)、深度置信网络（Deep belief network ，DBN）、深度自动编码器(Deep autoencoder，DAN)和递归神经网络(Recurrent neural network，RNN)。深度人脸表情识别的流程如下，下图可以看出，深度网络模型部分有四种常用的模型。作者只是简单的介绍了几种网络模型，在这里我也不过多的赘述。CNN模型在我前几篇博文<a href="https://oysz2016.github.io/post/4ec18e58.html">卷积神经网络的结构与相关算法</a>和<a href="https://oysz2016.github.io/post/d86a012b.html">卷积神经网络模型解读汇总——LeNet5，AlexNet、ZFNet、VGG16、GoogLeNet和ResNet</a>有详细介绍。其余的网络模型以后有时间会逐一整理。</p><p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1527855909779.jpg" alt="enter description here"></p><h3 id="人脸表情分类"><a href="#人脸表情分类" class="headerlink" title="人脸表情分类"></a>人脸表情分类</h3><p>&emsp;&emsp;完成了特征提取后，最后一步对其进行分类。在传统FER系统中，特征提取和特征分类是独立的。而深度学习的FER是端到端的模型，可以在网络的末端添加损失层调节反向传播的误差，预测概率可以由网络直接输出。也可以将两者结合，即用深度学习提取特征，再用SVM等分类器分类。</p><h2 id="面部表情数据库"><a href="#面部表情数据库" class="headerlink" title="面部表情数据库"></a>面部表情数据库</h2><p>&emsp;&emsp;该部分总结了FER可用的公开数据集。</p><ul><li><strong>CK+</strong>:包括123个subjects, 593 个 image sequence。该数据库由118名受试者录制，在这593个image sequence中，有327个sequence 有 emotion的 label。除了中性外包含7种表情：愤怒、蔑视、厌恶、恐惧、高兴、悲伤和惊讶。</li><li><strong>MMI</strong>：包括32个subjects，326个image sequence。213个sequence 有 emotion的 label。包含6中表情（相比较于CK+没有蔑视），MMI更具挑战性，因为很多人都戴有配饰。</li><li><strong>JAFFE</strong>：包含213副（每幅图像分辨率为256*256）日本女性的脸部图像，包含7种表情。该数据库均为正面脸相，且对原始图像进行了调整和修剪，光照均为正面光源，但光照强度有差异。</li><li><strong>TFD</strong>：改数据库是几个面部表情数据集的集合，TFD包含112234张图片(每张图片被调整到48*48大小)，所有实验对象的眼睛都是相同的距离。其中4189张有标注，包含7种表情。</li><li><strong>FER2013</strong>：改数据库通过谷歌图片API自动收集，数据库中所有图片都修正了标签，将图片调整到48*48大小。包含28709张训练图像，3589张测试图像，包含7种表情。</li><li><strong>AFEW</strong>：AFEW数据集为Emotion Recognition In The Wild Challenge (EmotiW)系列情感识别挑战赛使用的数据集，该比赛从2013开始每年举办一次。 该数据集的内容是从电影中剪辑的包含表情的视频片段，包含7类表情。训练集、验证集和测试集分别包含773、383和653sample。</li><li><strong>SFEW</strong>：该数据集是从AFEW数据集中抽取的有表情的静态帧，包含7类表情。训练集、验证集和测试集分别包含958、436和372sample。</li><li><strong>Multi-PIE</strong>：包含4个场景9种光照条件15个视角下337个subject，总计有755370张图片。包含6种表情（没有蔑视）</li><li><strong>BU-3DFE</strong>：从100个人获取的606个面部表情sequence，包含6种表情（没有蔑视），多用于三维面部表情分析。</li><li><strong>Oulu-CASIA</strong>：80个没被标记的subject收集了2880个image sequence。包含6种表情（没有蔑视）。有红外（NIR）和可见光（VIS）两种摄像头在3种不同光照条件下拍摄。</li><li><strong>RaFD</strong>：包含67个subject的1608张图片，眼睛有不同的三种注视方向，包括前、左和右。包含7种表情。</li><li>KDEF:最初用于医学和心理学研究。数据集来自70个演员从5个角度的6种表情。</li><li><strong>EmotioNet</strong>：包含从网上收集到的接近100万张面部表情图片。</li><li><strong>RAF-DB</strong>：包含从网上收集的29672张面部图像，包含7中基本表情和11种复合表情。</li><li><strong>AffectNet</strong>：包含从网上收集的100多万张面部图像，其中45万张图片手工标注为7种表情。</li></ul><p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1527912456772.jpg" alt="enter description here"></p><h2 id="FER目前发展水平"><a href="#FER目前发展水平" class="headerlink" title="FER目前发展水平"></a>FER目前发展水平</h2><p>&emsp;&emsp;总结了基于静态图像和动态图像序列(视频)的FER进展。</p><h3 id="静态图像FER进展"><a href="#静态图像FER进展" class="headerlink" title="静态图像FER进展"></a>静态图像FER进展</h3><p>&emsp;&emsp;对于每一个数据集，下表显示了目前最优异的方法，在该数据集上取得的效果。</p><p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1527930265914.jpg" alt="enter description here"></p><h4 id="预训练和微调"><a href="#预训练和微调" class="headerlink" title="预训练和微调"></a>预训练和微调</h4><p>&emsp;&emsp;在相对较小的数据集上直接训练深度网络很容易导致过拟合。为了缓解这个问题，许多研究会在大数据集上先预训练网络，或者对已经训练好的网络进行微调。</p><p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1527930845031.jpg" alt="enter description here"></p><p>&emsp;&emsp;如上图所示，先在ImageNet数据集上训练，然后再在具体的人脸表情数据集上微调。微调有较好的效果，人脸表情识别有各种微调方式，比如分级、固定某些曾，不同网络层用不同数据集微调，具体可以看看原文中所引用的论文。<br>&emsp;&emsp;此外，文献<sup><a href="#fn_6" id="reffn_6">6</a></sup>指出FR和FER数据集存在巨大差异，人脸似乎别模型弱化了人脸表情的差异，提出了FaceNet2ExpNet网络消除这种影响。该模型分为两个阶段，首先用人脸识别模型提取特征，然后用表情识别网络消除人脸识别模型带来的情绪差异弱化。如下图所示。</p><p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1527932192340.jpg" alt="enter description here"></p><h4 id="多样化网络输入"><a href="#多样化网络输入" class="headerlink" title="多样化网络输入"></a>多样化网络输入</h4><p>&emsp;&emsp;传统的做法是使用原始的RGB图像作为网络的输入，然而原始数据缺乏重要的信息，如纹理信息，以及图像缩放、旋转、遮挡和光照等方面的不变性。因此可以借助一些手工设计的特征。如SIFT、LBP、MBP、AGEhe NCDV等。PCA可以裁剪出五官进行特征学习而不是整个脸部等。</p><h4 id="辅助块与层改进"><a href="#辅助块与层改进" class="headerlink" title="辅助块与层改进"></a>辅助块与层改进</h4><p>&emsp;&emsp;基于经典的CNN架构，有些研究设计了良好的辅助模块或者改进了网络层，这部分文中有列举几个例子，感兴趣可以找出相关论文翻看。<br>&emsp;&emsp;值得注意的是，Softmax在表情识别领域的表现不太理想。这是由于<strong>表情的类间区分度较低</strong>。作者整理了几种针对表情分类层的改进。</p><ul><li>受到center loss的启发，对特征与相应的类距离加了惩罚项，这分为两种<ul><li>一种是增加类间距离的island loss<sup><a href="#fn_7" id="reffn_7">7</a></sup>，如下图所示<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1528008215481.jpg" alt="enter description here"></li><li>另一种是减下类内距离的LP<sup><a href="#fn_8" id="reffn_8">8</a></sup> loss,使同一类的局部相邻特征结合在一起。</li></ul></li><li>基于triplet-loss，关于triplet-loss的想法可以参考原文和<a href="https://blog.csdn.net/tangwei2014/article/details/46788025" target="_blank" rel="noopener">这篇博文</a>。<ul><li>exponential triplet-based loss(增加困难样本的权重)</li><li>(N+M)-tupes cluster loss(降低anchor的选择难度，以及阈值化triplet不等式),如下图所示。<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1528008239408.jpg" alt="enter description here"><h4 id="网络集成"><a href="#网络集成" class="headerlink" title="网络集成"></a>网络集成</h4>&emsp;&emsp;之前的研究表明，多个网络的集合可以比单个网络表现的更好。在网络集成时，要考虑两点：</li></ul></li><li>网络模型要有充分的多样性，以确保网络之间具有互补性</li><li>要有可靠的集成算法</li></ul><p>&emsp;&emsp;关于第一点，网络的多样性产生有很多方法，不同的训练数据、不同的预处理方式、不同的网络模型、不同的参数都能产生不同的网络。<br>&emsp;&emsp;关于第二点集成算法。这其中也主要有两点，一个是特征集成，另一个是输出的决策集成。特征集成最常见的做法是将不同网络模型的特征直接链接，还有如下图的方式</p><p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1528008895990.jpg" alt="enter description here"></p><p>&emsp;&emsp;关于决策集成采用投票的机制，不同网络有不同的权重。关于决策集成的几种策略如下表所示。</p><p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1528009037931.jpg" alt="enter description here"></p><p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1528008969737.jpg" alt="enter description here"></p><h4 id="多任务网络"><a href="#多任务网络" class="headerlink" title="多任务网络"></a>多任务网络</h4><p>目前许多网络都是单一任务的输出，但在现实中，往往需要考虑其他多种因素的作用。多任务模型能从其他任务中学习到额外的信息有助于提高网络的泛化能力。关于多任务模型的好处，可以参考<a href="https://blog.csdn.net/laolu1573/article/details/78205180#3" target="_blank" rel="noopener">这篇博文</a>。如下如所示，在MSCNN<sup><a href="#fn_9" id="reffn_9">9</a></sup>模型中将脸部验证与表情识别两个任务集成在一个网络中。</p><p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1528009483598.jpg" alt="enter description here"></p><h4 id="网络级联"><a href="#网络级联" class="headerlink" title="网络级联"></a>网络级联</h4><p>&emsp;&emsp;在级联网络中，将不同模块处理不同的任务组合在一起设计一个更深层的网络，前一个模块的输出被后一个模块使用。如下图所示，在AUDN网络中，该网络由三部分组成。</p><p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1528009785945.jpg" alt="enter description here"></p><h3 id="动态图像序列FER进展"><a href="#动态图像序列FER进展" class="headerlink" title="动态图像序列FER进展"></a>动态图像序列FER进展</h3><p>&emsp;&emsp;基于动态的表情识别相比静态图片能更全面，这里指的动态图像序列，即在视频中。</p><h4 id="帧聚合"><a href="#帧聚合" class="headerlink" title="帧聚合"></a>帧聚合</h4><p>&emsp;&emsp;考虑到表情在不同时刻有不同的变化，但又不可能单独的统计每帧的结果作为输出，因此需要对一段帧序列给出一个识别结果，这就需要用到帧聚合。即用一个特征向量表示这一段时间序列。与集成算法类似，帧聚合有有两类，分别是决策级帧聚合和特征级帧聚合。这两部分感兴趣的可以参看论文。</p><h4 id="强度表达网络"><a href="#强度表达网络" class="headerlink" title="强度表达网络"></a>强度表达网络</h4><p>&emsp;&emsp;在视频中表情会有微妙的变化，而强度是指在视频中，所有帧表现某个表情的程度。一般在中间位置最能表达某个表情，即为强度峰值。大多数方法，都关注峰值附近而忽略了开始和结束时的低谷帧。这部分，主要介绍几个深度网络，输入是具有一定强度信息的样本序列，输出是某一个类表情中不同强度帧之间的相关性结果。如PPDN（peak-piloted），用以内在表情序列里帧之间相关性识别，还有基于PPDN的级联PPDN网络DCPN，具有更深更强的识别能力。虽然，这些网络，都考虑了一段序列里的表情变换，甚至为了计算表情的变化趋势，设计了不同的损失函数，但是，真心觉得，这种代价，对于工程来说，其实是没有意义的。有兴趣的，可以看看论文里对应的方法，这里不再赘述了。</p><h4 id="深度时空FER网络"><a href="#深度时空FER网络" class="headerlink" title="深度时空FER网络"></a>深度时空FER网络</h4><p>&emsp;&emsp;前面介绍的帧聚合和强度表达网络都属于传统的结构化流程，而在视频中将一些列帧作为单独的图像序列输入，输出某一类表情的分类结果。而RNN网络能利用”序列信息”，所以视频FER模型用RNN网络，还有C#D:</p><ul><li>RNN: 从理论上讲，它可以利用任意长序列的信息,RNN呢能对时间序列上的变化建模。</li><li>C3D: 在通常图像上的2D空间卷积的基础上，沿着时间轴加了一个时间维度，就形成了3D时空卷积。例如3DCNN-DAP<sup><a href="#fn_10" id="reffn_10">10</a></sup>，网络模型如下图所示。<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1528019915713.jpg" alt="enter description here"></li></ul><p>&emsp;&emsp;还有种“暴力”做法，不考虑时间维度，将帧序列拼接成大向量，再进行CNN分类，如DTAN<sup><a href="#fn_11" id="reffn_11">11</a></sup>。</p><ul><li>面部landmark运动轨迹：通过研究五官的变化轨迹，进而分析表情的变化，如深度几何空间网络(deep temporal geometry network，DTGN)。该方法联合每帧landmark的x,y坐标值，归一化处理后，将landmark作为一个运动轨迹维度，或者或者计算landmark特征点的成对L2距离特征，以及基于PHRNN用于获取帧内的空间变化信息。还有根据五官将landmark点分成4块，输入到BRNNs，定位局部特征，如下图：<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1528020429700.jpg" alt="enter description here"></li><li>级联网络：跟之前静态图像的级联网络思路一样，主要是CNN提取特征，级联RNN做序列特征分类。如LRCN，级联CNN与LSTM，类似的，还有级联DAE作为特征提取，LSTM进行分类，还有ResNet-LSTM,即在低级CNN层，直接用LSTM连接序列之间的低级CNN特征，3DIR用LSTM作为一个单元构建了一个3D Inception-ResNet特征层，其他还有很多类似的级联网络，包括，用CRFs代替了LSTM等等。</li><li>网络集成：如两路CNN网络模型用于行为识别，一路用多帧数据的稠密光流训练获取时间信息，一路用于单帧图像特征学习，最后融合两路CNN的输出。还有多通道训练，如一通道用于自然脸和表情脸之间的光流信息训练，一路用于脸部表情特征训练，然后用三种融合策略，平均融合，基于SVM融合，基于DNN融合。也有基于PHRNN时间网络和MSCNN空间网络相结合来提取局部整体关系，几何变化以及静动态信息。除了融合，也有联合训练的，如DTAN和DTGN联合fineturn训练。<br><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1528020602436.jpg" alt="enter description here"></li></ul><p>&emsp;&emsp;目前各个数据集上，动态序列的表情识别的最佳效果如下表所示：</p><p><img src="http://p7jiixmp8.bkt.clouddn.com/Blog/1528020691450.jpg" alt="enter description here"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote id="fn_1"> <sup>1</sup>. Li S, Deng W. Deep Facial Expression Recognition: A Survey[J]. 2018. <a href="#reffn_1" title="Jump back to footnote [1] in the text.">&#8617;</a></blockquote><blockquote id="fn_2"> <sup>2</sup>. Viola P, Jones M. Rapid object detection using a boosted cascade of simple features[J]. Proc Cvpr, 2001, 1:511. <a href="#reffn_2" title="Jump back to footnote [2] in the text.">&#8617;</a></blockquote><blockquote id="fn_3"> <sup>3</sup>. Torre F D L, Chu W S, Xiong X, et al. IntraFace[C]// IEEE International Conference and Workshops on Automatic Face and Gesture Recognition. IEEE, 2015:1-8. <a href="#reffn_3" title="Jump back to footnote [3] in the text.">&#8617;</a></blockquote><blockquote id="fn_4"> <sup>4</sup>. Goodfellow I J, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets[C]// International Conference on Neural Information Processing Systems. MIT Press, 2014:2672-2680. <a href="#reffn_4" title="Jump back to footnote [4] in the text.">&#8617;</a></blockquote><blockquote id="fn_5"> <sup>5</sup>. <a href="http://luks.fe.uni-lj.si/sl/osebje/vitomir/face tools/INFace/" target="_blank" rel="noopener">http://luks.fe.uni-lj.si/sl/osebje/vitomir/face tools/INFace/</a> <a href="#reffn_5" title="Jump back to footnote [5] in the text.">&#8617;</a></blockquote><blockquote id="fn_6"> <sup>6</sup>. Ding H, Zhou S K, Chellappa R. FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition[J]. 2016:118-126. <a href="#reffn_6" title="Jump back to footnote [6] in the text.">&#8617;</a></blockquote><blockquote id="fn_7"> <sup>7</sup>. Cai J, Meng Z, Khan A S, et al. Island Loss for Learning Discriminative Features in Facial Expression Recognition[J]. 2017. <a href="#reffn_7" title="Jump back to footnote [7] in the text.">&#8617;</a></blockquote><blockquote id="fn_8"> <sup>8</sup>. Li S, Deng W, Du J P. Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2017:2584-2593. <a href="#reffn_8" title="Jump back to footnote [8] in the text.">&#8617;</a></blockquote><blockquote id="fn_9"> <sup>9</sup>. Zhang K, Huang Y, Du Y, et al. Facial Expression Recognition Based on Deep Evolutional Spatial-Temporal Networks[J]. IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 2017, PP(99):1-1. <a href="#reffn_9" title="Jump back to footnote [9] in the text.">&#8617;</a></blockquote><blockquote id="fn_10"> <sup>10</sup>. Liu M, Li S, Shan S, et al. Deeply Learning Deformable Facial Action Parts Model for Dynamic Expression Analysis[M]// Computer Vision — ACCV 2014. Springer International Publishing, 2014:143-157. <a href="#reffn_10" title="Jump back to footnote [10] in the text.">&#8617;</a></blockquote><blockquote id="fn_11"> <sup>11</sup>. Jung H, Lee S, Yim J, et al. Joint Fine-Tuning in Deep Neural Networks for Facial Expression Recognition[C]// IEEE International Conference on Computer Vision. IEEE, 2016:2983-2991. <a href="#reffn_11" title="Jump back to footnote [11] in the text.">&#8617;</a></blockquote></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束<i class="fa fa-smile-o"></i>感谢您的阅读-------------</div></div></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'> <span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"> <img id="wechat_qr" src="http://p7jiixmp8.bkt.clouddn.com/wechatpay.jpg" alt="冲弱 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"> <img id="alipay_qr" src="http://p7jiixmp8.bkt.clouddn.com/alipay.jpg" alt="冲弱 支付宝"><p>支付宝</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/CVPR/" rel="tag"><i class="fa fa-tag"></i> CVPR</a><a href="/tags/深度学习/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a><a href="/tags/FER/" rel="tag"><i class="fa fa-tag"></i> FER</a></div><div><div class="my_post_copyright"><script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script><script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"></script><script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"></script><link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css"><p><span>本文标题:</span>《Deep Facial Expression Recognition:A Survey》论文笔记</p><p><span>文章作者:</span>冲弱</p><p><span>发布时间:</span>2018年06月01日 - 17:22:25</p><p><span>最后更新:</span>2018年06月03日 - 23:58:25</p><p><span>原始链接:</span><a href="/post/5d962f61.html" title="《Deep Facial Expression Recognition:A Survey》论文笔记">https://oysz2016.github.io/post/5d962f61.html</a><span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://oysz2016.github.io/post/5d962f61.html" aria-label="复制成功！"></i></span></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p></div><script>var clipboard=new Clipboard(".fa-clipboard");clipboard.on("success",$(function(){$(".fa-clipboard").click(function(){swal({title:"",text:"复制成功",html:!1,timer:500,showConfirmButton:!1})})}))</script></div><div class="post-widgets"><div class="wp_rating"><div style="color:rgba(0,0,0,.75);font-size:13px;letter-spacing:3px">(&gt;给我的文章打个分吧&lt;)</div><div id="wpac-rating"></div></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/post/8611e6fb.html" rel="next" title="数学公式语法——Mathjax教程"><i class="fa fa-chevron-left"></i> 数学公式语法——Mathjax教程</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/post/ad14e7f5.html" rel="prev" title="极简人类史">极简人类史<i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5adaebc9009ff58b" async="async"></script></div></div></div></div><div class="comments" id="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC8zNTg2Ni8xMjQwMg=="></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div id="sidebar-dimmer"></div><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap"> 站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a href="/" class="site-author-image" rel="start" style="border:none"><img class="site-author-image" itemprop="image" src="http://p7jiixmp8.bkt.clouddn.com/%E6%B4%BE%E5%A4%A7%E6%98%9F.png" alt="冲弱"></a><p class="site-author-name" itemprop="name">冲弱</p><p class="site-description motion-element" itemprop="description">输出即获得，分享即反哺</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">12</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">4</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">27</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a><a title="把这个链接拖到你的工具栏中,任何网页都可以High" href='javascript:(
    /*
     * Copyright (C) 2016 Never_yu (Neveryu.github.io) <React.dong.yu@gmail.com>
     * Sina Weibo (http://weibo.com/Neveryu)
     *
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     *      http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     */
    function go() {


    var songs = ["http://p7jiixmp8.bkt.clouddn.com/%E5%B0%8F%E9%AD%82%20-%20%E7%A6%BB%E4%BA%BA%E6%84%81%EF%BC%88Cover%20%E6%9B%B2%E8%82%96%E5%86%B0%EF%BC%89%20%28clip%29.mp3","http://p7jiixmp8.bkt.clouddn.com/%E5%9B%9E%E5%BF%86%E9%82%A3%E4%B9%88%E4%BC%A4_190649.mp3","http://p7jiixmp8.bkt.clouddn.com/%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90strong%28clip%29.mp3","http://p7jiixmp8.bkt.clouddn.com/%E8%A5%BF%E7%93%9CJUN%20-%20%E7%8B%82%E9%87%8E%E6%83%B3%E4%B9%A12%20%28clip%29.mp3"
    ];

    
    function c() {
        var e = document.createElement("link");
        e.setAttribute("type", "text/css");
        e.setAttribute("rel", "stylesheet");
        e.setAttribute("href", f);
        e.setAttribute("class", l);
        document.body.appendChild(e)
    }
 
    function h() {
        var e = document.getElementsByClassName(l);
        for (var t = 0; t < e.length; t++) {
            document.body.removeChild(e[t])
        }
    }
 
    function p() {
        var e = document.createElement("div");
        e.setAttribute("class", a);
        document.body.appendChild(e);
        setTimeout(function() {
            document.body.removeChild(e)
        }, 100)
    }
 
    function d(e) {
        return {
            height : e.offsetHeight,
            width : e.offsetWidth
        }
    }
 
    function v(i) {
        var s = d(i);
        return s.height > e && s.height < n && s.width > t && s.width < r
    }
 
    function m(e) {
        var t = e;
        var n = 0;
        while (!!t) {
            n += t.offsetTop;
            t = t.offsetParent
        }
        return n
    }
 
    function g() {
        var e = document.documentElement;
        if (!!window.innerWidth) {
            return window.innerHeight
        } else if (e && !isNaN(e.clientHeight)) {
            return e.clientHeight
        }
        return 0
    }
 
    function y() {
        if (window.pageYOffset) {
            return window.pageYOffset
        }
        return Math.max(document.documentElement.scrollTop, document.body.scrollTop)
    }
 
    function E(e) {
        var t = m(e);
        return t >= w && t <= b + w
    }

    function S() {
        var e = document.getElementById("audio_element_id");
        if(e != null){
            var index = parseInt(e.getAttribute("curSongIndex"));
            if(index > songs.length - 2) {
                index = 0;
            } else {
                index++;
            }
            e.setAttribute("curSongIndex", index);
            N();
        }

        e.src = i;
        e.play()
    }
 
    function x(e) {
        e.className += " " + s + " " + o
    }
 
    function T(e) {
        e.className += " " + s + " " + u[Math.floor(Math.random() * u.length)]
    }
 
    function N() {
        var e = document.getElementsByClassName(s);
        var t = new RegExp("\\b" + s + "\\b");
        for (var n = 0; n < e.length; ) {
            e[n].className = e[n].className.replace(t, "")
        }
    }

    function initAudioEle() {
        var e = document.getElementById("audio_element_id");
        if(e === null){
            e = document.createElement("audio");
            e.setAttribute("class", l);
            e.setAttribute("curSongIndex", 0);
            e.id = "audio_element_id";
            e.loop = false;
            e.bgcolor = 0;
            e.addEventListener("canplay", function() {
            setTimeout(function() {
                x(k)
            }, 500);
            setTimeout(function() {
                N();
                p();
                for (var e = 0; e < O.length; e++) {
                    T(O[e])
                }
            }, 15500)
        }, true);
        e.addEventListener("ended", function() {
            N();
            h();
            go();
        }, true);
        e.innerHTML = " <p>If you are reading this, it is because your browser does not support the audio element. We recommend that you get a new browser.</p> <p>";
        document.body.appendChild(e);
        }
    }
    
    initAudioEle();
    var e = 30;
    var t = 30;
    var n = 350;
    var r = 350;

    var curSongIndex = parseInt(document.getElementById("audio_element_id").getAttribute("curSongIndex"));
    var i = songs[curSongIndex];
    
    var s = "mw-harlem_shake_me";
    var o = "im_first";
    var u = ["im_drunk", "im_baked", "im_trippin", "im_blown"];
    var a = "mw-strobe_light";

    /* harlem-shake-style.css，替换成你的位置，也可以直接使用：//s3.amazonaws.com/moovweb-marketing/playground/harlem-shake-style.css */
    var f = "//s3.amazonaws.com/moovweb-marketing/playground/harlem-shake-style.css";
    
    var l = "mw_added_css";
    var b = g();
    var w = y();
    var C = document.getElementsByTagName("*");
    var k = null;
    for (var L = 0; L < C.length; L++) {
        var A = C[L];
        if (v(A)) {
            if (E(A)) {
                k = A;
                break
            }
        }
    }
    if (A === null) {
        console.warn("Could not find a node of the right size. Please try a different page.");
        return
    }
    c();
    S();
    var O = [];
    for (var L = 0; L < C.length; L++) {
        var A = C[L];
        if (v(A)) {
            O.push(A)
        }
    }
    })()'><i class="fa fa-music"></i> ShakeMusic!</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/oysz2016" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:ouyang-sz@foxmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://juejin.im/user/5ae26032f265da0b9f3ffeb2" target="_blank" title="掘金"><i class="fa fa-fw fa-spinner"></i> 掘金</a></span><span class="links-of-author-item"><a href="https://www.jianshu.com/u/d6119d98a96a" target="_blank" title="简书"><i class="fa fa-fw fa-book"></i> 简书</a></span></div><div id="days"></div><script language="javascript">function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("04/22/2018 15:00:00"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行"+daysold+"天"+hrsold+"小时"+minsold+"分"+seconds+"秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#介绍"><span class="nav-number">1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深度人脸表情识别"><span class="nav-number">2.</span> <span class="nav-text">深度人脸表情识别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#预处理"><span class="nav-number">2.1.</span> <span class="nav-text">预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#人脸对齐"><span class="nav-number">2.1.1.</span> <span class="nav-text">人脸对齐</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据增强"><span class="nav-number">2.1.2.</span> <span class="nav-text">数据增强</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#人脸归一化"><span class="nav-number">2.1.3.</span> <span class="nav-text">人脸归一化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#深度特征学习"><span class="nav-number">2.2.</span> <span class="nav-text">深度特征学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#人脸表情分类"><span class="nav-number">2.3.</span> <span class="nav-text">人脸表情分类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#面部表情数据库"><span class="nav-number">3.</span> <span class="nav-text">面部表情数据库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FER目前发展水平"><span class="nav-number">4.</span> <span class="nav-text">FER目前发展水平</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#静态图像FER进展"><span class="nav-number">4.1.</span> <span class="nav-text">静态图像FER进展</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#预训练和微调"><span class="nav-number">4.1.1.</span> <span class="nav-text">预训练和微调</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多样化网络输入"><span class="nav-number">4.1.2.</span> <span class="nav-text">多样化网络输入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#辅助块与层改进"><span class="nav-number">4.1.3.</span> <span class="nav-text">辅助块与层改进</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#网络集成"><span class="nav-number">4.1.4.</span> <span class="nav-text">网络集成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多任务网络"><span class="nav-number">4.1.5.</span> <span class="nav-text">多任务网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#网络级联"><span class="nav-number">4.1.6.</span> <span class="nav-text">网络级联</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#动态图像序列FER进展"><span class="nav-number">4.2.</span> <span class="nav-text">动态图像序列FER进展</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#帧聚合"><span class="nav-number">4.2.1.</span> <span class="nav-text">帧聚合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#强度表达网络"><span class="nav-number">4.2.2.</span> <span class="nav-text">强度表达网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#深度时空FER网络"><span class="nav-number">4.2.3.</span> <span class="nav-text">深度时空FER网络</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">5.</span> <span class="nav-text">参考文献</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span><span class="with-love" id="heart"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">冲弱</span></div><div class="powered-by"><i class="fa fa-user-md"></i> <span id="busuanzi_container_site_uv">本站访客数:<span id="busuanzi_value_site_uv"></span></span></div> <span class="post-meta-divider">|</span><div class="theme-info"><div class="powered-by"></div> <span class="post-count">博客全站共33.9k字</span></div><div class="weixin-box"><div class="weixin-menu"><div class="weixin-hover"><div class="weixin-description">微信扫一扫，订阅本博客</div></div></div></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">!function(e,t){var n,c=e.getElementsByTagName(t)[0];"function"!=typeof LivereTower&&((n=e.createElement(t)).src="https://cdn-city.livere.com/js/embed.dist.js",n.async=!0,c.parentNode.insertBefore(n,c))}(document,"script")</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("7yvqM96LoALQrcwK8vCi5wPx-gzGzoHsz","shWOzxYkisgnXyjW2U3Ldj9V")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),t.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><script type="text/javascript">wpac_init=window.wpac_init||[],wpac_init.push({widget:"Rating",id:11277,el:"wpac-rating",color:"f79533"}),function(){if(!("WIDGETPACK_LOADED"in window)){WIDGETPACK_LOADED=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//embed.widgetpack.com/widget.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t.nextSibling)}}()</script><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script><script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>;<script>$("body").backstretch("http://p7jiixmp8.bkt.clouddn.com/WallpaperStudio10-81831.jpg")</script></body></html><script type="text/javascript" src="/js/src/love.js"></script>